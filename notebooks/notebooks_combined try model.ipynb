{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": true,
    "id": "qx4oxDctftL6",
    "outputId": "106369c9-07d7-47d3-d141-446758a4d3d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from tensorflow) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from tensorflow) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from tensorflow) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from tensorflow) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from tensorflow) (1.68.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from tensorflow) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from tensorflow) (3.7.0)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from tensorflow) (2.0.2)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from tensorflow) (3.12.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from tensorflow) (0.4.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from tensorflow) (0.37.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: rich in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
      "Requirement already satisfied: namex in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow) (0.13.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: keras-nlp in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (0.18.1)\n",
      "Requirement already satisfied: keras-hub==0.18.1 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from keras-nlp) (0.18.1)\n",
      "Requirement already satisfied: absl-py in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from keras-hub==0.18.1->keras-nlp) (1.4.0)\n",
      "Requirement already satisfied: numpy in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from keras-hub==0.18.1->keras-nlp) (2.0.2)\n",
      "Requirement already satisfied: packaging in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from keras-hub==0.18.1->keras-nlp) (24.2)\n",
      "Requirement already satisfied: regex in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from keras-hub==0.18.1->keras-nlp) (2024.11.6)\n",
      "Requirement already satisfied: rich in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from keras-hub==0.18.1->keras-nlp) (13.9.4)\n",
      "Requirement already satisfied: kagglehub in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from keras-hub==0.18.1->keras-nlp) (0.3.4)\n",
      "Requirement already satisfied: tensorflow-text in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from keras-hub==0.18.1->keras-nlp) (2.18.0)\n",
      "Requirement already satisfied: requests in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from kagglehub->keras-hub==0.18.1->keras-nlp) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from kagglehub->keras-hub==0.18.1->keras-nlp) (4.67.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from rich->keras-hub==0.18.1->keras-nlp) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from rich->keras-hub==0.18.1->keras-nlp) (2.18.0)\n",
      "Requirement already satisfied: typing-extensions<5.0,>=4.0.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from rich->keras-hub==0.18.1->keras-nlp) (4.12.2)\n",
      "Requirement already satisfied: tensorflow<2.19,>=2.18.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from tensorflow-text->keras-hub==0.18.1->keras-nlp) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras-hub==0.18.1->keras-nlp) (0.1.2)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras-nlp) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras-nlp) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras-nlp) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras-nlp) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras-nlp) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras-nlp) (3.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras-nlp) (3.20.3)\n",
      "Requirement already satisfied: setuptools in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras-nlp) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras-nlp) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras-nlp) (2.5.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras-nlp) (1.17.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras-nlp) (1.68.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras-nlp) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras-nlp) (3.7.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras-nlp) (3.12.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras-nlp) (0.4.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras-nlp) (0.37.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from requests->kagglehub->keras-hub==0.18.1->keras-nlp) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from requests->kagglehub->keras-hub==0.18.1->keras-nlp) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from requests->kagglehub->keras-hub==0.18.1->keras-nlp) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from requests->kagglehub->keras-hub==0.18.1->keras-nlp) (2024.8.30)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras-nlp) (0.44.0)\n",
      "Requirement already satisfied: namex in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras-nlp) (0.0.8)\n",
      "Requirement already satisfied: optree in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras-nlp) (0.13.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras-nlp) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras-nlp) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras-nlp) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras-nlp) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: keras in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (3.7.0)\n",
      "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7feb0adfd180>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/keras/\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: absl-py in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from keras) (1.4.0)\n",
      "Requirement already satisfied: numpy in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from keras) (2.0.2)\n",
      "Requirement already satisfied: rich in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from keras) (13.9.4)\n",
      "Requirement already satisfied: namex in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from keras) (0.0.8)\n",
      "Requirement already satisfied: h5py in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from keras) (3.12.1)\n",
      "Requirement already satisfied: optree in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from keras) (0.13.1)\n",
      "Requirement already satisfied: ml-dtypes in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from keras) (0.4.1)\n",
      "Requirement already satisfied: packaging in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from keras) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from optree->keras) (4.12.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from rich->keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from rich->keras) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: spacy in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (3.8.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from spacy) (1.0.11)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from spacy) (8.3.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from spacy) (0.15.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from spacy) (2.10.3)\n",
      "Requirement already satisfied: jinja2 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from spacy) (75.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from spacy) (24.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from spacy) (2.0.2)\n",
      "Requirement already satisfied: language-data>=1.2 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.1)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
      "Requirement already satisfied: blis<1.1.0,>=1.0.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from thinc<8.4.0,>=8.3.0->spacy) (1.0.1)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from thinc<8.4.0,>=8.3.0->spacy) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from jinja2->spacy) (3.0.2)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
      "Requirement already satisfied: wrapt in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade tensorflow\n",
    "%pip install --upgrade keras-nlp\n",
    "%pip install --upgrade keras\n",
    "!python -m pip install spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wze90GQI6AEI",
    "outputId": "8d214fff-605d-4375-b00a-a6f7d698ccb0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-09 11:54:45.636948: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-09 11:54:46.003489: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1733720086.147053     960 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1733720086.189990     960 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-09 11:54:46.553690: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.0\n"
     ]
    }
   ],
   "source": [
    "import keras;\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "WrDbvi-tfmdl"
   },
   "outputs": [],
   "source": [
    "# Import-import\n",
    "import pandas as pd\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import keras_nlp\n",
    "import keras\n",
    "import os\n",
    "import json\n",
    "import spacy\n",
    "import sklearn.model_selection\n",
    "import sklearn.utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lT1x77TTfmdm",
    "outputId": "77c0b43e-04ce-44ea-97dc-0bd465ff44af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOT in Colab\n"
     ]
    }
   ],
   "source": [
    "if os.getenv(\"COLAB_RELEASE_TAG\"):\n",
    "   print(\"Running in Colab\")\n",
    "   IN_COLAB = True\n",
    "else:\n",
    "   print(\"NOT in Colab\")\n",
    "   IN_COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 193
    },
    "id": "RwL6HScQfmdm",
    "outputId": "a56d55b2-3028-41d4-b1a5-2033db0bacfc"
   },
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[Errno 17] File exists: 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m      7\u001b[0m     data_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 8\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: 'data'"
     ]
    }
   ],
   "source": [
    "if IN_COLAB:\n",
    "    # Load the Drive helper and mount\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    data_dir = \"drive/MyDrive/data\"\n",
    "else:\n",
    "    data_dir = \"data\"\n",
    "os.mkdir(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TaFX1wv1fmdn",
    "outputId": "63411d86-f336-427d-d8a2-0837e18eb796"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-12-09 11:55:34--  https://drive.google.com/uc?export=download&id=1GJn2kEIBgto2OyD7-h2HQOv_NJUriqJh\n",
      "Resolving drive.google.com (drive.google.com)... 142.251.12.102, 142.251.12.113, 142.251.12.138, ...\n",
      "Connecting to drive.google.com (drive.google.com)|142.251.12.102|:443... connected.\n",
      "HTTP request sent, awaiting response... 303 See Other\n",
      "Location: https://drive.usercontent.google.com/download?id=1GJn2kEIBgto2OyD7-h2HQOv_NJUriqJh&export=download [following]\n",
      "--2024-12-09 11:55:35--  https://drive.usercontent.google.com/download?id=1GJn2kEIBgto2OyD7-h2HQOv_NJUriqJh&export=download\n",
      "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 172.253.118.132, 2404:6800:4003:c01::84\n",
      "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|172.253.118.132|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 31469558 (30M) [application/octet-stream]\n",
      "Saving to: ‘data/combined_data.csv’\n",
      "\n",
      "data/combined_data. 100%[===================>]  30.01M   122KB/s    in 2m 43s  \n",
      "\n",
      "2024-12-09 11:58:21 (188 KB/s) - ‘data/combined_data.csv’ saved [31469558/31469558]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download raw dataset\n",
    "!wget -O {data_dir+\"/combined_data.csv\"} \"https://drive.google.com/uc?export=download&id=1GJn2kEIBgto2OyD7-h2HQOv_NJUriqJh\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LwFY2EZEfmdn"
   },
   "source": [
    "# Step 1: Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "usXqdLlefmdo"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "yhYl6YGQfmdo"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_dir+\"/combined_data.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "xMZcubI5fmdo",
    "outputId": "f3dd057b-cdab-4c03-e1e6-0221d538b712"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>statement</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>oh my gosh</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>trouble sleeping, confused mind, restless hear...</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All wrong, back off dear, forward doubt. Stay ...</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I've shifted my focus to something else but I'...</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I'm restless and restless, it's been a month n...</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           statement   status\n",
       "0                                         oh my gosh  Anxiety\n",
       "1  trouble sleeping, confused mind, restless hear...  Anxiety\n",
       "2  All wrong, back off dear, forward doubt. Stay ...  Anxiety\n",
       "3  I've shifted my focus to something else but I'...  Anxiety\n",
       "4  I'm restless and restless, it's been a month n...  Anxiety"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zz0XrQghfmdo",
    "outputId": "0c5228d8-cbcb-481c-e507-793d432e729b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 53043 entries, 0 to 53042\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   statement  52681 non-null  object\n",
      " 1   status     53043 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 1.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wycr9Gj1fmdo"
   },
   "source": [
    "### Step 1a: Basic cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YbpOTg4xfmdo",
    "outputId": "03e33ba7-fec3-402f-d4b6-947d12147525"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with missing values: statement    362\n",
      "status         0\n",
      "dtype: int64\n",
      "Number of duplicate rows: 1969\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(f\"Number of rows with missing values: {df.isnull().sum()}\")\n",
    "\n",
    "# Check for duplicates\n",
    "print(f\"Number of duplicate rows: {df.duplicated(subset=['statement']).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "F-nz9lIHfmdp"
   },
   "outputs": [],
   "source": [
    "# Drop rows that contain empty values\n",
    "df = df.dropna()\n",
    "\n",
    "# Drop rows that contain duplicate values in the ‘statement’ column and keep only the first row\n",
    "df = df.drop_duplicates(subset=['statement'], keep='first')\n",
    "\n",
    "df.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bMxMHCd3fmdp",
    "outputId": "d92ca87b-0a67-47bc-b6da-47c00644196b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with missing values: statement    0\n",
      "status       0\n",
      "dtype: int64\n",
      "Number of duplicate rows: 0\n"
     ]
    }
   ],
   "source": [
    "# Recheck for missing values\n",
    "print(f\"Number of rows with missing values: {df.isnull().sum()}\")\n",
    "\n",
    "# Recheck for duplicates\n",
    "print(f\"Number of duplicate rows: {df.duplicated(subset=['statement']).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ihQ54ThCfmdp"
   },
   "source": [
    "### Step 1b: Deep cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Sclv0Apifmdp"
   },
   "outputs": [],
   "source": [
    "# Change the data type of ‘statement’ and ‘status’ columns to string\n",
    "df = df.astype({\"statement\":str, \"status\":str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "qtbWv3cTfmdp"
   },
   "outputs": [],
   "source": [
    "emoj = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\"\n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "\n",
    "stopwords = [\"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "zstHlPs8fmdp"
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower() \n",
    "    \n",
    "    # Hapus angka\n",
    "    text = re.sub(r'\\d+', '', text)  \n",
    "    \n",
    "    # Hapus emoji (Unicode Range)\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "    \n",
    "    # Hapus spasi ganda atau lebih\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    doc = nlp(text)\n",
    "    cleaned_text = []\n",
    "    for token in doc:\n",
    "        # remove stopwords\n",
    "        if token.is_stop:\n",
    "            continue\n",
    "        # replace verb with its lemma\n",
    "        elif token.pos_ == \"VERB\":\n",
    "            cleaned_text.append(token.lemma_)\n",
    "        else:\n",
    "            cleaned_text.append(token.text)\n",
    "\n",
    "    text = \" \".join(cleaned_text)\n",
    "\n",
    "    # Hapus punctuation marks\n",
    "    text = re.sub(r'[^\\w\\s]', '', text) \n",
    "    \n",
    "    # Hapus karakter berulang\n",
    "    text = re.sub(r'(.)\\1+', r'\\1', text)\n",
    "    \n",
    "    # Hapus karakter tunggal (misalnya huruf yang berdiri sendiri)\n",
    "    text = re.sub(r'\\b\\w\\b', '', text)\n",
    "\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "E8RHcyfIfmdq"
   },
   "outputs": [],
   "source": [
    "# CLEAN!!!\n",
    "df['statement'] = df['statement'].apply(clean_text)\n",
    "df = df[df['statement'] != \"\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aDcqaCdIfmdq"
   },
   "source": [
    "### Step 1c: Very deep cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "id": "_6A82beAfmdq",
    "outputId": "023966b5-55ee-4414-edb0-8823ab88d2bc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "status\n",
       "Normal                  15823\n",
       "Depression              15083\n",
       "Suicidal                10637\n",
       "Anxiety                  3616\n",
       "Bipolar                  2501\n",
       "Stress                   2293\n",
       "Personality disorder      895\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data distribution analysis of each label\n",
    "df.status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "PAHh2u0Dfmdq"
   },
   "outputs": [],
   "source": [
    "# Adding word count column for further analysis\n",
    "df['word_count'] = df['statement'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "kewGXeGufmdq"
   },
   "outputs": [],
   "source": [
    "# Define bins and labels for word count ranges\n",
    "bins = [0, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, float('inf')]  # Adjust as needed\n",
    "labels = ['1-100', '101-200', '201-300', '301-400', '401-500', '501-600', '601-700', '701-800', '801-900', '901-1000', '+1000']\n",
    "\n",
    "# Add a column to categorize statements into ranges\n",
    "df['word_count_range'] = pd.cut(df['word_count'], bins=bins, labels=labels, right=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 460
    },
    "id": "bK2xBB8mfmdq",
    "outputId": "c59185e6-7e11-4a2a-9b1a-1c97995c9faf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "word_count_range\n",
       "1-100       45615\n",
       "101-200      4044\n",
       "201-300       808\n",
       "301-400       240\n",
       "401-500        74\n",
       "501-600        37\n",
       "601-700        10\n",
       "701-800         5\n",
       "801-900         5\n",
       "901-1000        5\n",
       "+1000           5\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the number of statements in each range\n",
    "df['word_count_range'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 480
    },
    "id": "An_5QsLCfmdr",
    "outputId": "a24cb5f4-65b9-41f4-a733-c21c4858a635"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_960/3797327473.py:2: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  df.groupby(['word_count_range', 'status']).size().unstack(fill_value=0)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>status</th>\n",
       "      <th>Anxiety</th>\n",
       "      <th>Bipolar</th>\n",
       "      <th>Depression</th>\n",
       "      <th>Normal</th>\n",
       "      <th>Personality disorder</th>\n",
       "      <th>Stress</th>\n",
       "      <th>Suicidal</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word_count_range</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1-100</th>\n",
       "      <td>3026</td>\n",
       "      <td>2008</td>\n",
       "      <td>12586</td>\n",
       "      <td>15823</td>\n",
       "      <td>729</td>\n",
       "      <td>2154</td>\n",
       "      <td>9289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101-200</th>\n",
       "      <td>469</td>\n",
       "      <td>388</td>\n",
       "      <td>1920</td>\n",
       "      <td>0</td>\n",
       "      <td>132</td>\n",
       "      <td>110</td>\n",
       "      <td>1025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201-300</th>\n",
       "      <td>87</td>\n",
       "      <td>77</td>\n",
       "      <td>385</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>18</td>\n",
       "      <td>215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301-400</th>\n",
       "      <td>23</td>\n",
       "      <td>20</td>\n",
       "      <td>113</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401-500</th>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501-600</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601-700</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>701-800</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>801-900</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>901-1000</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>+1000</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "status            Anxiety  Bipolar  Depression  Normal  Personality disorder  \\\n",
       "word_count_range                                                               \n",
       "1-100                3026     2008       12586   15823                   729   \n",
       "101-200               469      388        1920       0                   132   \n",
       "201-300                87       77         385       0                    26   \n",
       "301-400                23       20         113       0                     5   \n",
       "401-500                 8        4          38       0                     2   \n",
       "501-600                 3        2          22       0                     0   \n",
       "601-700                 0        1           7       0                     0   \n",
       "701-800                 0        0           3       0                     0   \n",
       "801-900                 0        0           4       0                     0   \n",
       "901-1000                0        0           4       0                     0   \n",
       "+1000                   0        1           1       0                     1   \n",
       "\n",
       "status            Stress  Suicidal  \n",
       "word_count_range                    \n",
       "1-100               2154      9289  \n",
       "101-200              110      1025  \n",
       "201-300               18       215  \n",
       "301-400                8        71  \n",
       "401-500                2        20  \n",
       "501-600                0        10  \n",
       "601-700                1         1  \n",
       "701-800                0         2  \n",
       "801-900                0         1  \n",
       "901-1000               0         1  \n",
       "+1000                  0         2  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group by word count range and label, then count occurrences\n",
    "df.groupby(['word_count_range', 'status']).size().unstack(fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "id": "lAY7fYq2fmdr",
    "outputId": "273a0dc0-6768-4d04-f98e-f8f4ed221de3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "status\n",
       "Depression              13742\n",
       "Suicidal                 9165\n",
       "Normal                   3540\n",
       "Anxiety                  2994\n",
       "Bipolar                  2450\n",
       "Stress                   2228\n",
       "Personality disorder      841\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_export_candidate = df[(df['word_count'] >= 10) & (df['word_count'] <= 1000)].reset_index(drop=True)\n",
    "df_export_candidate.status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window_split(df, max_length, overlap_ratio=0.5):    \n",
    "    def split_sentence(sentence, max_length):\n",
    "        words = sentence.split()  # Split by spaces, assuming words are separated by spaces\n",
    "        chunks = []\n",
    "        step = int(max_length * (1 - overlap_ratio))  # Calculate the step size based on overlap ratio\n",
    "        \n",
    "        # Slide over the sentence in chunks of max_length with overlap\n",
    "        for i in range(0, len(words), step):\n",
    "            chunk = words[i:i + max_length]\n",
    "            chunks.append(' '.join(chunk))  # Join back to a sentence\n",
    "            if i + max_length >= len(words):  # Exit condition if the chunk exceeds the sentence\n",
    "                break\n",
    "        return chunks\n",
    "\n",
    "    expanded_rows = []\n",
    "\n",
    "    # Iterate over the original DataFrame\n",
    "    for idx, row in df.iterrows():\n",
    "        sentence = row['statement']\n",
    "        label = row['status']\n",
    "        \n",
    "        # Split the sentence into smaller chunks\n",
    "        chunks = split_sentence(sentence, max_length, )\n",
    "        \n",
    "        # Create new rows for each chunk\n",
    "        for chunk in chunks:\n",
    "            expanded_rows.append({'statement': chunk, 'status': label})\n",
    "    \n",
    "    # Return the expanded dataframe\n",
    "    return pd.DataFrame(expanded_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>statement</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>oh gosh</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>trouble slep confused mind restles heart tune</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wrong dear forward doubt stay restles restles ...</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>shift focus woried</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>restles restles month boy mean</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52975</th>\n",
       "      <td>anxiety cause faintnes stand title anxiety cau...</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52976</th>\n",
       "      <td>anxiety heart symptom similar help heart nt go...</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52977</th>\n",
       "      <td>travel anxiety hi long time anxiety suferer ti...</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52978</th>\n",
       "      <td>fomo things involve recently watch tv bit obse...</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52979</th>\n",
       "      <td>get day anxiety house kids adults overwhelming...</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>52980 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               statement   status\n",
       "0                                                oh gosh  Anxiety\n",
       "1          trouble slep confused mind restles heart tune  Anxiety\n",
       "2      wrong dear forward doubt stay restles restles ...  Anxiety\n",
       "3                                     shift focus woried  Anxiety\n",
       "4                         restles restles month boy mean  Anxiety\n",
       "...                                                  ...      ...\n",
       "52975  anxiety cause faintnes stand title anxiety cau...  Anxiety\n",
       "52976  anxiety heart symptom similar help heart nt go...  Anxiety\n",
       "52977  travel anxiety hi long time anxiety suferer ti...  Anxiety\n",
       "52978  fomo things involve recently watch tv bit obse...  Anxiety\n",
       "52979  get day anxiety house kids adults overwhelming...  Anxiety\n",
       "\n",
       "[52980 rows x 2 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_export_candidate = sliding_window_split(df, 192, 0.5)\n",
    "df_export_candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oFB19IUHfmdr",
    "outputId": "4a0e8d05-d019-4ba6-9320-a2f76905357f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label with the lowest number of examples: Personality disorder\n",
      "Number of examples: 959\n"
     ]
    }
   ],
   "source": [
    "# Count the number of examples for each label\n",
    "label_counts = df_export_candidate['status'].value_counts()\n",
    "\n",
    "# Find the label with the minimum count\n",
    "min_label = label_counts.idxmin()\n",
    "min_count = label_counts.min()\n",
    "\n",
    "print(f\"Label with the lowest number of examples: {min_label}\")\n",
    "print(f\"Number of examples: {min_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "5KHCgH_Ofmdr",
    "outputId": "60711178-c950-46a3-b613-34ccd4a066a4"
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'word_count'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_936/634620789.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_export_candidate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_export_candidate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'word_count'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_export_candidate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_export_candidate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'status'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf_export_candidate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf_export_candidate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/capstone/lib/python3.10/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001b[0m\n\u001b[1;32m   7185\u001b[0m             )\n\u001b[1;32m   7186\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7187\u001b[0m             \u001b[0;31m# len(by) == 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7189\u001b[0;31m             \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7191\u001b[0m             \u001b[0;31m# need to rewrap column in Series to apply key function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7192\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/capstone/lib/python3.10/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1907\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mother_axes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1908\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_level_reference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1909\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1910\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1911\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1912\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1913\u001b[0m         \u001b[0;31m# Check for duplicates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1914\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'word_count'"
     ]
    }
   ],
   "source": [
    "# df_export_candidate = df_export_candidate.sort_values(by='word_count', ascending=False)\n",
    "# df_export_candidate = df_export_candidate.groupby('status').head(min_count)\n",
    "# df_export_candidate.reset_index(drop=True, inplace=True)\n",
    "# df_export_candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "id": "yW2R5Fehfmdr",
    "outputId": "59637ac4-cc87-4176-bdfe-858eb7a32d9d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "status\n",
       "Depression              16141\n",
       "Normal                  15823\n",
       "Suicidal                11219\n",
       "Anxiety                  3802\n",
       "Bipolar                  2693\n",
       "Stress                   2343\n",
       "Personality disorder      959\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_export_candidate.status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nMEVNuL2fmdr"
   },
   "outputs": [],
   "source": [
    "# df_export_candidate.drop(['word_count', 'word_count_range'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "qWw5R0ztfmds"
   },
   "outputs": [],
   "source": [
    "df_export_candidate = df_export_candidate.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "hXaIhmwafmds"
   },
   "outputs": [],
   "source": [
    "# Optional, export the cleaned dataset\n",
    "df_export_candidate.to_csv(data_dir+'/cleaned_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "94tE6LPnfmds"
   },
   "source": [
    "# Step 2: Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "OeooFwA3fmds"
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 10000\n",
    "MAX_LENGTH = 500\n",
    "TRAINING_SPLIT = 0.8\n",
    "BATCH_SIZE = 32\n",
    "PADDING_TYPE = 'post'\n",
    "TRUNC_TYPE = 'post'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "YWoCiB0Nfmds"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1733721571.485835     960 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1767 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "# Run this code if you skip step 1. beware, cleaned dataset is not updated regularly\n",
    "# !wget -O {data_dir+\"/cleaned_data.csv\"} \"https://drive.google.com/uc?export=download&id=1yQ8tt6HF6X_A_P0eYwS3yFC5vxGZYdkY\"\n",
    "\n",
    "sentences = []\n",
    "labels = []\n",
    "\n",
    "with open(data_dir+\"/cleaned_data.csv\", 'r') as csvfile:\n",
    "    heading = next(csvfile)\n",
    "    reader_obj = csv.reader(csvfile)\n",
    "    for row in reader_obj:\n",
    "        labels.append(row[1])\n",
    "        sentences.append(row[0])\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((sentences, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-_D7MDhcfmds"
   },
   "source": [
    "### Step 2a: Create tf.data.Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OBrpjjNkfmds",
    "outputId": "8570e108-15c5-40b4-a905-c689274faab8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 42384 sentence-label pairs for training.\n",
      "\n",
      "There are 10596 sentence-label pairs for validation.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_size = int(len(dataset) * TRAINING_SPLIT)\n",
    "train_dataset = dataset.take(train_size)\n",
    "validation_dataset = dataset.skip(train_size)\n",
    "\n",
    "print(f\"There are {train_dataset.cardinality()} sentence-label pairs for training.\\n\")\n",
    "print(f\"There are {validation_dataset.cardinality()} sentence-label pairs for validation.\\n\")\n",
    "\n",
    "train_statement = train_dataset.map(lambda statement, status: statement)\n",
    "train_labels = train_dataset.map(lambda statement, status: status)\n",
    "\n",
    "test_statement = validation_dataset.map(lambda statement, status: statement)\n",
    "test_labels = validation_dataset.map(lambda statement, status: status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0etvY-Fdfmdt"
   },
   "source": [
    "### Step 2b: Create tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0kn9QBxufmdt",
    "outputId": "22c7dd85-bc28-473e-a9ec-596cb93f291c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-09 13:24:41.140259: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary contains 9666 words\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# comment this code if there's already vocab output file\n",
    "keras_nlp.tokenizers.compute_word_piece_vocabulary(\n",
    "    train_statement,\n",
    "    vocabulary_size=VOCAB_SIZE,\n",
    "    reserved_tokens=[\"[PAD]\", \"[UNK]\"],\n",
    "    vocabulary_output_file='mental_vocab_subwords.txt'\n",
    ")\n",
    "\n",
    "# Initialize the subword tokenizer\n",
    "vectorizer = keras_nlp.tokenizers.WordPieceTokenizer(\n",
    "    vocabulary='./mental_vocab_subwords.txt'\n",
    ")\n",
    "\n",
    "vocab_size = vectorizer.vocabulary_size()\n",
    "print(f\"Vocabulary contains {vocab_size} words\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r3YCAhMAfmdt"
   },
   "source": [
    "### Step 2c: Create label encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "inY46A8xfmdu",
    "outputId": "e8fbcc1a-0302-462c-bdfe-4e22a855f915"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels: [np.str_('Depression'), np.str_('Normal'), np.str_('Suicidal'), np.str_('Anxiety'), np.str_('Bipolar'), np.str_('Stress'), np.str_('Personality disorder')]\n"
     ]
    }
   ],
   "source": [
    "def fit_label_encoder(train_labels, validation_labels):\n",
    "    \"\"\"Creates an instance of a StringLookup, and trains it on all labels\n",
    "\n",
    "    Args:\n",
    "        train_labels (tf.data.Dataset): dataset of train labels\n",
    "        validation_labels (tf.data.Dataset): dataset of validation labels\n",
    "\n",
    "    Returns:\n",
    "        tf.keras.layers.StringLookup: adapted encoder for train and validation labels\n",
    "    \"\"\"\n",
    "    # join the two label datasets\n",
    "    labels = train_labels.concatenate(validation_labels) #concatenate the two datasets.\n",
    "\n",
    "    # Instantiate the StringLookup layer. Remember that you don't want any OOV tokens\n",
    "    label_encoder = tf.keras.layers.StringLookup(num_oov_indices=0)\n",
    "\n",
    "    # Fit the TextVectorization layer on the train_labels\n",
    "    label_encoder.adapt(labels)\n",
    "\n",
    "    return label_encoder\n",
    "\n",
    "# Create the label encoder\n",
    "label_encoder = fit_label_encoder(train_labels,test_labels)\n",
    "\n",
    "print(f'Unique labels: {label_encoder.get_vocabulary()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CwtLzjPYfmdu"
   },
   "source": [
    "### Step 2d: Dataset preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "F6J_HF3Rfmdu"
   },
   "outputs": [],
   "source": [
    "def padding_func(sequences):\n",
    "  '''Generates padded sequences from a tf.data.Dataset'''\n",
    "\n",
    "  # Put all elements in a single ragged batch\n",
    "  sequences = sequences.ragged_batch(batch_size=sequences.cardinality())\n",
    "\n",
    "  # Output a tensor from the single batch\n",
    "  sequences = sequences.get_single_element()\n",
    "\n",
    "  # Pad the sequences\n",
    "  padded_sequences = tf.keras.utils.pad_sequences(sequences.numpy(),\n",
    "                                                  maxlen=MAX_LENGTH,\n",
    "                                                  truncating=TRUNC_TYPE,\n",
    "                                                  padding=PADDING_TYPE\n",
    "                                                  )\n",
    "\n",
    "  # Convert back to a tf.data.Dataset\n",
    "  padded_sequences = tf.data.Dataset.from_tensor_slices(padded_sequences)\n",
    "\n",
    "  return padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ElMlNEjOfmdu",
    "outputId": "c3f94775-9b3d-42f1-868c-b76c1b35a117"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches in the train dataset: 1325\n",
      "Number of batches in the validation dataset: 332\n"
     ]
    }
   ],
   "source": [
    "# Preprocess dataset\n",
    "SHUFFLE_BUFFER_SIZE = 10000\n",
    "PREFETCH_BUFFER_SIZE = tf.data.AUTOTUNE\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Generate integer sequences using the subword tokenizer\n",
    "train_sequences_subword = train_statement.map(lambda statement: tokenizer.tokenize(statement)).apply(padding_func)\n",
    "test_sequences_subword = test_statement.map(lambda statement: tokenizer.tokenize(statement)).apply(padding_func)\n",
    "\n",
    "train_labels_encoded = train_labels.map(lambda label: label_encoder(label))\n",
    "test_labels_encoded = test_labels.map(lambda label: label_encoder(label))\n",
    "\n",
    "# Combine the integer sequence and labels\n",
    "train_dataset_vectorized = tf.data.Dataset.zip(train_sequences_subword,train_labels_encoded)\n",
    "test_dataset_vectorized = tf.data.Dataset.zip(test_sequences_subword,test_labels_encoded)\n",
    "\n",
    "# Optimize the datasets for training\n",
    "train_dataset_final = (train_dataset_vectorized\n",
    "                       .shuffle(SHUFFLE_BUFFER_SIZE)\n",
    "                       .cache()\n",
    "                       .prefetch(buffer_size=PREFETCH_BUFFER_SIZE)\n",
    "                       .batch(BATCH_SIZE)\n",
    "                       )\n",
    "\n",
    "test_dataset_final = (test_dataset_vectorized\n",
    "                      .cache()\n",
    "                      .prefetch(buffer_size=PREFETCH_BUFFER_SIZE)\n",
    "                      .batch(BATCH_SIZE)\n",
    "                      )\n",
    "\n",
    "\n",
    "print(f\"Number of batches in the train dataset: {train_dataset_final.cardinality()}\")\n",
    "print(f\"Number of batches in the validation dataset: {test_dataset_final.cardinality()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0uZgZoqHfmdu",
    "outputId": "198bc23a-1719-4daa-c93d-49d0d6ce1732"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the train batch: (32, 500)\n",
      "Shape of the validation batch: (32, 500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-09 13:13:04.648696: W tensorflow/core/kernels/data/cache_dataset_ops.cc:914] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2024-12-09 13:13:04.682396: W tensorflow/core/kernels/data/cache_dataset_ops.cc:914] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "train_batch = next(train_dataset_final.as_numpy_iterator())\n",
    "validation_batch = next(test_dataset_final.as_numpy_iterator())\n",
    "\n",
    "print(f\"Shape of the train batch: {train_batch[0].shape}\")\n",
    "print(f\"Shape of the validation batch: {validation_batch[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-09 13:26:41.689312: W tensorflow/core/kernels/data/cache_dataset_ops.cc:914] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2024-12-09 13:26:41.773932: W tensorflow/core/kernels/data/cache_dataset_ops.cc:914] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: {np.int64(0): np.float64(0.4701706121181195), np.int64(1): np.float64(0.4792130702696591), np.int64(2): np.float64(0.6747862635525624), np.int64(3): np.float64(1.9787114845938376), np.int64(4): np.float64(2.7572209211553473), np.int64(5): np.float64(3.227535790435577), np.int64(6): np.float64(7.904513241327863)}\n"
     ]
    }
   ],
   "source": [
    "train_labels_numpy = np.array(list(train_labels_encoded))\n",
    "\n",
    "# Tentukan kelas yang ada di dataset Anda\n",
    "classes = np.unique(train_labels_numpy)  # atau manual, misalnya classes = [0, 1, 2, 3, 4, 5, 6]\n",
    "\n",
    "# Menghitung class weights\n",
    "class_weights = sklearn.utils.compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=classes,\n",
    "    y=train_labels_numpy\n",
    ")\n",
    "\n",
    "# Membuat dictionary class weights\n",
    "class_weight_dict = {classes[i]: class_weights[i] for i in range(len(classes))}\n",
    "\n",
    "print(f\"Class weights: {class_weight_dict}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rhi60hT1fmdu"
   },
   "source": [
    "### Step 2e: Architect the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 433
    },
    "id": "rq4EGjQ1fmdu",
    "outputId": "a05392d5-7865-4976-c6b9-354185d1c1c9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_10\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_10\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">618,624</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,128</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_45 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_46 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_41 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,112</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_47 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_42 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_48 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_max_pooling1d_10         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_43 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">231</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_14 (\u001b[38;5;33mEmbedding\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │       \u001b[38;5;34m618,624\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_6 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │         \u001b[38;5;34m4,128\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_6 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_45 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_6 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │         \u001b[38;5;34m8,320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_46 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_41 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │         \u001b[38;5;34m2,112\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_47 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_42 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_48 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_max_pooling1d_10         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_43 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m)              │           \u001b[38;5;34m231\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">635,495</span> (2.42 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m635,495\u001b[0m (2.42 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">635,495</span> (2.42 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m635,495\u001b[0m (2.42 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "EMBEDDING_DIM = 64\n",
    "LSTM1_DIM = 32\n",
    "LSTM2_DIM = 8\n",
    "DENSE_DIM = 64\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.Input(shape=(None,)),\n",
    "    tf.keras.layers.Embedding(vectorizer.vocabulary_size(), EMBEDDING_DIM),\n",
    "    tf.keras.layers.Conv1D(32, 2, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling1D(2),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.LSTM(32, return_sequences=True, dropout=0.2, recurrent_dropout=0.1),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.GlobalMaxPooling1D(),\n",
    "    tf.keras.layers.Dense(7, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NG8hLpP5fmdv",
    "outputId": "df4c08d2-fbc8-4d1e-9442-d45e11112e2c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-09 13:35:50.478518: W tensorflow/core/kernels/data/cache_dataset_ops.cc:914] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2024-12-09 13:35:52.744349: W tensorflow/core/kernels/data/cache_dataset_ops.cc:914] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions have shape: (32, 7)\n"
     ]
    }
   ],
   "source": [
    "# Check model compatibility\n",
    "example_batch = train_dataset_final.take(1)\n",
    "\n",
    "try:\n",
    "    model.evaluate(example_batch, verbose=False)\n",
    "except Exception as e:\n",
    "    print(f\"Your model is not compatible with the dataset you defined earlier. Check that the loss function and last layer are compatible with one another. Error: {e}\")\n",
    "else:\n",
    "    predictions = model.predict(example_batch, verbose=False)\n",
    "    print(f\"predictions have shape: {predictions.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gmBrP0z0fmdv"
   },
   "source": [
    "# Step 3: How To Train Your ~~Dragon~~ ML Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HedQzuKZfmdv"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "id": "1zGaikEDfmdv"
   },
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "checkpoint_path = data_dir\n",
    "checkpoint_model_filepath = checkpoint_path+\"/checkpoint.keras\"\n",
    "checkpoint_num_epoch_filepath = checkpoint_path+\"/current_epoch.txt\"\n",
    "training_log_filepath = checkpoint_path+\"/training_log.json\"\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_model_filepath,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "\n",
    "class CustomCheckpointCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, epoch_file, log_file):\n",
    "        self.epoch_file = epoch_file\n",
    "        self.log_file = log_file\n",
    "        self.history = {\"epoch\": [], \"train_loss\": [], \"train_accuracy\": [],\n",
    "                        \"val_loss\": [], \"val_accuracy\": []}\n",
    "\n",
    "        # Load history and starting epoch if they exist\n",
    "        if os.path.exists(self.log_file):\n",
    "            with open(self.log_file, 'r') as f:\n",
    "                self.history = json.load(f)\n",
    "\n",
    "        if os.path.exists(self.epoch_file):\n",
    "            with open(self.epoch_file, 'r') as f:\n",
    "                self.starting_epoch = int(f.read())\n",
    "        else:\n",
    "            # with open(self.epoch_file, 'w') as f:\n",
    "            #     f.write(str(0))\n",
    "            self.starting_epoch = 0\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current_epoch = self.starting_epoch + epoch + 1\n",
    "        # Save the current epoch\n",
    "        with open(self.epoch_file, 'w') as f:\n",
    "            f.write(str(current_epoch + 1))\n",
    "\n",
    "        # Save logs (loss, accuracy, etc.) for plotting\n",
    "        self.history[\"epoch\"].append(current_epoch + 1)\n",
    "        self.history[\"train_loss\"].append(logs.get(\"loss\"))\n",
    "        self.history[\"train_accuracy\"].append(logs.get(\"accuracy\"))\n",
    "        self.history[\"val_loss\"].append(logs.get(\"val_loss\"))\n",
    "        self.history[\"val_accuracy\"].append(logs.get(\"val_accuracy\"))\n",
    "\n",
    "        # Save history to the log file\n",
    "        with open(self.log_file, 'w') as f:\n",
    "            json.dump(self.history, f, indent=4)\n",
    "\n",
    "custom_checkpoint_callback = CustomCheckpointCallback(checkpoint_num_epoch_filepath, training_log_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jZ-YPeiLfmdv",
    "outputId": "a70746fb-f93d-46c3-9f5c-b110bec5c659"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No saved epoch found\n"
     ]
    }
   ],
   "source": [
    "# run this code to delete checkpoint\n",
    "try:\n",
    "    os.remove(checkpoint_model_filepath)\n",
    "    os.remove(checkpoint_num_epoch_filepath)\n",
    "    os.remove(training_log_filepath)\n",
    "    print(\"Checkpoint deleted successfully\")\n",
    "except FileNotFoundError:\n",
    "    print(\"No saved epoch found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wSc1uib7fmdv",
    "outputId": "3f9c3f88-7b32-4639-a0f0-a1bd57ba923f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No saved epoch found. Starting from epoch 0\n"
     ]
    }
   ],
   "source": [
    "# run this code to load from checkpoint\n",
    "try:\n",
    "    with open(checkpoint_num_epoch_filepath, 'r') as f:\n",
    "        start_epoch = int(f.read())\n",
    "    print(f\"Resuming training from epoch {start_epoch}\")\n",
    "except FileNotFoundError:\n",
    "    start_epoch = 0\n",
    "    print(\"No saved epoch found. Starting from epoch 0\")\n",
    "\n",
    "# Load saved weights\n",
    "if start_epoch > 0:\n",
    "    model.load_weights(checkpoint_model_filepath)\n",
    "    print(f\"Loaded weights from {checkpoint_model_filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jYeszbOpfmdw",
    "outputId": "5c2f7d4d-23ce-46f6-95c6-cf8af2034129"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1325/1325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m927s\u001b[0m 697ms/step - accuracy: 0.5299 - loss: 1.2306 - val_accuracy: 0.6631 - val_loss: 1.0770\n",
      "Epoch 2/10\n",
      "\u001b[1m1325/1325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m974s\u001b[0m 735ms/step - accuracy: 0.6866 - loss: 0.8295 - val_accuracy: 0.7138 - val_loss: 0.9812\n",
      "Epoch 3/10\n",
      "\u001b[1m1325/1325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m946s\u001b[0m 714ms/step - accuracy: 0.7325 - loss: 0.7110 - val_accuracy: 0.7202 - val_loss: 0.9356\n",
      "Epoch 4/10\n",
      "\u001b[1m1325/1325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1040s\u001b[0m 785ms/step - accuracy: 0.7625 - loss: 0.6312 - val_accuracy: 0.7227 - val_loss: 0.8973\n",
      "Epoch 5/10\n",
      "\u001b[1m1325/1325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1147s\u001b[0m 866ms/step - accuracy: 0.7866 - loss: 0.5798 - val_accuracy: 0.7291 - val_loss: 0.8524\n",
      "Epoch 6/10\n",
      "\u001b[1m1325/1325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m996s\u001b[0m 752ms/step - accuracy: 0.8041 - loss: 0.5314 - val_accuracy: 0.7233 - val_loss: 0.8397\n",
      "Epoch 7/10\n",
      "\u001b[1m 229/1325\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m25:21\u001b[0m 1s/step - accuracy: 0.8021 - loss: 0.5332"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "num_of_epoch = 10\n",
    "history = model.fit(\n",
    "    train_dataset_final,\n",
    "    epochs=num_of_epoch - start_epoch,\n",
    "    validation_data=test_dataset_final,\n",
    "    callbacks=[\n",
    "        model_checkpoint_callback,\n",
    "        custom_checkpoint_callback\n",
    "    ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 957
    },
    "id": "TTR38rCrfmdw",
    "outputId": "0ad68241-5100-4ba0-81ee-ca57889d36c6"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_log_filepath' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m     plt\u001b[38;5;241m.\u001b[39mlegend()\n\u001b[1;32m     24\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m---> 26\u001b[0m plot_graphs(\u001b[43mtraining_log_filepath\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'training_log_filepath' is not defined"
     ]
    }
   ],
   "source": [
    "def plot_graphs(training_log):\n",
    "    # Load the log file\n",
    "    with open(training_log, \"r\") as f:\n",
    "        history = json.load(f)\n",
    "\n",
    "    # Plot loss\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history[\"epoch\"], history[\"train_loss\"], label=\"Training Loss\")\n",
    "    plt.plot(history[\"epoch\"], history[\"val_loss\"], label=\"Validation Loss\")\n",
    "    plt.title(\"Loss per Epoch\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history[\"epoch\"], history[\"train_accuracy\"], label=\"Training Accuracy\")\n",
    "    plt.plot(history[\"epoch\"], history[\"val_accuracy\"], label=\"Validation Accuracy\")\n",
    "    plt.title(\"Accuracy per Epoch\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_graphs(training_log_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IuvJIudufmdw"
   },
   "source": [
    "# Step 4: Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ByeseFDbfmdw",
    "outputId": "59a193a8-02bb-454a-b301-e0906c000d86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[  47    8   51  233 1195]\n",
      " [   0    0    0    0    0]\n",
      " [   0    0    0    0    0]\n",
      " ...\n",
      " [   0    0    0    0    0]\n",
      " [   0    0    0    0    0]\n",
      " [   0    0    0    0    0]], shape=(500, 5), dtype=int32)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step\n",
      "[[3.1016039e-02 1.6379152e-01 2.3748980e-01 ... 3.1037180e-02\n",
      "  1.8597090e-01 1.3207725e-01]\n",
      " [1.8049697e-03 2.5665307e-02 2.4951806e-02 ... 4.0975236e-05\n",
      "  7.0504652e-04 2.0696587e-04]\n",
      " [1.8049697e-03 2.5665307e-02 2.4951806e-02 ... 4.0975236e-05\n",
      "  7.0504652e-04 2.0696587e-04]\n",
      " ...\n",
      " [1.8049699e-03 2.5665309e-02 2.4951806e-02 ... 4.0975236e-05\n",
      "  7.0504652e-04 2.0696588e-04]\n",
      " [1.8049706e-03 2.5665324e-02 2.4951816e-02 ... 4.0975232e-05\n",
      "  7.0504716e-04 2.0696585e-04]\n",
      " [1.8049706e-03 2.5665324e-02 2.4951816e-02 ... 4.0975232e-05\n",
      "  7.0504710e-04 2.0696584e-04]]\n",
      "Predicted class: Personality disorder\n"
     ]
    }
   ],
   "source": [
    "rawtext_test = [\"i'm really nervous\"]\n",
    "sequence_test = padding_func(tf.data.Dataset.from_tensors(tokenizer.tokenize(rawtext_test)))\n",
    "for element in sequence_test:\n",
    "    print(element)\n",
    "predictions = model.predict(sequence_test)\n",
    "\n",
    "# predictions will be a numpy array of shape (1, num_classes) with probabilities for each class\n",
    "print(predictions)\n",
    "\n",
    "# To get the predicted class index\n",
    "predicted_class_index = tf.argmax(predictions, axis=1).numpy()[0]\n",
    "print(f\"Predicted class: {label_encoder.get_vocabulary()[predicted_class_index]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VcaCRQZcfmdx"
   },
   "source": [
    "# Step 5: Done, save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y-L64a4Kfmdx"
   },
   "outputs": [],
   "source": [
    "# Run this if you happy with the model\n",
    "with open(data_dir+\"/label_vocabulary.txt\", \"w\") as f:\n",
    "    for label in label_encoder.get_vocabulary():\n",
    "        f.write(label + \"\\n\")\n",
    "\n",
    "model.save(data_dir+'/second_iteration.keras')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
