{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/HealLink/ML-Model/blob/ryan's-experiment/notebook_bert_test2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "qx4oxDctftL6",
    "outputId": "d76ffec5-45a9-407d-973d-99a62695886c"
   },
   "outputs": [],
   "source": [
    "# !pip install tensorflow[and-cuda]\n",
    "%pip install -U \"tensorflow-text==2.13.*\"\n",
    "%pip install \"tf-models-official==2.13.*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "WrDbvi-tfmdl"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-08 12:15:25.458047: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1733634925.524935  422101 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1733634925.547047  422101 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-08 12:15:25.708960: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Import-import\n",
    "import os\n",
    "os.environ['TF_USE_LEGACY_KERAS'] = '1'\n",
    "os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'\n",
    "\n",
    "import shutil\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from official.nlp import optimization  # to create AdamW optimizer\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.model_selection\n",
    "import sklearn.utils\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hFZhZpL0ZZGL",
    "outputId": "9c5ac481-9f76-4782-cfd0-bf6f3ef98c0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lT1x77TTfmdm",
    "outputId": "838c36b8-4761-4e44-ca62-258f1bccbd4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOT in Colab\n"
     ]
    }
   ],
   "source": [
    "if os.getenv(\"COLAB_RELEASE_TAG\"):\n",
    "   print(\"Running in Colab\")\n",
    "   IN_COLAB = True\n",
    "else:\n",
    "   print(\"NOT in Colab\")\n",
    "   IN_COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RwL6HScQfmdm",
    "outputId": "4948b5fb-2411-4c97-bd9c-ce7eda77a411"
   },
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    # Load the Drive helper and mount\n",
    "    from google.colab import drive # type: ignore\n",
    "    drive.mount('/content/drive')\n",
    "    data_dir = \"drive/MyDrive/data\"\n",
    "else:\n",
    "    data_dir = \"../data\"\n",
    "if not os.path.exists(data_dir):\n",
    "    os.mkdir(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LwFY2EZEfmdn",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Step 1: Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TaFX1wv1fmdn",
    "outputId": "63411d86-f336-427d-d8a2-0837e18eb796"
   },
   "outputs": [],
   "source": [
    "# Download raw dataset\n",
    "!wget -O {data_dir+\"/combined_data.csv\"} \"https://drive.google.com/uc?export=download&id=1GJn2kEIBgto2OyD7-h2HQOv_NJUriqJh\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yhYl6YGQfmdo"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_dir+\"/combined_data.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "xMZcubI5fmdo",
    "outputId": "f3dd057b-cdab-4c03-e1e6-0221d538b712"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zz0XrQghfmdo",
    "outputId": "0c5228d8-cbcb-481c-e507-793d432e729b"
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wycr9Gj1fmdo"
   },
   "source": [
    "### Step 1a: Basic cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YbpOTg4xfmdo",
    "outputId": "03e33ba7-fec3-402f-d4b6-947d12147525"
   },
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(f\"Number of rows with missing values: {df.isnull().sum()}\")\n",
    "\n",
    "# Check for duplicates\n",
    "print(f\"Number of duplicate rows: {df.duplicated(subset=['statement']).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F-nz9lIHfmdp"
   },
   "outputs": [],
   "source": [
    "# Drop rows that contain empty values\n",
    "df = df.dropna()\n",
    "\n",
    "# Drop rows that contain duplicate values in the ‘statement’ column and keep only the first row\n",
    "df = df.drop_duplicates(subset=['statement'], keep='first')\n",
    "\n",
    "df.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bMxMHCd3fmdp",
    "outputId": "d92ca87b-0a67-47bc-b6da-47c00644196b"
   },
   "outputs": [],
   "source": [
    "# Recheck for missing values\n",
    "print(f\"Number of rows with missing values: {df.isnull().sum()}\")\n",
    "\n",
    "# Recheck for duplicates\n",
    "print(f\"Number of duplicate rows: {df.duplicated(subset=['statement']).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ihQ54ThCfmdp"
   },
   "source": [
    "### Step 1b: Deep cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sclv0Apifmdp"
   },
   "outputs": [],
   "source": [
    "# Change the data type of ‘statement’ and ‘status’ columns to string\n",
    "df = df.astype({\"statement\":str, \"status\":str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zstHlPs8fmdp"
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    # remove stopwords\n",
    "    # for word in stopwords:\n",
    "    #     if word[0] == \"'\":\n",
    "    #         text = re.sub(rf\"{word}\\b\", \"\", text)\n",
    "    #     else:\n",
    "    #         text = re.sub(rf\"\\b{word}\\b\", \"\", text)\n",
    "\n",
    "    # text = re.sub(r'[!\"“’#$%&()\\*\\+,-\\./:;<=>?@\\[\\\\\\]^_`{|}~\\']', ' ', text) # remove punctuation mark\n",
    "    # text = re.sub(emoj, ' ', text) # remove emoji\n",
    "    text = re.sub(r'\\d+', ' ', text) # remove number\n",
    "    # text = re.sub(r'(.)\\1+', r'\\1', text) # remove repeated character\n",
    "    # text = re.sub(r' [a-z] ', ' ', text) # remove single character\n",
    "    text = re.sub(r\"(https?:\\/\\/)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})([\\/\\w\\.-]*)\", \" \", text) # remove links\n",
    "    text = re.sub(r'\\s+', ' ', text) # remove multiple spaces\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E8RHcyfIfmdq"
   },
   "outputs": [],
   "source": [
    "# CLEAN!!!\n",
    "df['statement'] = df['statement'].apply(clean_text)\n",
    "df = df[df['statement'] != \"\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aDcqaCdIfmdq"
   },
   "source": [
    "### Step 1c: Very deep cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "id": "_6A82beAfmdq",
    "outputId": "023966b5-55ee-4414-edb0-8823ab88d2bc"
   },
   "outputs": [],
   "source": [
    "# Data distribution analysis of each label\n",
    "df.status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PAHh2u0Dfmdq"
   },
   "outputs": [],
   "source": [
    "# Adding word count column for further analysis\n",
    "df['word_count'] = df['statement'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kewGXeGufmdq"
   },
   "outputs": [],
   "source": [
    "# Define bins and labels for word count ranges\n",
    "bins = [0, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, float('inf')]  # Adjust as needed\n",
    "labels = ['1-100', '101-200', '201-300', '301-400', '401-500', '501-600', '601-700', '701-800', '801-900', '901-1000', '+1000']\n",
    "\n",
    "# Add a column to categorize statements into ranges\n",
    "df['word_count_range'] = pd.cut(df['word_count'], bins=bins, labels=labels, right=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 460
    },
    "id": "bK2xBB8mfmdq",
    "outputId": "c59185e6-7e11-4a2a-9b1a-1c97995c9faf"
   },
   "outputs": [],
   "source": [
    "# Count the number of statements in each range\n",
    "df['word_count_range'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 480
    },
    "id": "An_5QsLCfmdr",
    "outputId": "a24cb5f4-65b9-41f4-a733-c21c4858a635"
   },
   "outputs": [],
   "source": [
    "# Group by word count range and label, then count occurrences\n",
    "df.groupby(['word_count_range', 'status']).size().unstack(fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "id": "lAY7fYq2fmdr",
    "outputId": "273a0dc0-6768-4d04-f98e-f8f4ed221de3"
   },
   "outputs": [],
   "source": [
    "df_correct_size = df[(df['word_count'] >= 10) & (df['word_count'] <= 192)].reset_index(drop=True)\n",
    "df_correct_size.status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long_statement = df[(df['word_count'] > 200)].reset_index(drop=True)\n",
    "df_long_statement.status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window_split(df, max_length, overlap_ratio=0.5):    \n",
    "    def split_sentence(sentence, max_length):\n",
    "        words = sentence.split()  # Split by spaces, assuming words are separated by spaces\n",
    "        chunks = []\n",
    "        step = int(max_length * (1 - overlap_ratio))  # Calculate the step size based on overlap ratio\n",
    "        \n",
    "        # Slide over the sentence in chunks of max_length with overlap\n",
    "        for i in range(0, len(words), step):\n",
    "            chunk = words[i:i + max_length]\n",
    "            chunks.append(' '.join(chunk))  # Join back to a sentence\n",
    "            if i + max_length >= len(words):  # Exit condition if the chunk exceeds the sentence\n",
    "                break\n",
    "        return chunks\n",
    "\n",
    "    expanded_rows = []\n",
    "\n",
    "    # Iterate over the original DataFrame\n",
    "    for idx, row in df.iterrows():\n",
    "        sentence = row['statement']\n",
    "        label = row['status']\n",
    "        \n",
    "        # Split the sentence into smaller chunks\n",
    "        chunks = split_sentence(sentence, max_length, )\n",
    "        \n",
    "        # Create new rows for each chunk\n",
    "        for chunk in chunks:\n",
    "            expanded_rows.append({'statement': chunk, 'status': label})\n",
    "    \n",
    "    # Return the expanded dataframe\n",
    "    return pd.DataFrame(expanded_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_export_candidate = sliding_window_split(df, 192, 0.5)\n",
    "df_export_candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oFB19IUHfmdr",
    "outputId": "4a0e8d05-d019-4ba6-9320-a2f76905357f"
   },
   "outputs": [],
   "source": [
    "# Count the number of examples for each label\n",
    "label_counts = df_export_candidate['status'].value_counts()\n",
    "\n",
    "# Find the label with the minimum count\n",
    "min_label = label_counts.idxmin()\n",
    "min_count = label_counts.min()\n",
    "\n",
    "print(f\"Label with the lowest number of examples: {min_label}\")\n",
    "print(f\"Number of examples: {min_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "id": "yW2R5Fehfmdr",
    "outputId": "59637ac4-cc87-4176-bdfe-858eb7a32d9d"
   },
   "outputs": [],
   "source": [
    "df_export_candidate.status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hXaIhmwafmds"
   },
   "outputs": [],
   "source": [
    "# Optional, export the cleaned dataset\n",
    "df_export_candidate.to_csv(data_dir+'/cleaned_data_10-192_links_number_removed_sliding50percent.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df\n",
    "del df_correct_size\n",
    "del df_long_statement\n",
    "del df_export_candidate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "94tE6LPnfmds"
   },
   "source": [
    "# Step 2: Prepare model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "OeooFwA3fmds"
   },
   "outputs": [],
   "source": [
    "CLASS_NUM = 7\n",
    "\n",
    "SEQ_LENGTH = 256\n",
    "TRAINING_RATIO = 0.8\n",
    "BATCH_SIZE = 8\n",
    "SEED = 45\n",
    "\n",
    "MODEL_DROPOUT_RATE = 0.2\n",
    "MODEL_L2_REGULARIZER = 0.02\n",
    "\n",
    "EPOCHS = 5\n",
    "INIT_LR = 2e-5\n",
    "WARM_UP_RATIO = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2a: Choosing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UBzuTA9T06nD",
    "outputId": "18fde0a7-ba56-4a29-eb8e-0fb407268a62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT model selected           : https://www.kaggle.com/models/tensorflow/bert/TensorFlow2/bert-en-uncased-l-12-h-768-a-12/2\n",
      "Preprocess model auto-selected: https://kaggle.com/models/tensorflow/bert/TensorFlow2/en-uncased-preprocess/3\n"
     ]
    }
   ],
   "source": [
    "bert_model_name = 'bert-en-uncased-l-12-h-768-a-12'\n",
    "tfhub_handle_encoder = \"https://www.kaggle.com/models/tensorflow/bert/TensorFlow2/bert-en-uncased-l-12-h-768-a-12/2\"\n",
    "tfhub_handle_preprocess = \"https://kaggle.com/models/tensorflow/bert/TensorFlow2/en-uncased-preprocess/3\"\n",
    "\n",
    "print(f'BERT model selected           : {tfhub_handle_encoder}')\n",
    "print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2b: BERT Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Ne-WJSJ3ZZGV"
   },
   "outputs": [],
   "source": [
    "def make_bert_preprocess_model(tfhub_handle_preprocess, sentence_features, seq_length=SEQ_LENGTH):\n",
    "  \"\"\"Returns Model mapping string features to BERT inputs.\n",
    "\n",
    "  Args:\n",
    "    sentence_features: a list with the names of string-valued features.\n",
    "    seq_length: an integer that defines the sequence length of BERT inputs.\n",
    "\n",
    "  Returns:\n",
    "    A Keras Model that can be called on a list or dict of string Tensors\n",
    "    (with the order or names, resp., given by sentence_features) and\n",
    "    returns a dict of tensors for input to BERT.\n",
    "  \"\"\"\n",
    "\n",
    "  input_segments = [\n",
    "      tf.keras.layers.Input(shape=(), dtype=tf.string, name=ft)\n",
    "      for ft in sentence_features]\n",
    "\n",
    "  # Tokenize the text to word pieces.\n",
    "  bert_preprocess = hub.load(tfhub_handle_preprocess)\n",
    "  tokenizer = hub.KerasLayer(bert_preprocess.tokenize, name='tokenizer')\n",
    "  segments = [tokenizer(s) for s in input_segments]\n",
    "\n",
    "  # Optional: Trim segments in a smart way to fit seq_length.\n",
    "  # Simple cases (like this example) can skip this step and let\n",
    "  # the next step apply a default truncation to approximately equal lengths.\n",
    "  truncated_segments = segments\n",
    "\n",
    "  # Pack inputs. The details (start/end token ids, dict of output tensors)\n",
    "  # are model-dependent, so this gets loaded from the SavedModel.\n",
    "  packer = hub.KerasLayer(bert_preprocess.bert_pack_inputs,\n",
    "                          arguments=dict(seq_length=seq_length),\n",
    "                          name='packer')\n",
    "  model_inputs = packer(truncated_segments)\n",
    "  return tf.keras.Model(input_segments, model_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1733634933.428877  422101 gpu_process_state.cc:201] Using CUDA malloc Async allocator for GPU: 0\n",
      "I0000 00:00:1733634933.433276  422101 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1753 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Ti Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys           :  ['input_type_ids', 'input_mask', 'input_word_ids']\n",
      "Shape Word Ids :  (1, 256)\n",
      "Word Ids       :  tf.Tensor(\n",
      "[ 101 2070 6721 3231 6251  102 2178 6251  102    0    0    0    0    0\n",
      "    0    0], shape=(16,), dtype=int32)\n",
      "Shape Mask     :  (1, 256)\n",
      "Input Mask     :  tf.Tensor([1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0], shape=(16,), dtype=int32)\n",
      "Shape Type Ids :  (1, 256)\n",
      "Type Ids       :  tf.Tensor([0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0], shape=(16,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "test_preprocess_model = make_bert_preprocess_model(tfhub_handle_preprocess, ['my_input1', 'my_input2'])\n",
    "test_text = [np.array(['some random test sentence']),\n",
    "             np.array(['another sentence'])]\n",
    "text_preprocessed = test_preprocess_model(test_text)\n",
    "\n",
    "print('Keys           : ', list(text_preprocessed.keys()))\n",
    "print('Shape Word Ids : ', text_preprocessed['input_word_ids'].shape)\n",
    "print('Word Ids       : ', text_preprocessed['input_word_ids'][0, :16])\n",
    "print('Shape Mask     : ', text_preprocessed['input_mask'].shape)\n",
    "print('Input Mask     : ', text_preprocessed['input_mask'][0, :16])\n",
    "print('Shape Type Ids : ', text_preprocessed['input_type_ids'].shape)\n",
    "print('Type Ids       : ', text_preprocessed['input_type_ids'][0, :16])\n",
    "\n",
    "del test_preprocess_model\n",
    "del test_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2c: Defining classifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "6_9zJLhA2YZQ"
   },
   "outputs": [],
   "source": [
    "@tf.keras.utils.register_keras_serializable('capstone')\n",
    "class Classifier(tf.keras.Model):\n",
    "  def __init__(self, tfhub_handle_encoder, num_classes, dropout_rate, l2):\n",
    "    super(Classifier, self).__init__(name=\"prediction\")\n",
    "    self.num_classes = num_classes\n",
    "    self.dropout_rate = dropout_rate\n",
    "    self.l2 = l2\n",
    "    self.tfhub_handle_encoder = tfhub_handle_encoder\n",
    "    self.encoder = hub.KerasLayer(self.tfhub_handle_encoder, trainable=True)\n",
    "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "    self.dense = tf.keras.layers.Dense(num_classes, kernel_regularizer=tf.keras.regularizers.l2(l2))\n",
    "\n",
    "  def call(self, preprocessed_text):\n",
    "    encoder_outputs = self.encoder(preprocessed_text)\n",
    "    pooled_output = encoder_outputs[\"pooled_output\"]\n",
    "    x = self.dropout(pooled_output)\n",
    "    x = self.dense(x)\n",
    "    return x\n",
    "  \n",
    "  def get_config(self):\n",
    "    config = super(Classifier, self).get_config()\n",
    "    config.update({\n",
    "        \"num_classes\": self.num_classes,\n",
    "        \"dropout_rate\": self.dropout_rate,\n",
    "        \"l2\": self.l2,\n",
    "        \"tfhub_handle_encoder\": self.tfhub_handle_encoder\n",
    "    })\n",
    "    return config\n",
    "  \n",
    "  @classmethod\n",
    "  def from_config(cls, config):\n",
    "    return cls(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bd8A6_xI2afq",
    "outputId": "d57ae791-1690-4402-c082-1c2ec356641f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the output: (1, 2)\n",
      "tf.Tensor([[ 0.4686162  -0.00454919]], shape=(1, 2), dtype=float32)\n",
      "tf.Tensor([[0.61505616 0.4988627 ]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "test_classifier_model = Classifier(tfhub_handle_encoder, 2, 0.1, 0.01)\n",
    "bert_raw_result = test_classifier_model(text_preprocessed)\n",
    "\n",
    "print(f'Shape of the output: {bert_raw_result.shape}')\n",
    "print(bert_raw_result)\n",
    "print(tf.sigmoid(bert_raw_result))\n",
    "\n",
    "del test_classifier_model\n",
    "del bert_raw_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NRcAy15CcNBa"
   },
   "source": [
    "# Step 3: Preprocess dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3a: Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "YWoCiB0Nfmds"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There're 33305 sentences\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "labels = []\n",
    "\n",
    "with open(data_dir+'/cleaned_data_10-192_imbalanced_links_number_removed_no_sliding.csv', 'r') as csvfile:\n",
    "    heading = next(csvfile)\n",
    "    reader_obj = csv.reader(csvfile)\n",
    "    for row in reader_obj:\n",
    "        labels.append(row[1])\n",
    "        sentences.append(row[0])\n",
    "\n",
    "print(f\"There're {len(sentences)} sentences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-_D7MDhcfmds"
   },
   "source": [
    "## Step 3b: Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences, val_test_sentences, train_labels, val_test_labels = sklearn.model_selection.train_test_split(\n",
    "    sentences, labels,\n",
    "    train_size=TRAINING_RATIO,\n",
    "    stratify=labels,\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "val_sentences, test_sentences, val_labels, test_labels = sklearn.model_selection.train_test_split(\n",
    "    val_test_sentences, val_test_labels,\n",
    "    train_size=0.5,\n",
    "    stratify=val_test_labels,\n",
    "    random_state=SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3c: Encode label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['Anxiety' 'Bipolar' 'Depression' 'Normal' 'Personality disorder' 'Stress'\n",
      " 'Suicidal']\n",
      "Class Weights: {0: 1.942973820462335, 1: 2.7844079841153726, 2: 0.45644390385966116, 3: 0.5755762459225335, 4: 8.150504741511165, 5: 2.338013338013338, 6: 0.6069663074925393}\n",
      "Class mapping: {'Anxiety': 0, 'Bipolar': 1, 'Depression': 2, 'Normal': 3, 'Personality disorder': 4, 'Stress': 5, 'Suicidal': 6}\n"
     ]
    }
   ],
   "source": [
    "classes = np.unique(train_labels)\n",
    "class_weights = sklearn.utils.class_weight.compute_class_weight(class_weight='balanced', classes=classes, y=train_labels)\n",
    "class_weight_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
    "class_to_index = {label: index for index, label in enumerate(classes)}\n",
    "\n",
    "train_labels_encoded = np.array([class_to_index[label] for label in train_labels])\n",
    "val_labels_encoded = np.array([class_to_index[label] for label in val_labels])\n",
    "test_labels_encoded = np.array([class_to_index[label] for label in test_labels])\n",
    "\n",
    "print(\"Classes:\", classes)\n",
    "print(\"Class Weights:\", class_weight_dict)\n",
    "print(\"Class mapping:\", class_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3d: create tf.data.Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 26644 sentence-label pairs for training.\n",
      "There are 3330 sentence-label pairs for validation.\n",
      "There are 3330 sentence-label pairs for test.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_sentences, train_labels_encoded))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_sentences, val_labels_encoded))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((val_sentences, val_labels_encoded))\n",
    "\n",
    "train_dataset = train_dataset.map(lambda x, y : (x, tf.one_hot(y, depth=CLASS_NUM)))\n",
    "val_dataset = val_dataset.map(lambda x, y : (x, tf.one_hot(y, depth=CLASS_NUM)))\n",
    "test_dataset = test_dataset.map(lambda x, y : (x, tf.one_hot(y, depth=CLASS_NUM)))\n",
    "\n",
    "print(f\"There are {train_dataset.cardinality()} sentence-label pairs for training.\")\n",
    "print(f\"There are {val_dataset.cardinality()} sentence-label pairs for validation.\")\n",
    "print(f\"There are {test_dataset.cardinality()} sentence-label pairs for test.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3e: optimize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "mrvcdFOIhY60"
   },
   "outputs": [],
   "source": [
    "# Optimize the datasets for training\n",
    "train_dataset_batched = (train_dataset\n",
    "                       .shuffle(train_dataset.cardinality(), seed=SEED)\n",
    "                       .cache()\n",
    "                       .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "                       .batch(BATCH_SIZE)\n",
    "                       )\n",
    "\n",
    "val_dataset_batched = (val_dataset\n",
    "                      .cache()\n",
    "                      .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "                      .batch(BATCH_SIZE)\n",
    "                      )\n",
    "\n",
    "test_dataset_batched = (test_dataset\n",
    "                      .cache()\n",
    "                      .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "                      .batch(BATCH_SIZE)\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3c: preprocess statement using bert_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0HXaQqNBfgyd",
    "outputId": "bffdb82e-cfa7-4695-d258-34ba754eca14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'i have been hiding in a cave within myself, it is been a few years since i have no friends, i walked away from everything and everyone, i think this was a mechanism of defense. i became a coward, i thought nothing scared me, but one thing scares me a shit: the pain, and nowadays i am so weak that even before i feel pain i think about suicide to relieve me. i think nietzsche was right when he said, \"the idea of suicide is a big one consolation, helps to apply many bad nights\". ** under the cave**', shape=(), dtype=string)\n",
      "tf.Tensor([0. 0. 0. 0. 0. 0. 1.], shape=(7,), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-08 12:15:44.259971: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "for statement, label in train_dataset.take(1):\n",
    "    print(statement)\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "Ts1DsjYXcSEx"
   },
   "outputs": [],
   "source": [
    "bert_preprocess_model = make_bert_preprocess_model(tfhub_handle_preprocess, ['sentence'], SEQ_LENGTH)\n",
    "\n",
    "train_dataset_final = train_dataset_batched.map(lambda text, label: (bert_preprocess_model(text), label))\n",
    "val_dataset_final = val_dataset_batched.map(lambda text, label: (bert_preprocess_model(text), label))\n",
    "test_dataset_final = test_dataset_batched.map(lambda text, label: (bert_preprocess_model(text), label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete previous variable\n",
    "del sentences\n",
    "del train_sentences\n",
    "del val_test_sentences\n",
    "del val_sentences\n",
    "del test_sentences\n",
    "\n",
    "del labels\n",
    "del train_labels\n",
    "del val_test_labels\n",
    "del val_labels\n",
    "del test_labels\n",
    "\n",
    "del train_dataset\n",
    "del val_dataset\n",
    "del test_dataset\n",
    "\n",
    "del train_dataset_batched\n",
    "del val_dataset_batched\n",
    "del test_dataset_batched"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gmBrP0z0fmdv"
   },
   "source": [
    "# Step 4: How To Train Your ~~Dragon~~ ML Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4a: preparing classifier_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.04075588 0.16310142 0.4962448  0.15984029 0.07474155 0.04117482\n",
      "  0.02414133]], shape=(1, 7), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "classifier_model = Classifier(tfhub_handle_encoder, CLASS_NUM, MODEL_DROPOUT_RATE, MODEL_L2_REGULARIZER)\n",
    "bert_raw_result = classifier_model(text_preprocessed)\n",
    "print(tf.nn.softmax(bert_raw_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.keras.utils.register_keras_serializable('capstone')\n",
    "class MatthewsCorrelationCoefficient(tf.keras.metrics.Metric):\n",
    "    def __init__(self, num_classes, dtype=None, name=\"matthews_correlation_coefficient\"):\n",
    "        super(MatthewsCorrelationCoefficient, self).__init__(name=name)\n",
    "        self.num_classes = num_classes\n",
    "        self.c = self.add_weight(name=\"total_correct_predicted\", initializer=\"zeros\", dtype=tf.float32)\n",
    "        self.s = self.add_weight(name=\"total_samples\", initializer=\"zeros\", dtype=tf.float32)\n",
    "        self.t = self.add_weight(name=\"num_true\", shape=(num_classes,), initializer=\"zeros\", dtype=tf.float32)\n",
    "        self.p = self.add_weight(name=\"num_pred\", shape=(num_classes,), initializer=\"zeros\", dtype=tf.float32)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        # Convert one-hot labels to integer labels\n",
    "        if len(y_true.shape) > 1:\n",
    "            y_true = tf.argmax(y_true, axis=-1)\n",
    "        if len(y_pred.shape) > 1:\n",
    "            y_pred = tf.argmax(y_pred, axis=-1)\n",
    "        \n",
    "        # Total number of samples\n",
    "        new_s = tf.cast(tf.size(y_true), tf.float32)\n",
    "        # Total number of correctly predicted labels\n",
    "        new_c = tf.reduce_sum(tf.cast(tf.equal(y_true, y_pred), tf.float32))\n",
    "        \n",
    "        # Update state\n",
    "        self.s.assign(tf.add(self.s, new_s))\n",
    "        self.c.assign(tf.add(self.c, new_c))\n",
    "\n",
    "        for k in range(self.num_classes):\n",
    "            k = tf.cast(k, y_true.dtype)\n",
    "            tk = tf.reduce_sum(tf.cast(tf.equal(y_true, k), tf.float32))\n",
    "            pk = tf.reduce_sum(tf.cast(tf.equal(y_pred, k), tf.float32))\n",
    "            self.t[k].assign(tf.add(self.t[k], tk))\n",
    "            self.p[k].assign(tf.add(self.p[k], pk))\n",
    "    \n",
    "    def result(self):\n",
    "        num = self.c * self.s - tf.reduce_sum(self.t * self.p)\n",
    "        denom_t = tf.reduce_sum(self.t * self.t)\n",
    "        denom_p = tf.reduce_sum(self.p * self.p)\n",
    "        denom = tf.sqrt((self.s**2 - denom_t) * (self.s**2 - denom_p))\n",
    "        \n",
    "        mcc = tf.divide(num, denom + 1e-6)\n",
    "        return mcc\n",
    "    \n",
    "    def reset_state(self):\n",
    "        self.c.assign(0)\n",
    "        self.s.assign(0)\n",
    "        self.t.assign(tf.zeros_like(self.t))\n",
    "        self.p.assign(tf.zeros_like(self.p))\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(MatthewsCorrelationCoefficient, self).get_config()\n",
    "        config.update({\n",
    "            \"num_classes\": self.num_classes\n",
    "            })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dlRw8RmI3Gzs"
   },
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.CategoricalFocalCrossentropy(from_logits=True)\n",
    "metrics = [tf.keras.metrics.CategoricalAccuracy()]\n",
    "\n",
    "steps_per_epoch = tf.data.experimental.cardinality(train_dataset_final).numpy()\n",
    "num_train_steps = steps_per_epoch * EPOCHS\n",
    "num_warmup_steps = int(WARM_UP_RATIO * num_train_steps)\n",
    "\n",
    "optimizer = optimization.create_optimizer(\n",
    "        init_lr=INIT_LR,\n",
    "        num_train_steps=num_train_steps,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        optimizer_type='adamw'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "Ls6cy0yT3Wm3"
   },
   "outputs": [],
   "source": [
    "classifier_model.compile(optimizer='adam',\n",
    "                         loss='categorical_crossentropy',\n",
    "                         metrics='categorical_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"prediction\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " keras_layer_1 (KerasLayer)  multiple                  109482241 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         multiple                  0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             multiple                  5383      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 109487624 (417.66 MB)\n",
      "Trainable params: 109487623 (417.66 MB)\n",
      "Non-trainable params: 1 (1.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classifier_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NG8hLpP5fmdv",
    "outputId": "4918ee8a-75c5-43b4-9b2a-7878ee50b1e8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-08 12:16:49.279633: W tensorflow/core/kernels/data/cache_dataset_ops.cc:914] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions have shape: (8, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-08 12:16:51.450938: W tensorflow/core/kernels/data/cache_dataset_ops.cc:914] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "# Check model compatibility\n",
    "example_batch = train_dataset_final.take(1)\n",
    "\n",
    "try:\n",
    "\tclassifier_model.evaluate(example_batch, verbose=False)\n",
    "except:\n",
    "\tprint(\"Your model is not compatible with the dataset you defined earlier. Check that the loss function and last layer are compatible with one another.\")\n",
    "else:\n",
    "\tpredictions = classifier_model.predict(example_batch, verbose=False)\n",
    "\tprint(f\"predictions have shape: {predictions.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t3H_70gmh941"
   },
   "source": [
    "## Step 4b: Preparing callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_checkpoint = data_dir+'/checkpoint'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run thif if you don't determine to load existing checkpoint\n",
    "checkpoint_path = base_checkpoint\n",
    "counter = 1\n",
    "\n",
    "while os.path.exists(checkpoint_path):\n",
    "    checkpoint_path = f\"{base_checkpoint}_{counter}\"\n",
    "    counter += 1\n",
    "\n",
    "os.makedirs(checkpoint_path)\n",
    "\n",
    "checkpoint_model_filepath = checkpoint_path+\"/checkpoint_{epoch:02d}.tf\"\n",
    "checkpoint_num_epoch_filepath = checkpoint_path+\"/current_epoch.txt\"\n",
    "training_log_filepath = checkpoint_path+\"/training_log.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this if you want to load certain checkpoint\n",
    "checkpoint_path = f\"{data_dir}/checkpoint_6\" # customize this line\n",
    "\n",
    "checkpoint_model_filepath = checkpoint_path+\"/checkpoint_{epoch:02d}.tf\"\n",
    "checkpoint_num_epoch_filepath = checkpoint_path+\"/current_epoch.txt\"\n",
    "training_log_filepath = checkpoint_path+\"/training_log.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1zGaikEDfmdv"
   },
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_model_filepath)\n",
    "\n",
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    patience=5, \n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "class CustomCheckpointCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, epoch_file, log_file):\n",
    "        self.epoch_file = epoch_file\n",
    "        self.log_file = log_file\n",
    "        self.history = {\"epoch\": [], \"loss\": [], \"categorical_accuracy\": [], \"matthews_correlation_coefficient\": [],\n",
    "                        \"val_loss\": [], \"val_categorical_accuracy\": [], \"val_matthews_correlation_coefficient\": []}\n",
    "\n",
    "        # Load history and starting epoch if they exist\n",
    "        if os.path.exists(self.log_file):\n",
    "            with open(self.log_file, 'r') as f:\n",
    "                self.history = json.load(f)\n",
    "\n",
    "        if os.path.exists(self.epoch_file):\n",
    "            with open(self.epoch_file, 'r') as f:\n",
    "                self.starting_epoch = int(f.read())\n",
    "        else:\n",
    "            with open(self.epoch_file, 'w') as f:\n",
    "                f.write(str(0))\n",
    "            self.starting_epoch = 0\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current_epoch = self.starting_epoch + epoch + 1\n",
    "        # Save the current epoch\n",
    "        with open(self.epoch_file, 'w') as f:\n",
    "            f.write(str(current_epoch))\n",
    "\n",
    "        # Save logs (loss, accuracy, etc.) for plotting\n",
    "        self.history[\"epoch\"].append(current_epoch)\n",
    "        self.history[\"loss\"].append(logs.get(\"loss\"))\n",
    "        self.history[\"categorical_accuracy\"].append(logs.get(\"categorical_accuracy\"))\n",
    "        self.history[\"matthews_correlation_coefficient\"].append(logs.get(\"matthews_correlation_coefficient\"))\n",
    "        self.history[\"val_loss\"].append(logs.get(\"val_loss\"))\n",
    "        self.history[\"val_categorical_accuracy\"].append(logs.get(\"val_categorical_accuracy\"))\n",
    "        self.history[\"val_matthews_correlation_coefficient\"].append(logs.get(\"val_matthews_correlation_coefficient\"))\n",
    "\n",
    "        # Save history to the log file\n",
    "        with open(self.log_file, 'w') as f:\n",
    "            json.dump(self.history, f, indent=4)\n",
    "\n",
    "custom_checkpoint_callback = CustomCheckpointCallback(checkpoint_num_epoch_filepath, training_log_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wSc1uib7fmdv",
    "outputId": "44cff1f9-e5f3-4908-a1b7-bbbb778dcfd1"
   },
   "outputs": [],
   "source": [
    "# Load checkpoint\n",
    "checkpoint_folder_name = \"checkpoint_7\"\n",
    "try:\n",
    "    with open(f\"{data_dir}/{checkpoint_folder_name}/current_epoch.txt\", 'r') as f:\n",
    "        start_epoch = int(f.read())\n",
    "    print(f\"Resuming training from epoch {start_epoch}\")\n",
    "except FileNotFoundError:\n",
    "    start_epoch = 0\n",
    "    print(\"No saved epoch found. Starting from epoch 0\")\n",
    "\n",
    "# Load saved model\n",
    "if start_epoch > 0:\n",
    "    classifier_model = tf.keras.models.load_model(f\"{data_dir}/{checkpoint_folder_name}/checkpoint_{start_epoch:02d}.tf\", custom_objects={\n",
    "                                                                                                \"Classifier\": Classifier, \n",
    "                                                                                                \"AdamWeightDecay\": optimization.AdamWeightDecay,\n",
    "                                                                                                \"WarmUp\": optimization.WarmUp,\n",
    "                                                                                                \"MatthewsCorrelationCoefficient\": MatthewsCorrelationCoefficient,\n",
    "                                                                                            })\n",
    "    print(f\"Loaded model from {checkpoint_folder_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z-xwb4hkiDKg"
   },
   "source": [
    "## Step 4c: TRAIN!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "id": "jYeszbOpfmdw",
    "outputId": "459b18a1-a39f-4aff-e334-f8526cb4d73b"
   },
   "outputs": [],
   "source": [
    "history = classifier_model.fit(\n",
    "    train_dataset_final,\n",
    "    epochs=EPOCHS,\n",
    "    initial_epoch=start_epoch,\n",
    "    validation_data=val_dataset_final,\n",
    "    class_weight=dict(enumerate(class_weights)),\n",
    "    callbacks=[\n",
    "        model_checkpoint_callback,\n",
    "        custom_checkpoint_callback,\n",
    "        early_stopping_callback\n",
    "    ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4d: EVALUATE!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cSwEMAVD7DVL",
    "outputId": "6d89403d-207b-4158-e93f-754e3525e148"
   },
   "outputs": [],
   "source": [
    "loss, mcc, accuracy = classifier_model.evaluate(test_dataset_final)\n",
    "\n",
    "print(f'Loss: {loss}')\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'MCC: {mcc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions on the test set\n",
    "y_pred_probs = classifier_model.predict(test_dataset_final)\n",
    "\n",
    "# Convert probabilities to class indices\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# Get true labels from the test dataset\n",
    "y_true = np.argmax(np.concatenate([y for x, y in test_dataset_final], axis=0), axis=-1)\n",
    "\n",
    "# Optional: Normalize the confusion matrix\n",
    "cm_normalized = confusion_matrix(y_true, y_pred, normalize='true')\n",
    "\n",
    "cm_normalized_short= np.around(cm_normalized.astype('float') / cm_normalized.sum(axis=1)[:, np.newaxis], decimals=3)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(cm_normalized, cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Labels')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.colorbar()\n",
    "\n",
    "# Add labels to each cell\n",
    "for i in range(cm_normalized_short.shape[0]):\n",
    "    for j in range(cm_normalized_short.shape[1]):\n",
    "        plt.text(j, i, cm_normalized_short[i, j], ha='center', va='center', color='red')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 957
    },
    "id": "TTR38rCrfmdw",
    "outputId": "0ad68241-5100-4ba0-81ee-ca57889d36c6"
   },
   "outputs": [],
   "source": [
    "def plot_graphs(training_log):\n",
    "    # Load the log file\n",
    "    with open(training_log, \"r\") as f:\n",
    "        history = json.load(f)\n",
    "    \n",
    "    # Plot loss\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history[\"epoch\"], history[\"loss\"], label=\"Training Loss\")\n",
    "    plt.plot(history[\"epoch\"], history[\"val_loss\"], label=\"Validation Loss\")\n",
    "    plt.title(\"Loss per Epoch\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history[\"epoch\"], history[\"matthews_correlation_coefficient\"], label=\"Training MCC\")\n",
    "    plt.plot(history[\"epoch\"], history[\"val_matthews_correlation_coefficient\"], label=\"Validation MCC\")\n",
    "    plt.title(\"MCC per Epoch\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"MCC\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history[\"epoch\"], history[\"categorical_accuracy\"], label=\"Training Accuracy\")\n",
    "    plt.plot(history[\"epoch\"], history[\"val_categorical_accuracy\"], label=\"Validation Accuracy\")\n",
    "    plt.title(\"Accuracy per Epoch\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_graphs(training_log_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IuvJIudufmdw"
   },
   "source": [
    "# Step 5: Save & Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_save_path = '../models'\n",
    "bert_type = tfhub_handle_encoder.split('/')[-2]\n",
    "saved_model_name = f'{\"test_model\"}_{bert_type}'\n",
    "\n",
    "saved_model_path = os.path.join(main_save_path, saved_model_name)\n",
    "preprocess_inputs = bert_preprocess_model.inputs\n",
    "bert_encoder_inputs = bert_preprocess_model(preprocess_inputs)\n",
    "bert_outputs = classifier_model(bert_encoder_inputs)\n",
    "model_for_export = tf.keras.Model(preprocess_inputs, bert_outputs)\n",
    "model_for_export.save(saved_model_path, include_optimizer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ByeseFDbfmdw",
    "outputId": "59a193a8-02bb-454a-b301-e0906c000d86"
   },
   "outputs": [],
   "source": [
    "rawtext_test = tf.constant([\"tomorrow is our presentation. i feel so nervous. what if something going wrong\"])\n",
    "predictions = model_for_export(rawtext_test)\n",
    "\n",
    "# predictions will be a numpy array of shape (1, num_classes) with probabilities for each class\n",
    "print(classes)\n",
    "print(tf.nn.softmax(predictions))\n",
    "\n",
    "# To get the predicted class index\n",
    "predicted_class_index = tf.argmax(predictions, axis=1).numpy()[0]\n",
    "print(f\"Predicted class: {classes[predicted_class_index]}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "capstone-bert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
