{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/HealLink/ML-Model/blob/ryan's-experiment/notebook_bert_test2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "qx4oxDctftL6",
    "outputId": "d76ffec5-45a9-407d-973d-99a62695886c"
   },
   "outputs": [],
   "source": [
    "# !pip install tensorflow[and-cuda]\n",
    "!pip install -U \"tensorflow-text==2.13.*\"\n",
    "!pip install \"tf-models-official==2.13.*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "EGRmbqkXZZGK",
    "outputId": "8a3df5ca-2d75-415f-b910-e21e58f56d23"
   },
   "outputs": [],
   "source": [
    "!pip3 install tf_keras==2.18\n",
    "!pip3 install --upgrade tensorflow-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "8KC6GScKZZGK"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_USE_LEGACY_KERAS'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "tt-4olqsZZGK"
   },
   "outputs": [],
   "source": [
    "os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "WrDbvi-tfmdl"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-01 12:47:19.258753: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1733032039.364504  390361 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1733032039.395036  390361 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-01 12:47:19.650228: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Import-import\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from official.nlp import optimization  # to create AdamW optimizer\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hFZhZpL0ZZGL",
    "outputId": "9c5ac481-9f76-4782-cfd0-bf6f3ef98c0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lT1x77TTfmdm",
    "outputId": "838c36b8-4761-4e44-ca62-258f1bccbd4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOT in Colab\n"
     ]
    }
   ],
   "source": [
    "if os.getenv(\"COLAB_RELEASE_TAG\"):\n",
    "   print(\"Running in Colab\")\n",
    "   IN_COLAB = True\n",
    "else:\n",
    "   print(\"NOT in Colab\")\n",
    "   IN_COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RwL6HScQfmdm",
    "outputId": "4948b5fb-2411-4c97-bd9c-ce7eda77a411"
   },
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    # Load the Drive helper and mount\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    data_dir = \"drive/MyDrive/data\"\n",
    "else:\n",
    "    data_dir = \"data\"\n",
    "if not os.path.exists(data_dir):\n",
    "    os.mkdir(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LwFY2EZEfmdn"
   },
   "source": [
    "# Step 1: Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TaFX1wv1fmdn",
    "outputId": "63411d86-f336-427d-d8a2-0837e18eb796"
   },
   "outputs": [],
   "source": [
    "# Download raw dataset\n",
    "!wget -O {data_dir+\"/combined_data.csv\"} \"https://drive.google.com/uc?export=download&id=1GJn2kEIBgto2OyD7-h2HQOv_NJUriqJh\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "usXqdLlefmdo"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yhYl6YGQfmdo"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_dir+\"/combined_data.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "xMZcubI5fmdo",
    "outputId": "f3dd057b-cdab-4c03-e1e6-0221d538b712"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zz0XrQghfmdo",
    "outputId": "0c5228d8-cbcb-481c-e507-793d432e729b"
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wycr9Gj1fmdo"
   },
   "source": [
    "### Step 1a: Basic cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YbpOTg4xfmdo",
    "outputId": "03e33ba7-fec3-402f-d4b6-947d12147525"
   },
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(f\"Number of rows with missing values: {df.isnull().sum()}\")\n",
    "\n",
    "# Check for duplicates\n",
    "print(f\"Number of duplicate rows: {df.duplicated(subset=['statement']).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F-nz9lIHfmdp"
   },
   "outputs": [],
   "source": [
    "# Drop rows that contain empty values\n",
    "df = df.dropna()\n",
    "\n",
    "# Drop rows that contain duplicate values in the ‘statement’ column and keep only the first row\n",
    "df = df.drop_duplicates(subset=['statement'], keep='first')\n",
    "\n",
    "df.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bMxMHCd3fmdp",
    "outputId": "d92ca87b-0a67-47bc-b6da-47c00644196b"
   },
   "outputs": [],
   "source": [
    "# Recheck for missing values\n",
    "print(f\"Number of rows with missing values: {df.isnull().sum()}\")\n",
    "\n",
    "# Recheck for duplicates\n",
    "print(f\"Number of duplicate rows: {df.duplicated(subset=['statement']).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ihQ54ThCfmdp"
   },
   "source": [
    "### Step 1b: Deep cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sclv0Apifmdp"
   },
   "outputs": [],
   "source": [
    "# Change the data type of ‘statement’ and ‘status’ columns to string\n",
    "df = df.astype({\"statement\":str, \"status\":str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qtbWv3cTfmdp"
   },
   "outputs": [],
   "source": [
    "emoj = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\"\n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "\n",
    "stopwords = [\"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zstHlPs8fmdp"
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    # remove stopwords\n",
    "    for word in stopwords:\n",
    "        if word[0] == \"'\":\n",
    "            text = re.sub(rf\"{word}\\b\", \"\", text)\n",
    "        else:\n",
    "            text = re.sub(rf\"\\b{word}\\b\", \"\", text)\n",
    "\n",
    "    text = re.sub(r'[!\"“’#$%&()\\*\\+,-\\./:;<=>?@\\[\\\\\\]^_`{|}~\\']', ' ', text) # remove punctuation mark\n",
    "    # text = re.sub(emoj, ' ', text) # remove emoji\n",
    "    text = re.sub(r'\\d+', ' ', text) # remove number\n",
    "    # text = re.sub(r'(.)\\1+', r'\\1', text) # remove repeated character\n",
    "    # text = re.sub(r' [a-z] ', ' ', text) # remove single character\n",
    "    text = re.sub(r'\\s+', ' ', text) # remove multiple spaces\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E8RHcyfIfmdq"
   },
   "outputs": [],
   "source": [
    "# CLEAN!!!\n",
    "df['statement'] = df['statement'].apply(clean_text)\n",
    "df = df[df['statement'] != \"\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aDcqaCdIfmdq"
   },
   "source": [
    "### Step 1c: Very deep cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "id": "_6A82beAfmdq",
    "outputId": "023966b5-55ee-4414-edb0-8823ab88d2bc"
   },
   "outputs": [],
   "source": [
    "# Data distribution analysis of each label\n",
    "df.status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PAHh2u0Dfmdq"
   },
   "outputs": [],
   "source": [
    "# Adding word count column for further analysis\n",
    "df['word_count'] = df['statement'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kewGXeGufmdq"
   },
   "outputs": [],
   "source": [
    "# Define bins and labels for word count ranges\n",
    "bins = [0, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, float('inf')]  # Adjust as needed\n",
    "labels = ['1-100', '101-200', '201-300', '301-400', '401-500', '501-600', '601-700', '701-800', '801-900', '901-1000', '+1000']\n",
    "\n",
    "# Add a column to categorize statements into ranges\n",
    "df['word_count_range'] = pd.cut(df['word_count'], bins=bins, labels=labels, right=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 460
    },
    "id": "bK2xBB8mfmdq",
    "outputId": "c59185e6-7e11-4a2a-9b1a-1c97995c9faf"
   },
   "outputs": [],
   "source": [
    "# Count the number of statements in each range\n",
    "df['word_count_range'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 480
    },
    "id": "An_5QsLCfmdr",
    "outputId": "a24cb5f4-65b9-41f4-a733-c21c4858a635"
   },
   "outputs": [],
   "source": [
    "# Group by word count range and label, then count occurrences\n",
    "df.groupby(['word_count_range', 'status']).size().unstack(fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "id": "lAY7fYq2fmdr",
    "outputId": "273a0dc0-6768-4d04-f98e-f8f4ed221de3"
   },
   "outputs": [],
   "source": [
    "df_export_candidate = df[(df['word_count'] >= 10) & (df['word_count'] <= 256)].reset_index(drop=True)\n",
    "df_export_candidate.status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oFB19IUHfmdr",
    "outputId": "4a0e8d05-d019-4ba6-9320-a2f76905357f"
   },
   "outputs": [],
   "source": [
    "# Count the number of examples for each label\n",
    "label_counts = df_export_candidate['status'].value_counts()\n",
    "\n",
    "# Find the label with the minimum count\n",
    "min_label = label_counts.idxmin()\n",
    "min_count = label_counts.min()\n",
    "\n",
    "print(f\"Label with the lowest number of examples: {min_label}\")\n",
    "print(f\"Number of examples: {min_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_export_candidate = df_export_candidate.sort_values(by='word_count', ascending=False)\n",
    "df_export_candidate = df_export_candidate.groupby('status').head(min_count)\n",
    "df_export_candidate.reset_index(drop=True, inplace=True)\n",
    "df_export_candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "id": "yW2R5Fehfmdr",
    "outputId": "59637ac4-cc87-4176-bdfe-858eb7a32d9d"
   },
   "outputs": [],
   "source": [
    "df_export_candidate.status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nMEVNuL2fmdr"
   },
   "outputs": [],
   "source": [
    "df_export_candidate.drop(['word_count', 'word_count_range'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hXaIhmwafmds"
   },
   "outputs": [],
   "source": [
    "# Optional, export the cleaned dataset\n",
    "df_export_candidate.to_csv(data_dir+'/cleaned_data_10-256_imbalanced_removed_stopwords_punc_num_space.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df\n",
    "del df_export_candidate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "94tE6LPnfmds"
   },
   "source": [
    "# Step 2: Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "OeooFwA3fmds"
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 10000\n",
    "MAX_LENGTH = 256\n",
    "TRAINING_SPLIT = 0.8\n",
    "BATCH_SIZE = 4\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "YWoCiB0Nfmds"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There're 36675 sentences\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "labels = []\n",
    "\n",
    "with open(data_dir+'/cleaned_data_10-256_imbalanced_removed_stopwords_punc_num_space.csv', 'r') as csvfile:\n",
    "    heading = next(csvfile)\n",
    "    reader_obj = csv.reader(csvfile)\n",
    "    for row in reader_obj:\n",
    "        labels.append(row[1])\n",
    "        sentences.append(row[0])\n",
    "\n",
    "print(f\"There're {len(sentences)} sentences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-_D7MDhcfmds"
   },
   "source": [
    "### Step 2a: Create tf.data.Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 29340 sentence-label pairs for training.\n",
      "There are 3667 sentence-label pairs for validation.\n",
      "There are 3667 sentence-label pairs for test.\n",
      "Classes: ['Anxiety' 'Bipolar' 'Depression' 'Normal' 'Personality disorder' 'Stress'\n",
      " 'Suicidal']\n",
      "Class Weights: {0: 1.7334278624601205, 1: 2.229483282674772, 2: 0.38432317728118204, 3: 1.006828866545417, 4: 6.302900107411386, 5: 2.336359292881032, 6: 0.5578158865356098}\n",
      "Class mapping: {'Anxiety': 0, 'Bipolar': 1, 'Depression': 2, 'Normal': 3, 'Personality disorder': 4, 'Stress': 5, 'Suicidal': 6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1733032051.752625  390361 gpu_process_state.cc:201] Using CUDA malloc Async allocator for GPU: 0\n",
      "I0000 00:00:1733032051.759360  390361 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1753 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Ti Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "import sklearn.model_selection\n",
    "import sklearn.utils\n",
    "\n",
    "train_sentences, val_test_sentences, train_labels, val_test_labels = sklearn.model_selection.train_test_split(\n",
    "    sentences, labels,\n",
    "    train_size=TRAINING_SPLIT,\n",
    "    stratify=labels,\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "val_sentences, test_sentences, val_labels, test_labels = sklearn.model_selection.train_test_split(\n",
    "    val_test_sentences, val_test_labels,\n",
    "    train_size=0.5,\n",
    "    stratify=val_test_labels,\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "classes = np.unique(train_labels)\n",
    "class_weights = sklearn.utils.class_weight.compute_class_weight(class_weight='balanced', classes=classes, y=train_labels)\n",
    "class_weight_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
    "class_to_index = {label: index for index, label in enumerate(classes)}\n",
    "\n",
    "train_labels_encoded = np.array([class_to_index[label] for label in train_labels])\n",
    "val_labels_encoded = np.array([class_to_index[label] for label in val_labels])\n",
    "test_labels_encoded = np.array([class_to_index[label] for label in test_labels])\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_sentences, train_labels_encoded))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_sentences, val_labels_encoded))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((val_sentences, val_labels_encoded))\n",
    "\n",
    "print(f\"There are {train_dataset.cardinality()} sentence-label pairs for training.\")\n",
    "print(f\"There are {val_dataset.cardinality()} sentence-label pairs for validation.\")\n",
    "print(f\"There are {test_dataset.cardinality()} sentence-label pairs for test.\")\n",
    "print(\"Classes:\", classes)\n",
    "print(\"Class Weights:\", class_weight_dict)\n",
    "print(\"Class mapping:\", class_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rhi60hT1fmdu"
   },
   "source": [
    "### Step 2d: Architect the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f4LEhBst0pdx"
   },
   "source": [
    "#### Step 2d-1: Loading model - choose model to fine tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UBzuTA9T06nD",
    "outputId": "18fde0a7-ba56-4a29-eb8e-0fb407268a62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT model selected           : https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1\n",
      "Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\n"
     ]
    }
   ],
   "source": [
    "bert_model_name = 'small_bert/bert_en_uncased_L-12_H-768_A-12'\n",
    "\n",
    "map_name_to_handle = {\n",
    "    'bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\n",
    "    'bert_en_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\n",
    "    'bert_multi_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',\n",
    "    'albert_en_base':\n",
    "        'https://tfhub.dev/tensorflow/albert_en_base/2',\n",
    "    'electra_small':\n",
    "        'https://tfhub.dev/google/electra_small/2',\n",
    "    'electra_base':\n",
    "        'https://tfhub.dev/google/electra_base/2',\n",
    "    'experts_pubmed':\n",
    "        'https://tfhub.dev/google/experts/bert/pubmed/2',\n",
    "    'experts_wiki_books':\n",
    "        'https://tfhub.dev/google/experts/bert/wiki_books/2',\n",
    "    'talking-heads_base':\n",
    "        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\n",
    "}\n",
    "\n",
    "map_model_to_preprocess = {\n",
    "    'bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'bert_en_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'bert_multi_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3',\n",
    "    'albert_en_base':\n",
    "        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n",
    "    'electra_small':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'electra_base':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'experts_pubmed':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'experts_wiki_books':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'talking-heads_base':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "}\n",
    "\n",
    "tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n",
    "tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n",
    "\n",
    "print(f'BERT model selected           : {tfhub_handle_encoder}')\n",
    "print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wAFvI01r1JQr"
   },
   "source": [
    "#### Step 2d-2: Preprocessing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m-V-zwM31dJH",
    "outputId": "d70c1e8d-bb91-4a8f-9c15-fa54b8eadfd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys       : ['input_type_ids', 'input_mask', 'input_word_ids']\n",
      "Shape      : (1, 128)\n",
      "Word Ids   : [  101  1045 10587  3280   102     0     0     0     0     0     0     0]\n",
      "Input Mask : [1 1 1 1 1 0 0 0 0 0 0 0]\n",
      "Type Ids   : [0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)\n",
    "text_test = ['i wanna die']\n",
    "text_preprocessed = bert_preprocess_model(text_test)\n",
    "\n",
    "print(f'Keys       : {list(text_preprocessed.keys())}')\n",
    "print(f'Shape      : {text_preprocessed[\"input_word_ids\"].shape}')\n",
    "print(f'Word Ids   : {text_preprocessed[\"input_word_ids\"][0, :12]}')\n",
    "print(f'Input Mask : {text_preprocessed[\"input_mask\"][0, :12]}')\n",
    "print(f'Type Ids   : {text_preprocessed[\"input_type_ids\"][0, :12]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S5sSC48L1rN2"
   },
   "source": [
    "#### Step 2d-3: Using BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b9DhpY2E1w4U",
    "outputId": "4b6ec903-db20-43be-a66a-d2a917950cb7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.RaggedTensor [[[7592], [23435, 12314], [999]]]>\n"
     ]
    }
   ],
   "source": [
    "bert_preprocess = hub.load(tfhub_handle_preprocess)\n",
    "tok = bert_preprocess.tokenize(tf.constant(['Hello TensorFlow!']))\n",
    "print(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oFQ4Z5XBZZGU",
    "outputId": "2d4a7586-87c3-43e4-d7c1-7f5390b20aa4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape Word Ids :  (1, 20)\n",
      "Word Ids       :  tf.Tensor(\n",
      "[  101  7592 23435 12314   999   102  7592 23435 12314   999   102     0\n",
      "     0     0     0     0], shape=(16,), dtype=int32)\n",
      "Shape Mask     :  (1, 20)\n",
      "Input Mask     :  tf.Tensor([1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0], shape=(16,), dtype=int32)\n",
      "Shape Type Ids :  (1, 20)\n",
      "Type Ids       :  tf.Tensor([0 0 0 0 0 0 1 1 1 1 1 0 0 0 0 0], shape=(16,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "text_preprocessed = bert_preprocess.bert_pack_inputs([tok, tok], tf.constant(20))\n",
    "\n",
    "print('Shape Word Ids : ', text_preprocessed['input_word_ids'].shape)\n",
    "print('Word Ids       : ', text_preprocessed['input_word_ids'][0, :16])\n",
    "print('Shape Mask     : ', text_preprocessed['input_mask'].shape)\n",
    "print('Input Mask     : ', text_preprocessed['input_mask'][0, :16])\n",
    "print('Shape Type Ids : ', text_preprocessed['input_type_ids'].shape)\n",
    "print('Type Ids       : ', text_preprocessed['input_type_ids'][0, :16])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v2tuOrgF2UE_"
   },
   "source": [
    "#### Step 2d-4: Defining model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Ne-WJSJ3ZZGV"
   },
   "outputs": [],
   "source": [
    "def make_bert_preprocess_model(sentence_features, seq_length=512):\n",
    "  \"\"\"Returns Model mapping string features to BERT inputs.\n",
    "\n",
    "  Args:\n",
    "    sentence_features: a list with the names of string-valued features.\n",
    "    seq_length: an integer that defines the sequence length of BERT inputs.\n",
    "\n",
    "  Returns:\n",
    "    A Keras Model that can be called on a list or dict of string Tensors\n",
    "    (with the order or names, resp., given by sentence_features) and\n",
    "    returns a dict of tensors for input to BERT.\n",
    "  \"\"\"\n",
    "\n",
    "  input_segments = [\n",
    "      tf.keras.layers.Input(shape=(), dtype=tf.string, name=ft)\n",
    "      for ft in sentence_features]\n",
    "\n",
    "  # Tokenize the text to word pieces.\n",
    "  bert_preprocess = hub.load(tfhub_handle_preprocess)\n",
    "  tokenizer = hub.KerasLayer(bert_preprocess.tokenize, name='tokenizer')\n",
    "  segments = [tokenizer(s) for s in input_segments]\n",
    "\n",
    "  # Optional: Trim segments in a smart way to fit seq_length.\n",
    "  # Simple cases (like this example) can skip this step and let\n",
    "  # the next step apply a default truncation to approximately equal lengths.\n",
    "  truncated_segments = segments\n",
    "\n",
    "  # Pack inputs. The details (start/end token ids, dict of output tensors)\n",
    "  # are model-dependent, so this gets loaded from the SavedModel.\n",
    "  packer = hub.KerasLayer(bert_preprocess.bert_pack_inputs,\n",
    "                          arguments=dict(seq_length=seq_length),\n",
    "                          name='packer')\n",
    "  model_inputs = packer(truncated_segments)\n",
    "  return tf.keras.Model(input_segments, model_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "6_9zJLhA2YZQ"
   },
   "outputs": [],
   "source": [
    "@tf.keras.saving.register_keras_serializable()\n",
    "class Classifier(tf.keras.Model):\n",
    "  def __init__(self, num_classes):\n",
    "    super(Classifier, self).__init__(name=\"prediction\")\n",
    "    self.encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True)\n",
    "    self.dropout = tf.keras.layers.Dropout(0.3)\n",
    "    self.dense = tf.keras.layers.Dense(num_classes, kernel_regularizer=tf.keras.regularizers.l2(1e-4))\n",
    "\n",
    "  def call(self, preprocessed_text):\n",
    "    encoder_outputs = self.encoder(preprocessed_text)\n",
    "    pooled_output = encoder_outputs[\"pooled_output\"]\n",
    "    x = self.dropout(pooled_output)\n",
    "    x = self.dense(x)\n",
    "    return x\n",
    "  \n",
    "  @classmethod\n",
    "  def from_config(cls, config):\n",
    "    return cls(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rXb9xtfvZZGV",
    "outputId": "b669f262-a366-4104-eb0f-f8133e599fcd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys           :  ['input_type_ids', 'input_mask', 'input_word_ids']\n",
      "Shape Word Ids :  (1, 256)\n",
      "Word Ids       :  tf.Tensor(\n",
      "[ 101 2070 6721 3231 6251  102 2178 6251  102    0    0    0    0    0\n",
      "    0    0], shape=(16,), dtype=int32)\n",
      "Shape Mask     :  (1, 256)\n",
      "Input Mask     :  tf.Tensor([1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0], shape=(16,), dtype=int32)\n",
      "Shape Type Ids :  (1, 256)\n",
      "Type Ids       :  tf.Tensor([0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0], shape=(16,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "test_preprocess_model = make_bert_preprocess_model(['my_input1', 'my_input2'], 256)\n",
    "test_text = [np.array(['some random test sentence']),\n",
    "             np.array(['another sentence'])]\n",
    "text_preprocessed = test_preprocess_model(test_text)\n",
    "\n",
    "print('Keys           : ', list(text_preprocessed.keys()))\n",
    "print('Shape Word Ids : ', text_preprocessed['input_word_ids'].shape)\n",
    "print('Word Ids       : ', text_preprocessed['input_word_ids'][0, :16])\n",
    "print('Shape Mask     : ', text_preprocessed['input_mask'].shape)\n",
    "print('Input Mask     : ', text_preprocessed['input_mask'][0, :16])\n",
    "print('Shape Type Ids : ', text_preprocessed['input_type_ids'].shape)\n",
    "print('Type Ids       : ', text_preprocessed['input_type_ids'][0, :16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bd8A6_xI2afq",
    "outputId": "d57ae791-1690-4402-c082-1c2ec356641f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the output: (1, 7)\n",
      "tf.Tensor(\n",
      "[[-0.4136535  -0.49654543  1.21227     0.93250537 -0.21595645 -0.14548585\n",
      "   0.26007497]], shape=(1, 7), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "classifier_model = Classifier(7)\n",
    "bert_raw_result = classifier_model(text_preprocessed)\n",
    "\n",
    "print(f'Shape of the output: {bert_raw_result.shape}')\n",
    "print(bert_raw_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mcc_metric(y_true, y_pred, num_classes=7):\n",
    "    ''' Custom Mathew Correlation Coefficient for multiclass \n",
    "        For more details see: \n",
    "            \"https://en.wikipedia.org/wiki/Phi_coefficient\"\n",
    "        Inputs: \n",
    "            y_true (tensor)\n",
    "            y_pred (tensor)\n",
    "            num_classes - number of classes\n",
    "        Outputs:\n",
    "            mcc - Mathews Correlation Coefficient\n",
    "        '''\n",
    "    # obtain predictions here, we can add in a threshold if we would like to\n",
    "    y_pred = tf.argmax(y_pred, axis=-1)\n",
    "    # cast to int64\n",
    "    y_true = tf.squeeze(tf.cast(y_true, tf.int64), axis=-1)\n",
    "    y_pred = tf.cast(y_pred, tf.int64)\n",
    "    # total number of samples\n",
    "    s = tf.size(y_true, out_type=tf.int64)\n",
    "    # total number of correctly predicted labels\n",
    "    c = s - tf.math.count_nonzero(y_true - y_pred)\n",
    "    # number of times each class truely occured\n",
    "    t = []\n",
    "    # number of times each class was predicted\n",
    "    p = []\n",
    "\n",
    "    for k in range(num_classes):\n",
    "        k = tf.cast(k, tf.int64)\n",
    "        # number of times that the class truely occured\n",
    "        t.append(tf.reduce_sum(tf.cast(tf.equal(k, y_true), tf.int32)))\n",
    "        # number of times that the class was predicted\n",
    "        p.append(tf.reduce_sum(tf.cast(tf.equal(k, y_pred), tf.int32)))\n",
    "\n",
    "    t = tf.expand_dims(tf.stack(t), 0)\n",
    "    p = tf.expand_dims(tf.stack(p), 0)\n",
    "\n",
    "    s = tf.cast(s, tf.int32)\n",
    "    c = tf.cast(c, tf.int32)\n",
    "\n",
    "    num = tf.cast(c*s - tf.matmul(t, tf.transpose(p)), tf.float32)\n",
    "    dem = tf.math.sqrt(tf.cast(s**2 - tf.matmul(p, tf.transpose(p)), tf.float32)) \\\n",
    "          * tf.math.sqrt(tf.cast(s**2 - tf.matmul(t, tf.transpose(t)), tf.float32))\n",
    "\n",
    "    mcc = tf.divide(num, dem + 1e-6)\n",
    "    return mcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "dlRw8RmI3Gzs"
   },
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metrics = [mcc_metric, tf.metrics.SparseCategoricalAccuracy()]\n",
    "\n",
    "epochs = 10\n",
    "steps_per_epoch = tf.data.experimental.cardinality(train_dataset_final).numpy()\n",
    "num_train_steps = steps_per_epoch * epochs\n",
    "num_warmup_steps = int(0.1 * num_train_steps)\n",
    "\n",
    "def create_custom_optimizer(init_lr, num_train_steps, num_warmup_steps):\n",
    "    return optimization.create_optimizer(\n",
    "        init_lr=init_lr,\n",
    "        num_train_steps=num_train_steps,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        optimizer_type='adamw'\n",
    "    )\n",
    "\n",
    "init_lr = 2e-5\n",
    "optimizer = create_custom_optimizer(init_lr=init_lr,\n",
    "                                    num_train_steps=num_train_steps,\n",
    "                                    num_warmup_steps=num_warmup_steps)\n",
    "# optimizer = tf.keras.optimizers.AdamW()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "Ls6cy0yT3Wm3"
   },
   "outputs": [],
   "source": [
    "classifier_model.compile(optimizer= optimizer,\n",
    "                         loss=loss,\n",
    "                         metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"prediction\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " keras_layer_1 (KerasLayer)  multiple                  109482241 \n",
      "                                                                 \n",
      " dropout (Dropout)           multiple                  0         \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  5383      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 109487624 (417.66 MB)\n",
      "Trainable params: 109487623 (417.66 MB)\n",
      "Non-trainable params: 1 (1.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classifier_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NRcAy15CcNBa"
   },
   "source": [
    "# STEP IDK: tokenizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0HXaQqNBfgyd",
    "outputId": "bffdb82e-cfa7-4695-d258-34ba754eca14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'get attached easily one thing hate end worst enemy damn time', shape=(), dtype=string)\n",
      "tf.Tensor(6, shape=(), dtype=int64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-01 12:48:02.017975: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "for statement, label in train_dataset.take(1):\n",
    "    print(statement)\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "mrvcdFOIhY60"
   },
   "outputs": [],
   "source": [
    "#optimizing data\n",
    "# Optimize the datasets for training\n",
    "train_dataset_batched = (train_dataset\n",
    "                       .shuffle(train_dataset.cardinality(), seed=SEED)\n",
    "                       .cache()\n",
    "                       .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "                       .batch(BATCH_SIZE)\n",
    "                       )\n",
    "\n",
    "val_dataset_batched = (val_dataset\n",
    "                      .cache()\n",
    "                      .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "                      .batch(BATCH_SIZE)\n",
    "                      )\n",
    "\n",
    "test_dataset_batched = (test_dataset\n",
    "                      .cache()\n",
    "                      .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "                      .batch(BATCH_SIZE)\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "Ts1DsjYXcSEx"
   },
   "outputs": [],
   "source": [
    "bert_preprocess_model = make_bert_preprocess_model(['sentence'], 256)\n",
    "\n",
    "train_dataset_final = train_dataset_batched.map(lambda text, label: (bert_preprocess_model(text), label))\n",
    "val_dataset_final = val_dataset_batched.map(lambda text, label: (bert_preprocess_model(text), label))\n",
    "test_dataset_final = test_dataset_batched.map(lambda text, label: (bert_preprocess_model(text), label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NG8hLpP5fmdv",
    "outputId": "4918ee8a-75c5-43b4-9b2a-7878ee50b1e8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-01 12:48:22.582546: W tensorflow/core/kernels/data/cache_dataset_ops.cc:914] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions have shape: (4, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-01 12:48:26.607440: W tensorflow/core/kernels/data/cache_dataset_ops.cc:914] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "# Check model compatibility\n",
    "example_batch = train_dataset_final.take(1)\n",
    "\n",
    "try:\n",
    "\tclassifier_model.evaluate(example_batch, verbose=False)\n",
    "except:\n",
    "\tprint(\"Your model is not compatible with the dataset you defined earlier. Check that the loss function and last layer are compatible with one another.\")\n",
    "else:\n",
    "\tpredictions = classifier_model.predict(example_batch, verbose=False)\n",
    "\tprint(f\"predictions have shape: {predictions.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gmBrP0z0fmdv"
   },
   "source": [
    "# Step 3: How To Train Your ~~Dragon~~ ML Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t3H_70gmh941"
   },
   "source": [
    "### Callback-callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = data_dir\n",
    "checkpoint_model_filepath = checkpoint_path+\"/checkpoint.tf\"\n",
    "checkpoint_num_epoch_filepath = checkpoint_path+\"/current_epoch.txt\"\n",
    "training_log_filepath = checkpoint_path+\"/training_log.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jZ-YPeiLfmdv",
    "outputId": "5b6b9aa0-0dfb-49fd-8d05-f2da5610525a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No saved epoch found\n"
     ]
    }
   ],
   "source": [
    "# run this code to delete checkpoint\n",
    "try:\n",
    "    shutil.rmtree(checkpoint_model_filepath)\n",
    "    os.remove(checkpoint_num_epoch_filepath)\n",
    "    os.remove(training_log_filepath)\n",
    "    print(\"Checkpoint deleted successfully\")\n",
    "except FileNotFoundError:\n",
    "    print(\"No saved epoch found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "1zGaikEDfmdv"
   },
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_model_filepath)\n",
    "\n",
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    patience=5, \n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "class CustomCheckpointCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, epoch_file, log_file):\n",
    "        self.epoch_file = epoch_file\n",
    "        self.log_file = log_file\n",
    "        self.history = {\"epoch\": [], \"loss\": [], \"sparse_categorical_accuracy\": [], \"mcc_metric\": [],\n",
    "                        \"val_loss\": [], \"val_sparse_categorical_accuracy\": [], \"val_mcc_metric\": []}\n",
    "\n",
    "        # Load history and starting epoch if they exist\n",
    "        if os.path.exists(self.log_file):\n",
    "            with open(self.log_file, 'r') as f:\n",
    "                self.history = json.load(f)\n",
    "\n",
    "        if os.path.exists(self.epoch_file):\n",
    "            with open(self.epoch_file, 'r') as f:\n",
    "                self.starting_epoch = int(f.read())\n",
    "        else:\n",
    "            with open(self.epoch_file, 'w') as f:\n",
    "                f.write(str(0))\n",
    "            self.starting_epoch = 0\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current_epoch = self.starting_epoch + epoch + 1\n",
    "        # Save the current epoch\n",
    "        with open(self.epoch_file, 'w') as f:\n",
    "            f.write(str(current_epoch))\n",
    "\n",
    "        # Save logs (loss, accuracy, etc.) for plotting\n",
    "        self.history[\"epoch\"].append(current_epoch)\n",
    "        self.history[\"loss\"].append(logs.get(\"loss\"))\n",
    "        self.history[\"sparse_categorical_accuracy\"].append(logs.get(\"sparse_categorical_accuracy\"))\n",
    "        self.history[\"mcc_metric\"].append(logs.get(\"mcc_metric\"))\n",
    "        self.history[\"val_loss\"].append(logs.get(\"val_loss\"))\n",
    "        self.history[\"val_sparse_categorical_accuracy\"].append(logs.get(\"val_sparse_categorical_accuracy\"))\n",
    "        self.history[\"val_mcc_metric\"].append(logs.get(\"val_mcc_metric\"))\n",
    "\n",
    "        # Save history to the log file\n",
    "        with open(self.log_file, 'w') as f:\n",
    "            json.dump(self.history, f, indent=4)\n",
    "\n",
    "custom_checkpoint_callback = CustomCheckpointCallback(checkpoint_num_epoch_filepath, training_log_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wSc1uib7fmdv",
    "outputId": "44cff1f9-e5f3-4908-a1b7-bbbb778dcfd1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming training from epoch 0\n"
     ]
    }
   ],
   "source": [
    "# Check checkpoint\n",
    "try:\n",
    "    with open(checkpoint_num_epoch_filepath, 'r') as f:\n",
    "        start_epoch = int(f.read())\n",
    "    print(f\"Resuming training from epoch {start_epoch}\")\n",
    "except FileNotFoundError:\n",
    "    start_epoch = 0\n",
    "    print(\"No saved epoch found. Starting from epoch 0\")\n",
    "\n",
    "# Load saved model\n",
    "if start_epoch > 0:\n",
    "    classifier_model = tf.keras.models.load_model(checkpoint_model_filepath, custom_objects={\"Classifier\": Classifier, \n",
    "                                                                                       \"AdamWeightDecay\": optimization.AdamWeightDecay,\n",
    "                                                                                       \"WarmUp\": optimization.WarmUp,\n",
    "                                                                                       \"mcc_metric\": mcc_metric\n",
    "                                                                                       })\n",
    "    print(f\"Loaded model from {checkpoint_model_filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z-xwb4hkiDKg"
   },
   "source": [
    "### TRAIN!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "id": "jYeszbOpfmdw",
    "outputId": "459b18a1-a39f-4aff-e334-f8526cb4d73b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "7335/7335 [==============================] - 2725s 370ms/step - loss: 1.2634 - mcc_metric: 0.4682 - sparse_categorical_accuracy: 0.6025 - val_loss: 0.8589 - val_mcc_metric: 0.6227 - val_sparse_categorical_accuracy: 0.7128\n",
      "Epoch 2/10\n",
      "7335/7335 [==============================] - 2708s 369ms/step - loss: 0.8546 - mcc_metric: 0.6616 - sparse_categorical_accuracy: 0.7560 - val_loss: 0.7723 - val_mcc_metric: 0.6616 - val_sparse_categorical_accuracy: 0.7595\n",
      "Epoch 3/10\n",
      "7335/7335 [==============================] - 2713s 370ms/step - loss: 0.6313 - mcc_metric: 0.7570 - sparse_categorical_accuracy: 0.8310 - val_loss: 1.0370 - val_mcc_metric: 0.6703 - val_sparse_categorical_accuracy: 0.7657\n",
      "Epoch 4/10\n",
      "7335/7335 [==============================] - 2700s 368ms/step - loss: 0.4507 - mcc_metric: 0.8246 - sparse_categorical_accuracy: 0.8830 - val_loss: 1.2454 - val_mcc_metric: 0.6659 - val_sparse_categorical_accuracy: 0.7657\n",
      "Epoch 5/10\n",
      "7335/7335 [==============================] - 2705s 369ms/step - loss: 0.3306 - mcc_metric: 0.8723 - sparse_categorical_accuracy: 0.9206 - val_loss: 1.4658 - val_mcc_metric: 0.6522 - val_sparse_categorical_accuracy: 0.7565\n",
      "Epoch 6/10\n",
      "7335/7335 [==============================] - 2708s 369ms/step - loss: 0.2251 - mcc_metric: 0.9045 - sparse_categorical_accuracy: 0.9444 - val_loss: 1.6266 - val_mcc_metric: 0.6514 - val_sparse_categorical_accuracy: 0.7516\n",
      "Epoch 7/10\n",
      "7335/7335 [==============================] - 2706s 369ms/step - loss: 0.1506 - mcc_metric: 0.9267 - sparse_categorical_accuracy: 0.9630 - val_loss: 1.7917 - val_mcc_metric: 0.6548 - val_sparse_categorical_accuracy: 0.7521\n",
      "Epoch 7: early stopping\n",
      "Restoring model weights from the end of the best epoch: 2.\n"
     ]
    }
   ],
   "source": [
    "history = classifier_model.fit(\n",
    "    train_dataset_final,\n",
    "    epochs=epochs,\n",
    "    initial_epoch=start_epoch,\n",
    "    validation_data=val_dataset_final,\n",
    "    class_weight=dict(enumerate(class_weights)),\n",
    "    callbacks=[\n",
    "        model_checkpoint_callback,\n",
    "        custom_checkpoint_callback,\n",
    "        early_stopping_callback\n",
    "    ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "cSwEMAVD7DVL",
    "outputId": "6d89403d-207b-4158-e93f-754e3525e148"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "917/917 [==============================] - 84s 91ms/step - loss: 0.7723 - mcc_metric: 0.6616 - sparse_categorical_accuracy: 0.7595\n",
      "Loss: 0.7722732424736023\n",
      "Accuracy: 0.7594764232635498\n",
      "MCC: 0.6615625023841858\n"
     ]
    }
   ],
   "source": [
    "loss, mcc, accuracy = classifier_model.evaluate(test_dataset_final)\n",
    "\n",
    "print(f'Loss: {loss}')\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'MCC: {mcc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 957
    },
    "id": "TTR38rCrfmdw",
    "outputId": "0ad68241-5100-4ba0-81ee-ca57889d36c6"
   },
   "outputs": [],
   "source": [
    "def plot_graphs(training_log):\n",
    "    # Load the log file\n",
    "    with open(training_log, \"r\") as f:\n",
    "        history = json.load(f)\n",
    "    \n",
    "\n",
    "    # Plot loss\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history[\"epoch\"], history[\"loss\"], label=\"Training Loss\")\n",
    "    plt.plot(history[\"epoch\"], history[\"val_loss\"], label=\"Validation Loss\")\n",
    "    plt.title(\"Loss per Epoch\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history[\"epoch\"], history[\"sparse_categorical_accuracy\"], label=\"Training Accuracy\")\n",
    "    plt.plot(history[\"epoch\"], history[\"val_sparse_categorical_accuracy\"], label=\"Validation Accuracy\")\n",
    "    plt.title(\"Accuracy per Epoch\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_graphs(training_log_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IuvJIudufmdw"
   },
   "source": [
    "# Step 4: Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_save_path = './my_models'\n",
    "bert_type = tfhub_handle_encoder.split('/')[-2]\n",
    "saved_model_name = f'{\"cleaned_data_10-256_removed_stopwords_punc_num_space__add_more_layer_and_freezing_encoder\"}_{bert_type}'\n",
    "\n",
    "saved_model_path = os.path.join(main_save_path, saved_model_name)\n",
    "\n",
    "preprocess_inputs = bert_preprocess_model.inputs\n",
    "bert_encoder_inputs = bert_preprocess_model(preprocess_inputs)\n",
    "bert_outputs = classifier_model(bert_encoder_inputs)\n",
    "model_for_export = tf.keras.Model(preprocess_inputs, bert_outputs)\n",
    "model_for_export.save(saved_model_path, include_optimizer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ByeseFDbfmdw",
    "outputId": "59a193a8-02bb-454a-b301-e0906c000d86"
   },
   "outputs": [],
   "source": [
    "rawtext_test = [\"can take anymore wanna die\"]\n",
    "# sequence_test = padding_func(tf.data.Dataset.from_tensors(tokenizer.tokenize(rawtext_test)))\n",
    "predictions = model_for_export.predict(rawtext_test)\n",
    "\n",
    "# predictions will be a numpy array of shape (1, num_classes) with probabilities for each class\n",
    "print(predictions)\n",
    "\n",
    "# To get the predicted class index\n",
    "predicted_class_index = tf.argmax(predictions, axis=1).numpy()[0]\n",
    "print(f\"Predicted class: {label_encoder.get_vocabulary()[predicted_class_index]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VcaCRQZcfmdx"
   },
   "source": [
    "# Step 5: Done, save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y-L64a4Kfmdx"
   },
   "outputs": [],
   "source": [
    "# Run this if you happy with the model\n",
    "with open(data_dir+\"/label_vocabulary.txt\", \"w\") as f:\n",
    "    for label in label_encoder.get_vocabulary():\n",
    "        f.write(label + \"\\n\")\n",
    "\n",
    "model.save(data_dir+'/second_iteration.keras')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "capstone-bert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
