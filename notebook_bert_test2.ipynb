{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/HealLink/ML-Model/blob/ryan's-experiment/notebook_bert_test2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "qx4oxDctftL6",
    "outputId": "d76ffec5-45a9-407d-973d-99a62695886c"
   },
   "outputs": [],
   "source": [
    "# !pip install tensorflow[and-cuda]\n",
    "!pip install -U \"tensorflow-text==2.13.*\"\n",
    "!pip install \"tf-models-official==2.13.*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "EGRmbqkXZZGK",
    "outputId": "8a3df5ca-2d75-415f-b910-e21e58f56d23"
   },
   "outputs": [],
   "source": [
    "!pip3 install tf_keras==2.18\n",
    "!pip3 install --upgrade tensorflow-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "8KC6GScKZZGK"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_USE_LEGACY_KERAS'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "tt-4olqsZZGK"
   },
   "outputs": [],
   "source": [
    "os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "WrDbvi-tfmdl"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-30 09:46:45.265613: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1732934805.344006  137896 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1732934805.367096  137896 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-30 09:46:45.537185: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Import-import\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "import sklearn\n",
    "from official.nlp import optimization  # to create AdamW optimizer\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "import random\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hFZhZpL0ZZGL",
    "outputId": "9c5ac481-9f76-4782-cfd0-bf6f3ef98c0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lT1x77TTfmdm",
    "outputId": "838c36b8-4761-4e44-ca62-258f1bccbd4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOT in Colab\n"
     ]
    }
   ],
   "source": [
    "if os.getenv(\"COLAB_RELEASE_TAG\"):\n",
    "   print(\"Running in Colab\")\n",
    "   IN_COLAB = True\n",
    "else:\n",
    "   print(\"NOT in Colab\")\n",
    "   IN_COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RwL6HScQfmdm",
    "outputId": "4948b5fb-2411-4c97-bd9c-ce7eda77a411"
   },
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    # Load the Drive helper and mount\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    data_dir = \"drive/MyDrive/data\"\n",
    "else:\n",
    "    data_dir = \"data\"\n",
    "if not os.path.exists(data_dir):\n",
    "    os.mkdir(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LwFY2EZEfmdn"
   },
   "source": [
    "# Step 1: Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TaFX1wv1fmdn",
    "outputId": "63411d86-f336-427d-d8a2-0837e18eb796"
   },
   "outputs": [],
   "source": [
    "# Download raw dataset\n",
    "!wget -O {data_dir+\"/combined_data.csv\"} \"https://drive.google.com/uc?export=download&id=1GJn2kEIBgto2OyD7-h2HQOv_NJUriqJh\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "usXqdLlefmdo"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "yhYl6YGQfmdo"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_dir+\"/combined_data.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "xMZcubI5fmdo",
    "outputId": "f3dd057b-cdab-4c03-e1e6-0221d538b712"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>statement</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>oh my gosh</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>trouble sleeping, confused mind, restless hear...</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All wrong, back off dear, forward doubt. Stay ...</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I've shifted my focus to something else but I'...</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I'm restless and restless, it's been a month n...</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           statement   status\n",
       "0                                         oh my gosh  Anxiety\n",
       "1  trouble sleeping, confused mind, restless hear...  Anxiety\n",
       "2  All wrong, back off dear, forward doubt. Stay ...  Anxiety\n",
       "3  I've shifted my focus to something else but I'...  Anxiety\n",
       "4  I'm restless and restless, it's been a month n...  Anxiety"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zz0XrQghfmdo",
    "outputId": "0c5228d8-cbcb-481c-e507-793d432e729b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 53043 entries, 0 to 53042\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   statement  52681 non-null  object\n",
      " 1   status     53043 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 1.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wycr9Gj1fmdo"
   },
   "source": [
    "### Step 1a: Basic cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YbpOTg4xfmdo",
    "outputId": "03e33ba7-fec3-402f-d4b6-947d12147525"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with missing values: statement    362\n",
      "status         0\n",
      "dtype: int64\n",
      "Number of duplicate rows: 1969\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(f\"Number of rows with missing values: {df.isnull().sum()}\")\n",
    "\n",
    "# Check for duplicates\n",
    "print(f\"Number of duplicate rows: {df.duplicated(subset=['statement']).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "F-nz9lIHfmdp"
   },
   "outputs": [],
   "source": [
    "# Drop rows that contain empty values\n",
    "df = df.dropna()\n",
    "\n",
    "# Drop rows that contain duplicate values in the ‘statement’ column and keep only the first row\n",
    "df = df.drop_duplicates(subset=['statement'], keep='first')\n",
    "\n",
    "df.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bMxMHCd3fmdp",
    "outputId": "d92ca87b-0a67-47bc-b6da-47c00644196b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with missing values: statement    0\n",
      "status       0\n",
      "dtype: int64\n",
      "Number of duplicate rows: 0\n"
     ]
    }
   ],
   "source": [
    "# Recheck for missing values\n",
    "print(f\"Number of rows with missing values: {df.isnull().sum()}\")\n",
    "\n",
    "# Recheck for duplicates\n",
    "print(f\"Number of duplicate rows: {df.duplicated(subset=['statement']).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ihQ54ThCfmdp"
   },
   "source": [
    "### Step 1b: Deep cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "Sclv0Apifmdp"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Change the data type of ‘statement’ and ‘status’ columns to string\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mastype({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatement\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;28mstr\u001b[39m})\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# Change the data type of ‘statement’ and ‘status’ columns to string\n",
    "df = df.astype({\"statement\":str, \"status\":str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "qtbWv3cTfmdp"
   },
   "outputs": [],
   "source": [
    "emoj = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\"\n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "\n",
    "stopwords = [\"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "zstHlPs8fmdp"
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    # remove stopwords\n",
    "    for word in stopwords:\n",
    "        if word[0] == \"'\":\n",
    "            text = re.sub(rf\"{word}\\b\", \"\", text)\n",
    "        else:\n",
    "            text = re.sub(rf\"\\b{word}\\b\", \"\", text)\n",
    "\n",
    "    text = re.sub(r'[!\"“’#$%&()\\*\\+,-\\./:;<=>?@\\[\\\\\\]^_`{|}~\\']', ' ', text) # remove punctuation mark\n",
    "    # text = re.sub(emoj, ' ', text) # remove emoji\n",
    "    text = re.sub(r'\\d+', ' ', text) # remove number\n",
    "    # text = re.sub(r'(.)\\1+', r'\\1', text) # remove repeated character\n",
    "    # text = re.sub(r' [a-z] ', ' ', text) # remove single character\n",
    "    text = re.sub(r'\\s+', ' ', text) # remove multiple spaces\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "E8RHcyfIfmdq"
   },
   "outputs": [],
   "source": [
    "# CLEAN!!!\n",
    "df['statement'] = df['statement'].apply(clean_text)\n",
    "df = df[df['statement'] != \"\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aDcqaCdIfmdq"
   },
   "source": [
    "### Step 1c: Very deep cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "id": "_6A82beAfmdq",
    "outputId": "023966b5-55ee-4414-edb0-8823ab88d2bc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "status\n",
       "Normal                  15973\n",
       "Depression              15086\n",
       "Suicidal                10639\n",
       "Anxiety                  3617\n",
       "Bipolar                  2501\n",
       "Stress                   2293\n",
       "Personality disorder      895\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data distribution analysis of each label\n",
    "df.status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "PAHh2u0Dfmdq"
   },
   "outputs": [],
   "source": [
    "# Adding word count column for further analysis\n",
    "df['word_count'] = df['statement'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "kewGXeGufmdq"
   },
   "outputs": [],
   "source": [
    "# Define bins and labels for word count ranges\n",
    "bins = [0, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, float('inf')]  # Adjust as needed\n",
    "labels = ['1-100', '101-200', '201-300', '301-400', '401-500', '501-600', '601-700', '701-800', '801-900', '901-1000', '+1000']\n",
    "\n",
    "# Add a column to categorize statements into ranges\n",
    "df['word_count_range'] = pd.cut(df['word_count'], bins=bins, labels=labels, right=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 460
    },
    "id": "bK2xBB8mfmdq",
    "outputId": "c59185e6-7e11-4a2a-9b1a-1c97995c9faf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "word_count_range\n",
       "1-100       42531\n",
       "101-200      6121\n",
       "201-300      1510\n",
       "301-400       474\n",
       "401-500       198\n",
       "501-600        78\n",
       "601-700        39\n",
       "701-800        22\n",
       "801-900        10\n",
       "901-1000        6\n",
       "+1000          15\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the number of statements in each range\n",
    "df['word_count_range'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 480
    },
    "id": "An_5QsLCfmdr",
    "outputId": "a24cb5f4-65b9-41f4-a733-c21c4858a635"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_137896/3797327473.py:2: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  df.groupby(['word_count_range', 'status']).size().unstack(fill_value=0)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>status</th>\n",
       "      <th>Anxiety</th>\n",
       "      <th>Bipolar</th>\n",
       "      <th>Depression</th>\n",
       "      <th>Normal</th>\n",
       "      <th>Personality disorder</th>\n",
       "      <th>Stress</th>\n",
       "      <th>Suicidal</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word_count_range</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1-100</th>\n",
       "      <td>2705</td>\n",
       "      <td>1677</td>\n",
       "      <td>11106</td>\n",
       "      <td>15970</td>\n",
       "      <td>589</td>\n",
       "      <td>2053</td>\n",
       "      <td>8431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101-200</th>\n",
       "      <td>672</td>\n",
       "      <td>592</td>\n",
       "      <td>2849</td>\n",
       "      <td>3</td>\n",
       "      <td>232</td>\n",
       "      <td>178</td>\n",
       "      <td>1595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201-300</th>\n",
       "      <td>159</td>\n",
       "      <td>162</td>\n",
       "      <td>720</td>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>45</td>\n",
       "      <td>377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301-400</th>\n",
       "      <td>50</td>\n",
       "      <td>41</td>\n",
       "      <td>222</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>9</td>\n",
       "      <td>135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401-500</th>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>84</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501-600</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601-700</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>701-800</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>801-900</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>901-1000</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>+1000</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "status            Anxiety  Bipolar  Depression  Normal  Personality disorder  \\\n",
       "word_count_range                                                               \n",
       "1-100                2705     1677       11106   15970                   589   \n",
       "101-200               672      592        2849       3                   232   \n",
       "201-300               159      162         720       0                    47   \n",
       "301-400                50       41         222       0                    17   \n",
       "401-500                21       21          84       0                     7   \n",
       "501-600                 5        4          48       0                     2   \n",
       "601-700                 2        1          23       0                     0   \n",
       "701-800                 3        2          13       0                     0   \n",
       "801-900                 0        0           8       0                     0   \n",
       "901-1000                0        0           4       0                     0   \n",
       "+1000                   0        1           9       0                     1   \n",
       "\n",
       "status            Stress  Suicidal  \n",
       "word_count_range                    \n",
       "1-100               2053      8431  \n",
       "101-200              178      1595  \n",
       "201-300               45       377  \n",
       "301-400                9       135  \n",
       "401-500                6        59  \n",
       "501-600                0        19  \n",
       "601-700                1        12  \n",
       "701-800                0         4  \n",
       "801-900                1         1  \n",
       "901-1000               0         2  \n",
       "+1000                  0         4  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group by word count range and label, then count occurrences\n",
    "df.groupby(['word_count_range', 'status']).size().unstack(fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "id": "lAY7fYq2fmdr",
    "outputId": "273a0dc0-6768-4d04-f98e-f8f4ed221de3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "status\n",
       "Depression              13633\n",
       "Suicidal                 9393\n",
       "Normal                   5204\n",
       "Anxiety                  3022\n",
       "Bipolar                  2350\n",
       "Stress                   2242\n",
       "Personality disorder      831\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_export_candidate = df[(df['word_count'] >= 10) & (df['word_count'] <= 256)].reset_index(drop=True)\n",
    "df_export_candidate.status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oFB19IUHfmdr",
    "outputId": "4a0e8d05-d019-4ba6-9320-a2f76905357f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label with the lowest number of examples: Personality disorder\n",
      "Number of examples: 831\n"
     ]
    }
   ],
   "source": [
    "# Count the number of examples for each label\n",
    "label_counts = df_export_candidate['status'].value_counts()\n",
    "\n",
    "# Find the label with the minimum count\n",
    "min_label = label_counts.idxmin()\n",
    "min_count = label_counts.min()\n",
    "\n",
    "print(f\"Label with the lowest number of examples: {min_label}\")\n",
    "print(f\"Number of examples: {min_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_export_candidate = df_export_candidate.sort_values(by='word_count', ascending=False)\n",
    "df_export_candidate = df_export_candidate.groupby('status').head(min_count)\n",
    "df_export_candidate.reset_index(drop=True, inplace=True)\n",
    "df_export_candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "id": "yW2R5Fehfmdr",
    "outputId": "59637ac4-cc87-4176-bdfe-858eb7a32d9d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "status\n",
       "Depression              13633\n",
       "Suicidal                 9393\n",
       "Normal                   5204\n",
       "Anxiety                  3022\n",
       "Bipolar                  2350\n",
       "Stress                   2242\n",
       "Personality disorder      831\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_export_candidate.status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "nMEVNuL2fmdr"
   },
   "outputs": [],
   "source": [
    "df_export_candidate.drop(['word_count', 'word_count_range'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "hXaIhmwafmds"
   },
   "outputs": [],
   "source": [
    "# Optional, export the cleaned dataset\n",
    "df_export_candidate.to_csv(data_dir+'/cleaned_data_10-256_imbalanced_removed_stopwords_punc_num_space.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df\n",
    "del df_export_candidate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "94tE6LPnfmds"
   },
   "source": [
    "# Step 2: Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "OeooFwA3fmds"
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 10000\n",
    "MAX_LENGTH = 256\n",
    "TRAINING_SPLIT = 0.8\n",
    "BATCH_SIZE = 4\n",
    "SEED = 45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YWoCiB0Nfmds"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There're 36675 sentences\n"
     ]
    }
   ],
   "source": [
    "# suicidal = []\n",
    "# stress = []\n",
    "# personality_disorder = []\n",
    "# normal = []\n",
    "# depression = []\n",
    "# bipolar = []\n",
    "# anxiety = []\n",
    "\n",
    "sentences = []\n",
    "labels = []\n",
    "\n",
    "with open(data_dir+'/cleaned_data_10-256_imbalanced_removed_stopwords_punc_num_space.csv', 'r') as csvfile:\n",
    "    heading = next(csvfile)\n",
    "    reader_obj = csv.reader(csvfile)\n",
    "    for row in reader_obj:\n",
    "        label = row[1]\n",
    "        sentence = row[0]\n",
    "        labels.append(label)\n",
    "        sentences.append(sentence)\n",
    "        # if label == 'Suicidal':\n",
    "        #     suicidal.append(sentence)\n",
    "        # elif label == 'Stress':\n",
    "        #     stress.append(sentence)\n",
    "        # elif label == 'Personality disorder':\n",
    "        #     personality_disorder.append(sentence)\n",
    "        # elif label == 'Normal':\n",
    "        #     normal.append(sentence)\n",
    "        # elif label == 'Depression':\n",
    "        #     depression.append(sentence)\n",
    "        # elif label == 'Bipolar':\n",
    "        #     bipolar.append(sentence)\n",
    "        # else:\n",
    "        #     anxiety.append(sentence)\n",
    "\n",
    "# random.Random(SEED).shuffle(suicidal)\n",
    "# random.Random(SEED).shuffle(stress)\n",
    "# random.Random(SEED).shuffle(personality_disorder)\n",
    "# random.Random(SEED).shuffle(normal)\n",
    "# random.Random(SEED).shuffle(depression)\n",
    "# random.Random(SEED).shuffle(bipolar)\n",
    "# random.Random(SEED).shuffle(anxiety)\n",
    "\n",
    "# print(f\"There're {len(suicidal)} suicidal\")\n",
    "# print(f\"There're {len(stress)} stress\")\n",
    "# print(f\"There're {len(personality_disorder)} personality_disorder\")\n",
    "# print(f\"There're {len(normal)} normal\")\n",
    "# print(f\"There're {len(depression)} depression\")\n",
    "# print(f\"There're {len(bipolar)} bipolar\")\n",
    "# print(f\"There're {len(anxiety)} anxiety\")\n",
    "\n",
    "print(f\"There're {len(sentences)} sentences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-_D7MDhcfmds"
   },
   "source": [
    "### Step 2a: Create tf.data.Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 29340 sentence-label pairs for training.\n",
      "There are 7335 sentence-label pairs for validation.\n",
      "Classes: ['Anxiety' 'Bipolar' 'Depression' 'Normal' 'Personality disorder' 'Stress'\n",
      " 'Suicidal']\n",
      "Class Weights: {0: 1.7334278624601205, 1: 2.229483282674772, 2: 0.38432317728118204, 3: 1.006828866545417, 4: 6.302900107411386, 5: 2.336359292881032, 6: 0.5578158865356098}\n",
      "Class mapping: {'Anxiety': 0, 'Bipolar': 1, 'Depression': 2, 'Normal': 3, 'Personality disorder': 4, 'Stress': 5, 'Suicidal': 6}\n"
     ]
    }
   ],
   "source": [
    "# data_sum = len(suicidal + stress + personality_disorder + normal + depression + bipolar + anxiety)\n",
    "# train_size = int(data_sum * TRAINING_SPLIT)\n",
    "# train_size_per_label = int(train_size / 7)\n",
    "\n",
    "# sentences_train = suicidal[:train_size_per_label] + stress[:train_size_per_label] + personality_disorder[:train_size_per_label] + normal[:train_size_per_label] + depression[:train_size_per_label] + bipolar[:train_size_per_label] + anxiety[:train_size_per_label]\n",
    "# labels_train = ['Suicidal' for i in range(train_size_per_label)] + ['Stress' for i in range(train_size_per_label)] + ['Personality disorder' for i in range(train_size_per_label)] + ['Normal' for i in range(train_size_per_label)] + ['Depression' for i in range(train_size_per_label)] + ['Bipolar' for i in range(train_size_per_label)] + ['Anxiety' for i in range(train_size_per_label)]\n",
    "\n",
    "# sentences_val = suicidal[train_size_per_label:int(data_sum/7)] + stress[train_size_per_label:int(data_sum/7)] + personality_disorder[train_size_per_label:int(data_sum/7)] + normal[train_size_per_label:int(data_sum/7)] + depression[train_size_per_label:int(data_sum/7)] + bipolar[train_size_per_label:int(data_sum/7)] + anxiety[train_size_per_label:int(data_sum/7)]\n",
    "# labels_val = ['Suicidal' for i in range(int(data_sum/7)-train_size_per_label)] + ['Stress' for i in range(int(data_sum/7)-train_size_per_label)] + ['Personality disorder' for i in range(int(data_sum/7)-train_size_per_label)] + ['Normal' for i in range(int(data_sum/7)-train_size_per_label)] + ['Depression' for i in range(int(data_sum/7)-train_size_per_label)] + ['Bipolar' for i in range(int(data_sum/7)-train_size_per_label)] + ['Anxiety' for i in range(int(data_sum/7)-train_size_per_label)]\n",
    "\n",
    "import sklearn.model_selection\n",
    "import sklearn.utils\n",
    "\n",
    "\n",
    "train_sentences, val_sentences, train_labels, val_labels = sklearn.model_selection.train_test_split(\n",
    "    sentences, labels,\n",
    "    train_size=TRAINING_SPLIT,\n",
    "    stratify=labels,\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "classes = np.unique(train_labels)\n",
    "class_weights = sklearn.utils.class_weight.compute_class_weight(class_weight='balanced', classes=classes, y=train_labels)\n",
    "class_weight_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
    "class_to_index = {label: index for index, label in enumerate(classes)}\n",
    "\n",
    "train_labels_encoded = np.array([class_to_index[label] for label in train_labels])\n",
    "val_labels_encoded = np.array([class_to_index[label] for label in val_labels])\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_sentences, train_labels_encoded))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_sentences, val_labels_encoded))\n",
    "\n",
    "print(f\"There are {train_dataset.cardinality()} sentence-label pairs for training.\")\n",
    "print(f\"There are {val_dataset.cardinality()} sentence-label pairs for validation.\")\n",
    "print(\"Classes:\", classes)\n",
    "print(\"Class Weights:\", class_weight_dict)\n",
    "print(\"Class mapping:\", class_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OBrpjjNkfmds",
    "outputId": "f2116cdb-4e7e-4253-9f73-d986fae5b266"
   },
   "outputs": [],
   "source": [
    "train_statement = train_dataset.map(lambda statement, status: statement)\n",
    "train_labels = train_dataset.map(lambda statement, status: status)\n",
    "\n",
    "test_statement = val_dataset.map(lambda statement, status: statement)\n",
    "test_labels = val_dataset.map(lambda statement, status: status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uq08OU89ZZGT"
   },
   "outputs": [],
   "source": [
    "# del suicidal\n",
    "# del stress\n",
    "# del personality_disorder\n",
    "# del normal\n",
    "# del depression\n",
    "# del bipolar\n",
    "# del anxiety"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rhi60hT1fmdu"
   },
   "source": [
    "### Step 2d: Architect the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f4LEhBst0pdx"
   },
   "source": [
    "#### Step 2d-1: Loading model - choose model to fine tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UBzuTA9T06nD",
    "outputId": "18fde0a7-ba56-4a29-eb8e-0fb407268a62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT model selected           : https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3\n",
      "Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\n"
     ]
    }
   ],
   "source": [
    "bert_model_name = 'bert_en_uncased_L-12_H-768_A-12'\n",
    "\n",
    "map_name_to_handle = {\n",
    "    'bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\n",
    "    'bert_en_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\n",
    "    'bert_multi_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',\n",
    "    'albert_en_base':\n",
    "        'https://tfhub.dev/tensorflow/albert_en_base/2',\n",
    "    'electra_small':\n",
    "        'https://tfhub.dev/google/electra_small/2',\n",
    "    'electra_base':\n",
    "        'https://tfhub.dev/google/electra_base/2',\n",
    "    'experts_pubmed':\n",
    "        'https://tfhub.dev/google/experts/bert/pubmed/2',\n",
    "    'experts_wiki_books':\n",
    "        'https://tfhub.dev/google/experts/bert/wiki_books/2',\n",
    "    'talking-heads_base':\n",
    "        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\n",
    "}\n",
    "\n",
    "map_model_to_preprocess = {\n",
    "    'bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'bert_en_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'bert_multi_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3',\n",
    "    'albert_en_base':\n",
    "        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n",
    "    'electra_small':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'electra_base':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'experts_pubmed':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'experts_wiki_books':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'talking-heads_base':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "}\n",
    "\n",
    "tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n",
    "tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n",
    "\n",
    "print(f'BERT model selected           : {tfhub_handle_encoder}')\n",
    "print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wAFvI01r1JQr"
   },
   "source": [
    "#### Step 2d-2: Preprocessing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m-V-zwM31dJH",
    "outputId": "d70c1e8d-bb91-4a8f-9c15-fa54b8eadfd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys       : ['input_type_ids', 'input_mask', 'input_word_ids']\n",
      "Shape      : (1, 128)\n",
      "Word Ids   : [  101  1045 10587  3280   102     0     0     0     0     0     0     0]\n",
      "Input Mask : [1 1 1 1 1 0 0 0 0 0 0 0]\n",
      "Type Ids   : [0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)\n",
    "text_test = ['i wanna die']\n",
    "text_preprocessed = bert_preprocess_model(text_test)\n",
    "\n",
    "print(f'Keys       : {list(text_preprocessed.keys())}')\n",
    "print(f'Shape      : {text_preprocessed[\"input_word_ids\"].shape}')\n",
    "print(f'Word Ids   : {text_preprocessed[\"input_word_ids\"][0, :12]}')\n",
    "print(f'Input Mask : {text_preprocessed[\"input_mask\"][0, :12]}')\n",
    "print(f'Type Ids   : {text_preprocessed[\"input_type_ids\"][0, :12]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S5sSC48L1rN2"
   },
   "source": [
    "#### Step 2d-3: Using BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b9DhpY2E1w4U",
    "outputId": "4b6ec903-db20-43be-a66a-d2a917950cb7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.RaggedTensor [[[7592], [23435, 12314], [999]]]>\n"
     ]
    }
   ],
   "source": [
    "bert_preprocess = hub.load(tfhub_handle_preprocess)\n",
    "tok = bert_preprocess.tokenize(tf.constant(['Hello TensorFlow!']))\n",
    "print(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oFQ4Z5XBZZGU",
    "outputId": "2d4a7586-87c3-43e4-d7c1-7f5390b20aa4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape Word Ids :  (1, 20)\n",
      "Word Ids       :  tf.Tensor(\n",
      "[  101  7592 23435 12314   999   102  7592 23435 12314   999   102     0\n",
      "     0     0     0     0], shape=(16,), dtype=int32)\n",
      "Shape Mask     :  (1, 20)\n",
      "Input Mask     :  tf.Tensor([1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0], shape=(16,), dtype=int32)\n",
      "Shape Type Ids :  (1, 20)\n",
      "Type Ids       :  tf.Tensor([0 0 0 0 0 0 1 1 1 1 1 0 0 0 0 0], shape=(16,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "text_preprocessed = bert_preprocess.bert_pack_inputs([tok, tok], tf.constant(20))\n",
    "\n",
    "print('Shape Word Ids : ', text_preprocessed['input_word_ids'].shape)\n",
    "print('Word Ids       : ', text_preprocessed['input_word_ids'][0, :16])\n",
    "print('Shape Mask     : ', text_preprocessed['input_mask'].shape)\n",
    "print('Input Mask     : ', text_preprocessed['input_mask'][0, :16])\n",
    "print('Shape Type Ids : ', text_preprocessed['input_type_ids'].shape)\n",
    "print('Type Ids       : ', text_preprocessed['input_type_ids'][0, :16])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v2tuOrgF2UE_"
   },
   "source": [
    "#### Step 2d-4: Defining model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "Ne-WJSJ3ZZGV"
   },
   "outputs": [],
   "source": [
    "def make_bert_preprocess_model(sentence_features, seq_length=512):\n",
    "  \"\"\"Returns Model mapping string features to BERT inputs.\n",
    "\n",
    "  Args:\n",
    "    sentence_features: a list with the names of string-valued features.\n",
    "    seq_length: an integer that defines the sequence length of BERT inputs.\n",
    "\n",
    "  Returns:\n",
    "    A Keras Model that can be called on a list or dict of string Tensors\n",
    "    (with the order or names, resp., given by sentence_features) and\n",
    "    returns a dict of tensors for input to BERT.\n",
    "  \"\"\"\n",
    "\n",
    "  input_segments = [\n",
    "      tf.keras.layers.Input(shape=(), dtype=tf.string, name=ft)\n",
    "      for ft in sentence_features]\n",
    "\n",
    "  # Tokenize the text to word pieces.\n",
    "  bert_preprocess = hub.load(tfhub_handle_preprocess)\n",
    "  tokenizer = hub.KerasLayer(bert_preprocess.tokenize, name='tokenizer')\n",
    "  segments = [tokenizer(s) for s in input_segments]\n",
    "\n",
    "  # Optional: Trim segments in a smart way to fit seq_length.\n",
    "  # Simple cases (like this example) can skip this step and let\n",
    "  # the next step apply a default truncation to approximately equal lengths.\n",
    "  truncated_segments = segments\n",
    "\n",
    "  # Pack inputs. The details (start/end token ids, dict of output tensors)\n",
    "  # are model-dependent, so this gets loaded from the SavedModel.\n",
    "  packer = hub.KerasLayer(bert_preprocess.bert_pack_inputs,\n",
    "                          arguments=dict(seq_length=seq_length),\n",
    "                          name='packer')\n",
    "  model_inputs = packer(truncated_segments)\n",
    "  return tf.keras.Model(input_segments, model_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "6_9zJLhA2YZQ"
   },
   "outputs": [],
   "source": [
    "@tf.keras.saving.register_keras_serializable()\n",
    "class Classifier(tf.keras.Model):\n",
    "  def __init__(self, num_classes):\n",
    "    super(Classifier, self).__init__(name=\"prediction\")\n",
    "    self.encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True)\n",
    "    self.dropout_1 = tf.keras.layers.Dropout(0.1)\n",
    "    # self.dense_1 = tf.keras.layers.Dense(128, activation='relu')\n",
    "    # self.dropout_2 = tf.keras.layers.Dropout(0.1)\n",
    "    # self.dense_2 = tf.keras.layers.Dense(64, activation='relu')\n",
    "    # self.dropout_3 = tf.keras.layers.Dropout(0.1)\n",
    "    # self.dense_3 = tf.keras.layers.Dense(32)\n",
    "    # self.dropout_4 = tf.keras.layers.Dropout(0.1)\n",
    "    self.dense_4 = tf.keras.layers.Dense(num_classes)\n",
    "\n",
    "  def call(self, preprocessed_text):\n",
    "    encoder_outputs = self.encoder(preprocessed_text)\n",
    "    pooled_output = encoder_outputs[\"pooled_output\"]\n",
    "    x = self.dropout_1(pooled_output)\n",
    "    # x = self.dense_1(x)\n",
    "    # x = self.dropout_2(x)\n",
    "    # x = self.dense_2(x)\n",
    "    # x = self.dropout_3(x)\n",
    "    # x = self.dense_3(x)\n",
    "    # x = self.dropout_4(x)\n",
    "    x = self.dense_4(x)\n",
    "    return x\n",
    "  \n",
    "  @classmethod\n",
    "  def from_config(cls, config):\n",
    "    return cls(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rXb9xtfvZZGV",
    "outputId": "b669f262-a366-4104-eb0f-f8133e599fcd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys           :  ['input_type_ids', 'input_mask', 'input_word_ids']\n",
      "Shape Word Ids :  (1, 256)\n",
      "Word Ids       :  tf.Tensor(\n",
      "[ 101 2070 6721 3231 6251  102 2178 6251  102    0    0    0    0    0\n",
      "    0    0], shape=(16,), dtype=int32)\n",
      "Shape Mask     :  (1, 256)\n",
      "Input Mask     :  tf.Tensor([1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0], shape=(16,), dtype=int32)\n",
      "Shape Type Ids :  (1, 256)\n",
      "Type Ids       :  tf.Tensor([0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0], shape=(16,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "test_preprocess_model = make_bert_preprocess_model(['my_input1', 'my_input2'], 256)\n",
    "test_text = [np.array(['some random test sentence']),\n",
    "             np.array(['another sentence'])]\n",
    "text_preprocessed = test_preprocess_model(test_text)\n",
    "\n",
    "print('Keys           : ', list(text_preprocessed.keys()))\n",
    "print('Shape Word Ids : ', text_preprocessed['input_word_ids'].shape)\n",
    "print('Word Ids       : ', text_preprocessed['input_word_ids'][0, :16])\n",
    "print('Shape Mask     : ', text_preprocessed['input_mask'].shape)\n",
    "print('Input Mask     : ', text_preprocessed['input_mask'][0, :16])\n",
    "print('Shape Type Ids : ', text_preprocessed['input_type_ids'].shape)\n",
    "print('Type Ids       : ', text_preprocessed['input_type_ids'][0, :16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bd8A6_xI2afq",
    "outputId": "d57ae791-1690-4402-c082-1c2ec356641f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the output: (1, 7)\n",
      "tf.Tensor(\n",
      "[[ 0.00515634 -0.47386822 -0.8674062   0.96887815  0.5102396   0.77322614\n",
      "   1.6813855 ]], shape=(1, 7), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "classifier_model = Classifier(7)\n",
    "bert_raw_result = classifier_model(text_preprocessed)\n",
    "\n",
    "print(f'Shape of the output: {bert_raw_result.shape}')\n",
    "print(bert_raw_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "dlRw8RmI3Gzs"
   },
   "outputs": [],
   "source": [
    "def mcc_metric(y_true, y_pred):\n",
    "  predicted = tf.cast(tf.greater(y_pred, 0.5), tf.float32)\n",
    "  true_pos = tf.math.count_nonzero(predicted * y_true)\n",
    "  true_neg = tf.math.count_nonzero((predicted - 1) * (y_true - 1))\n",
    "  false_pos = tf.math.count_nonzero(predicted * (y_true - 1))\n",
    "  false_neg = tf.math.count_nonzero((predicted - 1) * y_true)\n",
    "  x = tf.cast((true_pos + false_pos) * (true_pos + false_neg) \n",
    "      * (true_neg + false_pos) * (true_neg + false_neg), tf.float32)\n",
    "  return tf.cast((true_pos * true_neg) - (false_pos * false_neg), tf.float32) / tf.sqrt(x)\n",
    "\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metrics = [mcc_metric, tf.metrics.SparseCategoricalAccuracy()]\n",
    "\n",
    "epochs = 50\n",
    "steps_per_epoch = tf.data.experimental.cardinality(train_dataset_final).numpy()\n",
    "num_train_steps = steps_per_epoch * epochs\n",
    "num_warmup_steps = int(0.1 * num_train_steps)\n",
    "\n",
    "def create_custom_optimizer(init_lr, num_train_steps, num_warmup_steps):\n",
    "    return optimization.create_optimizer(\n",
    "        init_lr=init_lr,\n",
    "        num_train_steps=num_train_steps,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        optimizer_type='adamw'\n",
    "    )\n",
    "\n",
    "init_lr = 2e-5\n",
    "optimizer = create_custom_optimizer(init_lr=init_lr,\n",
    "                                    num_train_steps=num_train_steps,\n",
    "                                    num_warmup_steps=num_warmup_steps)\n",
    "# optimizer = tf.keras.optimizers.AdamW(learning_rate=3e-5, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "Ls6cy0yT3Wm3"
   },
   "outputs": [],
   "source": [
    "classifier_model.compile(optimizer= optimizer,\n",
    "                         loss=loss,\n",
    "                         metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"prediction\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " keras_layer_1 (KerasLayer)  multiple                  109482241 \n",
      "                                                                 \n",
      " dropout (Dropout)           multiple                  0         \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  5383      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 109487624 (417.66 MB)\n",
      "Trainable params: 109487623 (417.66 MB)\n",
      "Non-trainable params: 1 (1.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classifier_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NRcAy15CcNBa"
   },
   "source": [
    "# STEP IDK: tokenizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0HXaQqNBfgyd",
    "outputId": "bffdb82e-cfa7-4695-d258-34ba754eca14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'depakote amp food f recently started depakote today s week taking s working really well mood swings general bipolar issues one thing ve noticed don t really want eat just don t get hungry ve heard everyone talk increased appetite since ve started taking times day ve just eating small breakfast small dinner m just not hungry isn t problem overweight stand lose couple pounds s just weird medications ve made voraciously hungry seroquel example made beast kitchen eat everything another thing ve noticed might play loss appetite everything tastes awful things used love now kind just meh m not necessarily complaining just wondering current previous depakote users felt way', shape=(), dtype=string)\n",
      "tf.Tensor(1, shape=(), dtype=int64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-30 10:14:01.460306: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "for statement, label in train_dataset.take(1):\n",
    "    print(statement)\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "mrvcdFOIhY60"
   },
   "outputs": [],
   "source": [
    "#optimizing data\n",
    "# Optimize the datasets for training\n",
    "train_dataset_batched = (train_dataset\n",
    "                       .shuffle(train_dataset.cardinality(), seed=SEED)\n",
    "                       .cache()\n",
    "                       .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "                       .batch(BATCH_SIZE)\n",
    "                       )\n",
    "\n",
    "test_dataset_batched = (val_dataset\n",
    "                      .cache()\n",
    "                      .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "                      .batch(BATCH_SIZE)\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "Ts1DsjYXcSEx"
   },
   "outputs": [],
   "source": [
    "bert_preprocess_model = make_bert_preprocess_model(['sentence'], 256)\n",
    "\n",
    "train_dataset_final = train_dataset_batched.map(lambda text, label: (bert_preprocess_model(text), label))\n",
    "test_dataset_final = test_dataset_batched.map(lambda text, label: (bert_preprocess_model(text), label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NG8hLpP5fmdv",
    "outputId": "4918ee8a-75c5-43b4-9b2a-7878ee50b1e8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-30 10:15:23.907169: W tensorflow/core/kernels/data/cache_dataset_ops.cc:914] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions have shape: (4, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-30 10:15:24.271090: W tensorflow/core/kernels/data/cache_dataset_ops.cc:914] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "# Check model compatibility\n",
    "example_batch = train_dataset_final.take(1)\n",
    "\n",
    "try:\n",
    "\tclassifier_model.evaluate(example_batch, verbose=False)\n",
    "except:\n",
    "\tprint(\"Your model is not compatible with the dataset you defined earlier. Check that the loss function and last layer are compatible with one another.\")\n",
    "else:\n",
    "\tpredictions = classifier_model.predict(example_batch, verbose=False)\n",
    "\tprint(f\"predictions have shape: {predictions.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gmBrP0z0fmdv"
   },
   "source": [
    "# Step 3: How To Train Your ~~Dragon~~ ML Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t3H_70gmh941"
   },
   "source": [
    "### Callback-callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = data_dir\n",
    "checkpoint_model_filepath = checkpoint_path+\"/checkpoint.tf\"\n",
    "checkpoint_num_epoch_filepath = checkpoint_path+\"/current_epoch.txt\"\n",
    "training_log_filepath = checkpoint_path+\"/training_log.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jZ-YPeiLfmdv",
    "outputId": "5b6b9aa0-0dfb-49fd-8d05-f2da5610525a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No saved epoch found\n"
     ]
    }
   ],
   "source": [
    "# run this code to delete checkpoint\n",
    "try:\n",
    "    shutil.rmtree(checkpoint_model_filepath)\n",
    "    os.remove(checkpoint_num_epoch_filepath)\n",
    "    os.remove(training_log_filepath)\n",
    "    print(\"Checkpoint deleted successfully\")\n",
    "except FileNotFoundError:\n",
    "    print(\"No saved epoch found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "1zGaikEDfmdv"
   },
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_model_filepath,\n",
    "    monitor='mcc_metric',\n",
    "    save_best_only=False,\n",
    "    save_weights_only=False)\n",
    "\n",
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    patience=5, \n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "class CustomCheckpointCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, epoch_file, log_file):\n",
    "        self.epoch_file = epoch_file\n",
    "        self.log_file = log_file\n",
    "        self.history = {\"epoch\": [], \"loss\": [], \"sparse_categorical_accuracy\": [], \"mcc_metric\": [],\n",
    "                        \"val_loss\": [], \"val_sparse_categorical_accuracy\": [], \"val_mcc_metric\": []}\n",
    "\n",
    "        # Load history and starting epoch if they exist\n",
    "        if os.path.exists(self.log_file):\n",
    "            with open(self.log_file, 'r') as f:\n",
    "                self.history = json.load(f)\n",
    "\n",
    "        if os.path.exists(self.epoch_file):\n",
    "            with open(self.epoch_file, 'r') as f:\n",
    "                self.starting_epoch = int(f.read())\n",
    "        else:\n",
    "            with open(self.epoch_file, 'w') as f:\n",
    "                f.write(str(0))\n",
    "            self.starting_epoch = 0\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current_epoch = self.starting_epoch + epoch + 1\n",
    "        # Save the current epoch\n",
    "        with open(self.epoch_file, 'w') as f:\n",
    "            f.write(str(current_epoch))\n",
    "\n",
    "        # Save logs (loss, accuracy, etc.) for plotting\n",
    "        self.history[\"epoch\"].append(current_epoch)\n",
    "        self.history[\"loss\"].append(logs.get(\"loss\"))\n",
    "        self.history[\"sparse_categorical_accuracy\"].append(logs.get(\"sparse_categorical_accuracy\"))\n",
    "        self.history[\"mcc_metric\"].append(logs.get(\"mcc_metric\"))\n",
    "        self.history[\"val_loss\"].append(logs.get(\"val_loss\"))\n",
    "        self.history[\"val_sparse_categorical_accuracy\"].append(logs.get(\"val_sparse_categorical_accuracy\"))\n",
    "        self.history[\"val_mcc_metric\"].append(logs.get(\"val_mcc_metric\"))\n",
    "\n",
    "        # Save history to the log file\n",
    "        with open(self.log_file, 'w') as f:\n",
    "            json.dump(self.history, f, indent=4)\n",
    "\n",
    "custom_checkpoint_callback = CustomCheckpointCallback(checkpoint_num_epoch_filepath, training_log_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wSc1uib7fmdv",
    "outputId": "44cff1f9-e5f3-4908-a1b7-bbbb778dcfd1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming training from epoch 0\n"
     ]
    }
   ],
   "source": [
    "# Check checkpoint\n",
    "try:\n",
    "    with open(checkpoint_num_epoch_filepath, 'r') as f:\n",
    "        start_epoch = int(f.read())\n",
    "    print(f\"Resuming training from epoch {start_epoch}\")\n",
    "except FileNotFoundError:\n",
    "    start_epoch = 0\n",
    "    print(\"No saved epoch found. Starting from epoch 0\")\n",
    "\n",
    "# Load saved model\n",
    "if start_epoch > 0:\n",
    "    classifier_model = tf.keras.models.load_model(checkpoint_model_filepath, custom_objects={\"Classifier\": Classifier, \n",
    "                                                                                       \"AdamWeightDecay\": optimization.AdamWeightDecay,\n",
    "                                                                                       \"WarmUp\": optimization.WarmUp\n",
    "                                                                                       })\n",
    "    print(f\"Loaded model from {checkpoint_model_filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 1.7334278624601205,\n",
       " 1: 2.229483282674772,\n",
       " 2: 0.38432317728118204,\n",
       " 3: 1.006828866545417,\n",
       " 4: 6.302900107411386,\n",
       " 5: 2.336359292881032,\n",
       " 6: 0.5578158865356098}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(enumerate(class_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z-xwb4hkiDKg"
   },
   "source": [
    "### TRAIN!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "id": "jYeszbOpfmdw",
    "outputId": "459b18a1-a39f-4aff-e334-f8526cb4d73b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "7335/7335 [==============================] - 2821s 384ms/step - loss: 1.4518 - mcc_metric: -0.0010 - sparse_categorical_accuracy: 0.5133 - val_loss: 0.7951 - val_mcc_metric: -0.0054 - val_sparse_categorical_accuracy: 0.7031\n",
      "Epoch 2/50\n",
      "1716/7335 [======>.......................] - ETA: 33:26 - loss: 0.9033 - mcc_metric: -0.0052 - sparse_categorical_accuracy: 0.6989"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[87], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mclassifier_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset_final\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_dataset_final\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclass_weights\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_checkpoint_callback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_checkpoint_callback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mearly_stopping_callback\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/capstone-bert/lib/python3.11/site-packages/tf_keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/capstone-bert/lib/python3.11/site-packages/tf_keras/src/engine/training.py:1804\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1796\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1797\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1798\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1801\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1802\u001b[0m ):\n\u001b[1;32m   1803\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1804\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1806\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/anaconda3/envs/capstone-bert/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/capstone-bert/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/anaconda3/envs/capstone-bert/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:869\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    866\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    867\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    868\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 869\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    873\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    874\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    875\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/anaconda3/envs/capstone-bert/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/capstone-bert/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/anaconda3/envs/capstone-bert/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/anaconda3/envs/capstone-bert/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/capstone-bert/lib/python3.11/site-packages/tensorflow/python/eager/context.py:1683\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1681\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1682\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1683\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1684\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1685\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1686\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1687\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1688\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1689\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1690\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1691\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1692\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1693\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1697\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1698\u001b[0m   )\n",
      "File \u001b[0;32m~/anaconda3/envs/capstone-bert/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = classifier_model.fit(\n",
    "    train_dataset_final,\n",
    "    epochs=epochs,\n",
    "    initial_epoch=start_epoch,\n",
    "    validation_data=test_dataset_final,\n",
    "    class_weight=dict(enumerate(class_weights)),\n",
    "    callbacks=[\n",
    "        model_checkpoint_callback,\n",
    "        custom_checkpoint_callback,\n",
    "        early_stopping_callback\n",
    "    ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cSwEMAVD7DVL",
    "outputId": "6d89403d-207b-4158-e93f-754e3525e148"
   },
   "outputs": [],
   "source": [
    "loss, accuracy = classifier_model.evaluate(test_dataset_final)\n",
    "\n",
    "print(f'Loss: {loss}')\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 957
    },
    "id": "TTR38rCrfmdw",
    "outputId": "0ad68241-5100-4ba0-81ee-ca57889d36c6"
   },
   "outputs": [],
   "source": [
    "def plot_graphs(training_log):\n",
    "    # Load the log file\n",
    "    # with open(training_log, \"r\") as f:\n",
    "    #     history = json.load(f)\n",
    "    \n",
    "\n",
    "    # Plot loss\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history[\"epoch\"], history[\"loss\"], label=\"Training Loss\")\n",
    "    plt.plot(history[\"epoch\"], history[\"val_loss\"], label=\"Validation Loss\")\n",
    "    plt.title(\"Loss per Epoch\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history[\"epoch\"], history[\"sparse_categorical_accuracy\"], label=\"Training Accuracy\")\n",
    "    plt.plot(history[\"epoch\"], history[\"val_sparse_categorical_accuracy\"], label=\"Validation Accuracy\")\n",
    "    plt.title(\"Accuracy per Epoch\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_graphs(training_log_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IuvJIudufmdw"
   },
   "source": [
    "# Step 4: Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_save_path = './my_models'\n",
    "bert_type = tfhub_handle_encoder.split('/')[-2]\n",
    "saved_model_name = f'{\"cleaned_data_10-256_removed_stopwords_punc_num_space__add_more_layer_and_freezing_encoder\"}_{bert_type}'\n",
    "\n",
    "saved_model_path = os.path.join(main_save_path, saved_model_name)\n",
    "\n",
    "preprocess_inputs = bert_preprocess_model.inputs\n",
    "bert_encoder_inputs = bert_preprocess_model(preprocess_inputs)\n",
    "bert_outputs = classifier_model(bert_encoder_inputs)\n",
    "model_for_export = tf.keras.Model(preprocess_inputs, bert_outputs)\n",
    "model_for_export.save(saved_model_path, include_optimizer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ByeseFDbfmdw",
    "outputId": "59a193a8-02bb-454a-b301-e0906c000d86"
   },
   "outputs": [],
   "source": [
    "rawtext_test = [\"can take anymore wanna die\"]\n",
    "# sequence_test = padding_func(tf.data.Dataset.from_tensors(tokenizer.tokenize(rawtext_test)))\n",
    "predictions = model_for_export.predict(rawtext_test)\n",
    "\n",
    "# predictions will be a numpy array of shape (1, num_classes) with probabilities for each class\n",
    "print(predictions)\n",
    "\n",
    "# To get the predicted class index\n",
    "predicted_class_index = tf.argmax(predictions, axis=1).numpy()[0]\n",
    "print(f\"Predicted class: {label_encoder.get_vocabulary()[predicted_class_index]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VcaCRQZcfmdx"
   },
   "source": [
    "# Step 5: Done, save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y-L64a4Kfmdx"
   },
   "outputs": [],
   "source": [
    "# Run this if you happy with the model\n",
    "with open(data_dir+\"/label_vocabulary.txt\", \"w\") as f:\n",
    "    for label in label_encoder.get_vocabulary():\n",
    "        f.write(label + \"\\n\")\n",
    "\n",
    "model.save(data_dir+'/second_iteration.keras')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "capstone-bert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
