{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": true,
    "id": "qx4oxDctftL6",
    "outputId": "106369c9-07d7-47d3-d141-446758a4d3d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (2.18.0)\n",
      "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7f7b365756f0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/tensorflow/\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: absl-py>=1.0.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from tensorflow) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from tensorflow) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from tensorflow) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from tensorflow) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from tensorflow) (1.68.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from tensorflow) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from tensorflow) (3.7.0)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from tensorflow) (2.0.2)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from tensorflow) (3.12.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from tensorflow) (0.4.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from tensorflow) (0.37.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: rich in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
      "Requirement already satisfied: namex in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow) (0.13.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: keras-nlp in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (0.18.1)\n",
      "Requirement already satisfied: keras-hub==0.18.1 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from keras-nlp) (0.18.1)\n",
      "Requirement already satisfied: absl-py in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from keras-hub==0.18.1->keras-nlp) (1.4.0)\n",
      "Requirement already satisfied: numpy in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from keras-hub==0.18.1->keras-nlp) (2.0.2)\n",
      "Requirement already satisfied: packaging in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from keras-hub==0.18.1->keras-nlp) (24.2)\n",
      "Requirement already satisfied: regex in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from keras-hub==0.18.1->keras-nlp) (2024.11.6)\n",
      "Requirement already satisfied: rich in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from keras-hub==0.18.1->keras-nlp) (13.9.4)\n",
      "Requirement already satisfied: kagglehub in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from keras-hub==0.18.1->keras-nlp) (0.3.4)\n",
      "Requirement already satisfied: tensorflow-text in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from keras-hub==0.18.1->keras-nlp) (2.18.0)\n",
      "Requirement already satisfied: requests in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from kagglehub->keras-hub==0.18.1->keras-nlp) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from kagglehub->keras-hub==0.18.1->keras-nlp) (4.67.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from rich->keras-hub==0.18.1->keras-nlp) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from rich->keras-hub==0.18.1->keras-nlp) (2.18.0)\n",
      "Requirement already satisfied: typing-extensions<5.0,>=4.0.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from rich->keras-hub==0.18.1->keras-nlp) (4.12.2)\n",
      "Requirement already satisfied: tensorflow<2.19,>=2.18.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from tensorflow-text->keras-hub==0.18.1->keras-nlp) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras-hub==0.18.1->keras-nlp) (0.1.2)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras-nlp) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras-nlp) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras-nlp) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras-nlp) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras-nlp) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras-nlp) (3.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras-nlp) (3.20.3)\n",
      "Requirement already satisfied: setuptools in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras-nlp) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras-nlp) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras-nlp) (2.5.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras-nlp) (1.17.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras-nlp) (1.68.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras-nlp) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras-nlp) (3.7.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras-nlp) (3.12.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras-nlp) (0.4.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras-nlp) (0.37.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from requests->kagglehub->keras-hub==0.18.1->keras-nlp) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from requests->kagglehub->keras-hub==0.18.1->keras-nlp) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from requests->kagglehub->keras-hub==0.18.1->keras-nlp) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from requests->kagglehub->keras-hub==0.18.1->keras-nlp) (2024.8.30)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras-nlp) (0.44.0)\n",
      "Requirement already satisfied: namex in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras-nlp) (0.0.8)\n",
      "Requirement already satisfied: optree in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras-nlp) (0.13.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras-nlp) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras-nlp) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras-nlp) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras-nlp) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: keras in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (3.7.0)\n",
      "Requirement already satisfied: absl-py in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from keras) (1.4.0)\n",
      "Requirement already satisfied: numpy in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from keras) (2.0.2)\n",
      "Requirement already satisfied: rich in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from keras) (13.9.4)\n",
      "Requirement already satisfied: namex in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from keras) (0.0.8)\n",
      "Requirement already satisfied: h5py in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from keras) (3.12.1)\n",
      "Requirement already satisfied: optree in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from keras) (0.13.1)\n",
      "Requirement already satisfied: ml-dtypes in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from keras) (0.4.1)\n",
      "Requirement already satisfied: packaging in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from keras) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from optree->keras) (4.12.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from rich->keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from rich->keras) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: spacy in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (3.8.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from spacy) (1.0.11)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from spacy) (8.3.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from spacy) (0.15.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from spacy) (2.10.3)\n",
      "Requirement already satisfied: jinja2 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from spacy) (75.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from spacy) (24.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from spacy) (2.0.2)\n",
      "Requirement already satisfied: language-data>=1.2 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.1)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
      "Requirement already satisfied: blis<1.1.0,>=1.0.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from thinc<8.4.0,>=8.3.0->spacy) (1.0.1)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from thinc<8.4.0,>=8.3.0->spacy) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from jinja2->spacy) (3.0.2)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
      "Requirement already satisfied: wrapt in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/danish/anaconda3/envs/capstone/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m149.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:02\u001b[0m00:04\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade tensorflow\n",
    "%pip install --upgrade keras-nlp\n",
    "%pip install --upgrade keras\n",
    "!python -m pip install spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wze90GQI6AEI",
    "outputId": "8d214fff-605d-4375-b00a-a6f7d698ccb0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.0\n"
     ]
    }
   ],
   "source": [
    "import keras;\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "WrDbvi-tfmdl"
   },
   "outputs": [],
   "source": [
    "# Import-import\n",
    "import pandas as pd\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import keras_nlp\n",
    "import keras\n",
    "import os\n",
    "import json\n",
    "import spacy\n",
    "import sklearn.model_selection\n",
    "import sklearn.utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lT1x77TTfmdm",
    "outputId": "77c0b43e-04ce-44ea-97dc-0bd465ff44af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOT in Colab\n"
     ]
    }
   ],
   "source": [
    "if os.getenv(\"COLAB_RELEASE_TAG\"):\n",
    "   print(\"Running in Colab\")\n",
    "   IN_COLAB = True\n",
    "else:\n",
    "   print(\"NOT in Colab\")\n",
    "   IN_COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 193
    },
    "id": "RwL6HScQfmdm",
    "outputId": "a56d55b2-3028-41d4-b1a5-2033db0bacfc"
   },
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[Errno 17] File exists: 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m      7\u001b[0m     data_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 8\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: 'data'"
     ]
    }
   ],
   "source": [
    "if IN_COLAB:\n",
    "    # Load the Drive helper and mount\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    data_dir = \"drive/MyDrive/data\"\n",
    "else:\n",
    "    data_dir = \"data\"\n",
    "os.mkdir(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TaFX1wv1fmdn",
    "outputId": "63411d86-f336-427d-d8a2-0837e18eb796"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-12-09 13:16:48--  https://drive.google.com/uc?export=download&id=1GJn2kEIBgto2OyD7-h2HQOv_NJUriqJh\n",
      "Resolving drive.google.com (drive.google.com)... 172.217.194.102, 172.217.194.113, 172.217.194.139, ...\n",
      "Connecting to drive.google.com (drive.google.com)|172.217.194.102|:443... connected.\n",
      "HTTP request sent, awaiting response... 303 See Other\n",
      "Location: https://drive.usercontent.google.com/download?id=1GJn2kEIBgto2OyD7-h2HQOv_NJUriqJh&export=download [following]\n",
      "--2024-12-09 13:17:29--  https://drive.usercontent.google.com/download?id=1GJn2kEIBgto2OyD7-h2HQOv_NJUriqJh&export=download\n",
      "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 142.251.175.132, 2404:6800:4003:c01::84\n",
      "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|142.251.175.132|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 31469558 (30M) [application/octet-stream]\n",
      "Saving to: ‘data/combined_data.csv’\n",
      "\n",
      "data/combined_data. 100%[===================>]  30.01M   232KB/s    in 4m 16s  \n",
      "\n",
      "2024-12-09 13:21:50 (120 KB/s) - ‘data/combined_data.csv’ saved [31469558/31469558]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download raw dataset\n",
    "!wget -O {data_dir+\"/combined_data.csv\"} \"https://drive.google.com/uc?export=download&id=1GJn2kEIBgto2OyD7-h2HQOv_NJUriqJh\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LwFY2EZEfmdn"
   },
   "source": [
    "# Step 1: Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "usXqdLlefmdo"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "yhYl6YGQfmdo"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_dir+\"/combined_data.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "xMZcubI5fmdo",
    "outputId": "f3dd057b-cdab-4c03-e1e6-0221d538b712"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>statement</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>oh my gosh</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>trouble sleeping, confused mind, restless hear...</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All wrong, back off dear, forward doubt. Stay ...</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I've shifted my focus to something else but I'...</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I'm restless and restless, it's been a month n...</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           statement   status\n",
       "0                                         oh my gosh  Anxiety\n",
       "1  trouble sleeping, confused mind, restless hear...  Anxiety\n",
       "2  All wrong, back off dear, forward doubt. Stay ...  Anxiety\n",
       "3  I've shifted my focus to something else but I'...  Anxiety\n",
       "4  I'm restless and restless, it's been a month n...  Anxiety"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zz0XrQghfmdo",
    "outputId": "0c5228d8-cbcb-481c-e507-793d432e729b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 53043 entries, 0 to 53042\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   statement  52681 non-null  object\n",
      " 1   status     53043 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 1.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wycr9Gj1fmdo"
   },
   "source": [
    "### Step 1a: Basic cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YbpOTg4xfmdo",
    "outputId": "03e33ba7-fec3-402f-d4b6-947d12147525"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with missing values: statement    362\n",
      "status         0\n",
      "dtype: int64\n",
      "Number of duplicate rows: 1969\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(f\"Number of rows with missing values: {df.isnull().sum()}\")\n",
    "\n",
    "# Check for duplicates\n",
    "print(f\"Number of duplicate rows: {df.duplicated(subset=['statement']).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "F-nz9lIHfmdp"
   },
   "outputs": [],
   "source": [
    "# Drop rows that contain empty values\n",
    "df = df.dropna()\n",
    "\n",
    "# Drop rows that contain duplicate values in the ‘statement’ column and keep only the first row\n",
    "df = df.drop_duplicates(subset=['statement'], keep='first')\n",
    "\n",
    "df.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bMxMHCd3fmdp",
    "outputId": "d92ca87b-0a67-47bc-b6da-47c00644196b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with missing values: statement    0\n",
      "status       0\n",
      "dtype: int64\n",
      "Number of duplicate rows: 0\n"
     ]
    }
   ],
   "source": [
    "# Recheck for missing values\n",
    "print(f\"Number of rows with missing values: {df.isnull().sum()}\")\n",
    "\n",
    "# Recheck for duplicates\n",
    "print(f\"Number of duplicate rows: {df.duplicated(subset=['statement']).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ihQ54ThCfmdp"
   },
   "source": [
    "### Step 1b: Deep cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "Sclv0Apifmdp"
   },
   "outputs": [],
   "source": [
    "# Change the data type of ‘statement’ and ‘status’ columns to string\n",
    "df = df.astype({\"statement\":str, \"status\":str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "qtbWv3cTfmdp"
   },
   "outputs": [],
   "source": [
    "emoj = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\"\n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "\n",
    "stopwords = [\"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "zstHlPs8fmdp"
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower() \n",
    "    \n",
    "    # Hapus angka\n",
    "    text = re.sub(r'\\d+', '', text)  \n",
    "    \n",
    "    # Hapus emoji (Unicode Range)\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "    \n",
    "    # Hapus spasi ganda atau lebih\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    doc = nlp(text)\n",
    "    cleaned_text = []\n",
    "    for token in doc:\n",
    "        # remove stopwords\n",
    "        if token.is_stop:\n",
    "            continue\n",
    "        # replace verb with its lemma\n",
    "        elif token.pos_ == \"VERB\":\n",
    "            cleaned_text.append(token.lemma_)\n",
    "        else:\n",
    "            cleaned_text.append(token.text)\n",
    "\n",
    "    text = \" \".join(cleaned_text)\n",
    "\n",
    "    # Hapus punctuation marks\n",
    "    text = re.sub(r'[^\\w\\s]', '', text) \n",
    "    \n",
    "    # Hapus karakter berulang\n",
    "    text = re.sub(r'(.)\\1+', r'\\1', text)\n",
    "    \n",
    "    # Hapus karakter tunggal (misalnya huruf yang berdiri sendiri)\n",
    "    text = re.sub(r'\\b\\w\\b', '', text)\n",
    "\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "E8RHcyfIfmdq"
   },
   "outputs": [],
   "source": [
    "# CLEAN!!!\n",
    "df['statement'] = df['statement'].apply(clean_text)\n",
    "df = df[df['statement'] != \"\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aDcqaCdIfmdq"
   },
   "source": [
    "### Step 1c: Very deep cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "id": "_6A82beAfmdq",
    "outputId": "023966b5-55ee-4414-edb0-8823ab88d2bc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "status\n",
       "Normal                  15823\n",
       "Depression              15083\n",
       "Suicidal                10637\n",
       "Anxiety                  3616\n",
       "Bipolar                  2501\n",
       "Stress                   2293\n",
       "Personality disorder      895\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data distribution analysis of each label\n",
    "df.status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "PAHh2u0Dfmdq"
   },
   "outputs": [],
   "source": [
    "# Adding word count column for further analysis\n",
    "df['word_count'] = df['statement'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "kewGXeGufmdq"
   },
   "outputs": [],
   "source": [
    "# Define bins and labels for word count ranges\n",
    "bins = [0, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, float('inf')]  # Adjust as needed\n",
    "labels = ['1-100', '101-200', '201-300', '301-400', '401-500', '501-600', '601-700', '701-800', '801-900', '901-1000', '+1000']\n",
    "\n",
    "# Add a column to categorize statements into ranges\n",
    "df['word_count_range'] = pd.cut(df['word_count'], bins=bins, labels=labels, right=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 460
    },
    "id": "bK2xBB8mfmdq",
    "outputId": "c59185e6-7e11-4a2a-9b1a-1c97995c9faf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "word_count_range\n",
       "1-100       45615\n",
       "101-200      4044\n",
       "201-300       808\n",
       "301-400       240\n",
       "401-500        74\n",
       "501-600        37\n",
       "601-700        10\n",
       "701-800         5\n",
       "801-900         5\n",
       "901-1000        5\n",
       "+1000           5\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the number of statements in each range\n",
    "df['word_count_range'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 480
    },
    "id": "An_5QsLCfmdr",
    "outputId": "a24cb5f4-65b9-41f4-a733-c21c4858a635"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13245/3797327473.py:2: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  df.groupby(['word_count_range', 'status']).size().unstack(fill_value=0)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>status</th>\n",
       "      <th>Anxiety</th>\n",
       "      <th>Bipolar</th>\n",
       "      <th>Depression</th>\n",
       "      <th>Normal</th>\n",
       "      <th>Personality disorder</th>\n",
       "      <th>Stress</th>\n",
       "      <th>Suicidal</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word_count_range</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1-100</th>\n",
       "      <td>3026</td>\n",
       "      <td>2008</td>\n",
       "      <td>12586</td>\n",
       "      <td>15823</td>\n",
       "      <td>729</td>\n",
       "      <td>2154</td>\n",
       "      <td>9289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101-200</th>\n",
       "      <td>469</td>\n",
       "      <td>388</td>\n",
       "      <td>1920</td>\n",
       "      <td>0</td>\n",
       "      <td>132</td>\n",
       "      <td>110</td>\n",
       "      <td>1025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201-300</th>\n",
       "      <td>87</td>\n",
       "      <td>77</td>\n",
       "      <td>385</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>18</td>\n",
       "      <td>215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301-400</th>\n",
       "      <td>23</td>\n",
       "      <td>20</td>\n",
       "      <td>113</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401-500</th>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501-600</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601-700</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>701-800</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>801-900</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>901-1000</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>+1000</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "status            Anxiety  Bipolar  Depression  Normal  Personality disorder  \\\n",
       "word_count_range                                                               \n",
       "1-100                3026     2008       12586   15823                   729   \n",
       "101-200               469      388        1920       0                   132   \n",
       "201-300                87       77         385       0                    26   \n",
       "301-400                23       20         113       0                     5   \n",
       "401-500                 8        4          38       0                     2   \n",
       "501-600                 3        2          22       0                     0   \n",
       "601-700                 0        1           7       0                     0   \n",
       "701-800                 0        0           3       0                     0   \n",
       "801-900                 0        0           4       0                     0   \n",
       "901-1000                0        0           4       0                     0   \n",
       "+1000                   0        1           1       0                     1   \n",
       "\n",
       "status            Stress  Suicidal  \n",
       "word_count_range                    \n",
       "1-100               2154      9289  \n",
       "101-200              110      1025  \n",
       "201-300               18       215  \n",
       "301-400                8        71  \n",
       "401-500                2        20  \n",
       "501-600                0        10  \n",
       "601-700                1         1  \n",
       "701-800                0         2  \n",
       "801-900                0         1  \n",
       "901-1000               0         1  \n",
       "+1000                  0         2  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group by word count range and label, then count occurrences\n",
    "df.groupby(['word_count_range', 'status']).size().unstack(fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "id": "lAY7fYq2fmdr",
    "outputId": "273a0dc0-6768-4d04-f98e-f8f4ed221de3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "status\n",
       "Depression              13742\n",
       "Suicidal                 9165\n",
       "Normal                   3540\n",
       "Anxiety                  2994\n",
       "Bipolar                  2450\n",
       "Stress                   2228\n",
       "Personality disorder      841\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_export_candidate = df[(df['word_count'] >= 10) & (df['word_count'] <= 1000)].reset_index(drop=True)\n",
    "df_export_candidate.status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window_split(df, max_length, overlap_ratio=0.5):    \n",
    "    def split_sentence(sentence, max_length):\n",
    "        words = sentence.split()  # Split by spaces, assuming words are separated by spaces\n",
    "        chunks = []\n",
    "        step = int(max_length * (1 - overlap_ratio))  # Calculate the step size based on overlap ratio\n",
    "        \n",
    "        # Slide over the sentence in chunks of max_length with overlap\n",
    "        for i in range(0, len(words), step):\n",
    "            chunk = words[i:i + max_length]\n",
    "            chunks.append(' '.join(chunk))  # Join back to a sentence\n",
    "            if i + max_length >= len(words):  # Exit condition if the chunk exceeds the sentence\n",
    "                break\n",
    "        return chunks\n",
    "\n",
    "    expanded_rows = []\n",
    "\n",
    "    # Iterate over the original DataFrame\n",
    "    for idx, row in df.iterrows():\n",
    "        sentence = row['statement']\n",
    "        label = row['status']\n",
    "        \n",
    "        # Split the sentence into smaller chunks\n",
    "        chunks = split_sentence(sentence, max_length, )\n",
    "        \n",
    "        # Create new rows for each chunk\n",
    "        for chunk in chunks:\n",
    "            expanded_rows.append({'statement': chunk, 'word_count': np.char.count(chunk, ' ') + 1, 'status': label})\n",
    "    \n",
    "    # Return the expanded dataframe\n",
    "    return pd.DataFrame(expanded_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>statement</th>\n",
       "      <th>word_count</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>oh gosh</td>\n",
       "      <td>7</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>trouble slep confused mind restles heart tune</td>\n",
       "      <td>45</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wrong dear forward doubt stay restles restles ...</td>\n",
       "      <td>51</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>shift focus woried</td>\n",
       "      <td>18</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>restles restles month boy mean</td>\n",
       "      <td>30</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52975</th>\n",
       "      <td>anxiety cause faintnes stand title anxiety cau...</td>\n",
       "      <td>80</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52976</th>\n",
       "      <td>anxiety heart symptom similar help heart nt go...</td>\n",
       "      <td>126</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52977</th>\n",
       "      <td>travel anxiety hi long time anxiety suferer ti...</td>\n",
       "      <td>460</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52978</th>\n",
       "      <td>fomo things involve recently watch tv bit obse...</td>\n",
       "      <td>349</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52979</th>\n",
       "      <td>get day anxiety house kids adults overwhelming...</td>\n",
       "      <td>152</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>52980 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               statement  word_count   status\n",
       "0                                                oh gosh           7  Anxiety\n",
       "1          trouble slep confused mind restles heart tune          45  Anxiety\n",
       "2      wrong dear forward doubt stay restles restles ...          51  Anxiety\n",
       "3                                     shift focus woried          18  Anxiety\n",
       "4                         restles restles month boy mean          30  Anxiety\n",
       "...                                                  ...         ...      ...\n",
       "52975  anxiety cause faintnes stand title anxiety cau...          80  Anxiety\n",
       "52976  anxiety heart symptom similar help heart nt go...         126  Anxiety\n",
       "52977  travel anxiety hi long time anxiety suferer ti...         460  Anxiety\n",
       "52978  fomo things involve recently watch tv bit obse...         349  Anxiety\n",
       "52979  get day anxiety house kids adults overwhelming...         152  Anxiety\n",
       "\n",
       "[52980 rows x 3 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_export_candidate = sliding_window_split(df, 192, 0.5)\n",
    "df_export_candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oFB19IUHfmdr",
    "outputId": "4a0e8d05-d019-4ba6-9320-a2f76905357f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label with the lowest number of examples: Bipolar\n",
      "Number of examples: 959\n"
     ]
    }
   ],
   "source": [
    "# Count the number of examples for each label\n",
    "label_counts = df_export_candidate['status'].value_counts()\n",
    "\n",
    "# Find the label with the minimum count\n",
    "min_label = label_counts.idxmin()\n",
    "min_count = label_counts.min()\n",
    "\n",
    "print(f\"Label with the lowest number of examples: {min_label}\")\n",
    "print(f\"Number of examples: {min_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "5KHCgH_Ofmdr",
    "outputId": "60711178-c950-46a3-b613-34ccd4a066a4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>statement</th>\n",
       "      <th>word_count</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>best bipolar videos find youtube diagnose nigh...</td>\n",
       "      <td>1665</td>\n",
       "      <td>Bipolar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>create transcendental platform transcend bipol...</td>\n",
       "      <td>1654</td>\n",
       "      <td>Bipolar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>death seasons self hatre spoiler listen favori...</td>\n",
       "      <td>1652</td>\n",
       "      <td>Bipolar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tdcs research procedures homeno person visits ...</td>\n",
       "      <td>1633</td>\n",
       "      <td>Suicidal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tdcs research procedures homeno person visits ...</td>\n",
       "      <td>1633</td>\n",
       "      <td>Depression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6708</th>\n",
       "      <td>hear suces stories get share</td>\n",
       "      <td>28</td>\n",
       "      <td>Personality disorder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6709</th>\n",
       "      <td>hypochondria hypochondria</td>\n",
       "      <td>25</td>\n",
       "      <td>Personality disorder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6710</th>\n",
       "      <td>stoicism help wonder way</td>\n",
       "      <td>24</td>\n",
       "      <td>Personality disorder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6711</th>\n",
       "      <td>slut shame edit point</td>\n",
       "      <td>21</td>\n",
       "      <td>Personality disorder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6712</th>\n",
       "      <td>selfies find faces</td>\n",
       "      <td>18</td>\n",
       "      <td>Personality disorder</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6713 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              statement  word_count  \\\n",
       "0     best bipolar videos find youtube diagnose nigh...        1665   \n",
       "1     create transcendental platform transcend bipol...        1654   \n",
       "2     death seasons self hatre spoiler listen favori...        1652   \n",
       "3     tdcs research procedures homeno person visits ...        1633   \n",
       "4     tdcs research procedures homeno person visits ...        1633   \n",
       "...                                                 ...         ...   \n",
       "6708                       hear suces stories get share          28   \n",
       "6709                          hypochondria hypochondria          25   \n",
       "6710                           stoicism help wonder way          24   \n",
       "6711                              slut shame edit point          21   \n",
       "6712                                 selfies find faces          18   \n",
       "\n",
       "                    status  \n",
       "0                  Bipolar  \n",
       "1                  Bipolar  \n",
       "2                  Bipolar  \n",
       "3                 Suicidal  \n",
       "4               Depression  \n",
       "...                    ...  \n",
       "6708  Personality disorder  \n",
       "6709  Personality disorder  \n",
       "6710  Personality disorder  \n",
       "6711  Personality disorder  \n",
       "6712  Personality disorder  \n",
       "\n",
       "[6713 rows x 3 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_export_candidate = df_export_candidate.sort_values(by='word_count', ascending=False)\n",
    "df_export_candidate = df_export_candidate.groupby('status').head(min_count)\n",
    "df_export_candidate.reset_index(drop=True, inplace=True)\n",
    "df_export_candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "id": "yW2R5Fehfmdr",
    "outputId": "59637ac4-cc87-4176-bdfe-858eb7a32d9d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "status\n",
       "Bipolar                 959\n",
       "Suicidal                959\n",
       "Depression              959\n",
       "Stress                  959\n",
       "Anxiety                 959\n",
       "Personality disorder    959\n",
       "Normal                  959\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_export_candidate.status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "nMEVNuL2fmdr"
   },
   "outputs": [],
   "source": [
    "# df_export_candidate.drop(['word_count', 'word_count_range'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "qWw5R0ztfmds"
   },
   "outputs": [],
   "source": [
    "df_export_candidate = df_export_candidate.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "hXaIhmwafmds"
   },
   "outputs": [],
   "source": [
    "# Optional, export the cleaned dataset\n",
    "df_export_candidate.to_csv(data_dir+'/cleaned_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "94tE6LPnfmds"
   },
   "source": [
    "# Step 2: Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "OeooFwA3fmds"
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 10000\n",
    "MAX_LENGTH = 500\n",
    "TRAINING_SPLIT = 0.8\n",
    "BATCH_SIZE = 32\n",
    "PADDING_TYPE = 'post'\n",
    "TRUNC_TYPE = 'post'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "YWoCiB0Nfmds"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1733728900.935436   13245 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1767 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "# Run this code if you skip step 1. beware, cleaned dataset is not updated regularly\n",
    "# !wget -O {data_dir+\"/cleaned_data.csv\"} \"https://drive.google.com/uc?export=download&id=1yQ8tt6HF6X_A_P0eYwS3yFC5vxGZYdkY\"\n",
    "\n",
    "sentences = []\n",
    "labels = []\n",
    "\n",
    "with open(data_dir+\"/cleaned_data.csv\", 'r') as csvfile:\n",
    "    heading = next(csvfile)\n",
    "    reader_obj = csv.reader(csvfile)\n",
    "    for row in reader_obj:\n",
    "        labels.append(row[1])\n",
    "        sentences.append(row[0])\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((sentences, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-_D7MDhcfmds"
   },
   "source": [
    "### Step 2a: Create tf.data.Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OBrpjjNkfmds",
    "outputId": "8570e108-15c5-40b4-a905-c689274faab8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 42384 sentence-label pairs for training.\n",
      "\n",
      "There are 10596 sentence-label pairs for validation.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_size = int(len(dataset) * TRAINING_SPLIT)\n",
    "train_dataset = dataset.take(train_size)\n",
    "validation_dataset = dataset.skip(train_size)\n",
    "\n",
    "print(f\"There are {train_dataset.cardinality()} sentence-label pairs for training.\\n\")\n",
    "print(f\"There are {validation_dataset.cardinality()} sentence-label pairs for validation.\\n\")\n",
    "\n",
    "train_statement = train_dataset.map(lambda statement, status: statement)\n",
    "train_labels = train_dataset.map(lambda statement, status: status)\n",
    "\n",
    "test_statement = validation_dataset.map(lambda statement, status: statement)\n",
    "test_labels = validation_dataset.map(lambda statement, status: status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0etvY-Fdfmdt"
   },
   "source": [
    "### Step 2b: Create tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0kn9QBxufmdt",
    "outputId": "22c7dd85-bc28-473e-a9ec-596cb93f291c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-09 14:22:44.034810: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary contains 9600 words\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# comment this code if there's already vocab output file\n",
    "keras_nlp.tokenizers.compute_word_piece_vocabulary(\n",
    "    train_statement,\n",
    "    vocabulary_size=VOCAB_SIZE,\n",
    "    reserved_tokens=[\"[PAD]\", \"[UNK]\"],\n",
    "    vocabulary_output_file='mental_vocab_subwords.txt'\n",
    ")\n",
    "\n",
    "# Initialize the subword tokenizer\n",
    "vectorizer = keras_nlp.tokenizers.WordPieceTokenizer(\n",
    "    vocabulary='./mental_vocab_subwords.txt'\n",
    ")\n",
    "\n",
    "vocab_size = vectorizer.vocabulary_size()\n",
    "print(f\"Vocabulary contains {vocab_size} words\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r3YCAhMAfmdt"
   },
   "source": [
    "### Step 2c: Create label encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "inY46A8xfmdu",
    "outputId": "e8fbcc1a-0302-462c-bdfe-4e22a855f915"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels: [np.str_('Depression'), np.str_('Normal'), np.str_('Suicidal'), np.str_('Anxiety'), np.str_('Bipolar'), np.str_('Stress'), np.str_('Personality disorder')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-09 14:27:43.234466: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "def fit_label_encoder(train_labels, validation_labels):\n",
    "    \"\"\"Creates an instance of a StringLookup, and trains it on all labels\n",
    "\n",
    "    Args:\n",
    "        train_labels (tf.data.Dataset): dataset of train labels\n",
    "        validation_labels (tf.data.Dataset): dataset of validation labels\n",
    "\n",
    "    Returns:\n",
    "        tf.keras.layers.StringLookup: adapted encoder for train and validation labels\n",
    "    \"\"\"\n",
    "    # join the two label datasets\n",
    "    labels = train_labels.concatenate(validation_labels) #concatenate the two datasets.\n",
    "\n",
    "    # Instantiate the StringLookup layer. Remember that you don't want any OOV tokens\n",
    "    label_encoder = tf.keras.layers.StringLookup(num_oov_indices=0)\n",
    "\n",
    "    # Fit the TextVectorization layer on the train_labels\n",
    "    label_encoder.adapt(labels)\n",
    "\n",
    "    return label_encoder\n",
    "\n",
    "# Create the label encoder\n",
    "label_encoder = fit_label_encoder(train_labels,test_labels)\n",
    "\n",
    "print(f'Unique labels: {label_encoder.get_vocabulary()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CwtLzjPYfmdu"
   },
   "source": [
    "### Step 2d: Dataset preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "F6J_HF3Rfmdu"
   },
   "outputs": [],
   "source": [
    "def padding_func(sequences):\n",
    "  '''Generates padded sequences from a tf.data.Dataset'''\n",
    "\n",
    "  # Put all elements in a single ragged batch\n",
    "  sequences = sequences.ragged_batch(batch_size=sequences.cardinality())\n",
    "\n",
    "  # Output a tensor from the single batch\n",
    "  sequences = sequences.get_single_element()\n",
    "\n",
    "  # Pad the sequences\n",
    "  padded_sequences = tf.keras.utils.pad_sequences(sequences.numpy(),\n",
    "                                                  maxlen=MAX_LENGTH,\n",
    "                                                  truncating=TRUNC_TYPE,\n",
    "                                                  padding=PADDING_TYPE\n",
    "                                                  )\n",
    "\n",
    "  # Convert back to a tf.data.Dataset\n",
    "  padded_sequences = tf.data.Dataset.from_tensor_slices(padded_sequences)\n",
    "\n",
    "  return padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ElMlNEjOfmdu",
    "outputId": "c3f94775-9b3d-42f1-868c-b76c1b35a117"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches in the train dataset: 1325\n",
      "Number of batches in the validation dataset: 332\n"
     ]
    }
   ],
   "source": [
    "# Preprocess dataset\n",
    "SHUFFLE_BUFFER_SIZE = 10000\n",
    "PREFETCH_BUFFER_SIZE = tf.data.AUTOTUNE\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Generate integer sequences using the subword tokenizer\n",
    "train_sequences_subword = train_statement.map(lambda statement: vectorizer.tokenize(statement)).apply(padding_func)\n",
    "test_sequences_subword = test_statement.map(lambda statement: vectorizer.tokenize(statement)).apply(padding_func)\n",
    "\n",
    "train_labels_encoded = train_labels.map(lambda label: label_encoder(label))\n",
    "test_labels_encoded = test_labels.map(lambda label: label_encoder(label))\n",
    "\n",
    "# Combine the integer sequence and labels\n",
    "train_dataset_vectorized = tf.data.Dataset.zip(train_sequences_subword,train_labels_encoded)\n",
    "test_dataset_vectorized = tf.data.Dataset.zip(test_sequences_subword,test_labels_encoded)\n",
    "\n",
    "# Optimize the datasets for training\n",
    "train_dataset_final = (train_dataset_vectorized\n",
    "                       .shuffle(SHUFFLE_BUFFER_SIZE)\n",
    "                       .cache()\n",
    "                       .prefetch(buffer_size=PREFETCH_BUFFER_SIZE)\n",
    "                       .batch(BATCH_SIZE)\n",
    "                       )\n",
    "\n",
    "test_dataset_final = (test_dataset_vectorized\n",
    "                      .cache()\n",
    "                      .prefetch(buffer_size=PREFETCH_BUFFER_SIZE)\n",
    "                      .batch(BATCH_SIZE)\n",
    "                      )\n",
    "\n",
    "\n",
    "print(f\"Number of batches in the train dataset: {train_dataset_final.cardinality()}\")\n",
    "print(f\"Number of batches in the validation dataset: {test_dataset_final.cardinality()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0uZgZoqHfmdu",
    "outputId": "198bc23a-1719-4daa-c93d-49d0d6ce1732"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the train batch: (32, 500)\n",
      "Shape of the validation batch: (32, 500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-09 14:29:07.734575: W tensorflow/core/kernels/data/cache_dataset_ops.cc:914] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2024-12-09 14:29:07.790259: W tensorflow/core/kernels/data/cache_dataset_ops.cc:914] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "train_batch = next(train_dataset_final.as_numpy_iterator())\n",
    "validation_batch = next(test_dataset_final.as_numpy_iterator())\n",
    "\n",
    "print(f\"Shape of the train batch: {train_batch[0].shape}\")\n",
    "print(f\"Shape of the validation batch: {validation_batch[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: {np.int64(0): np.float64(0.46860592391124084), np.int64(1): np.float64(0.47713610266801754), np.int64(2): np.float64(0.6762936605447496), np.int64(3): np.float64(2.009577544924375), np.int64(4): np.float64(2.8346709470304976), np.int64(5): np.float64(3.2206686930091184), np.int64(6): np.float64(7.654686653422431)}\n"
     ]
    }
   ],
   "source": [
    "train_labels_numpy = np.array(list(train_labels_encoded))\n",
    "\n",
    "# Tentukan kelas yang ada di dataset Anda\n",
    "classes = np.unique(train_labels_numpy)  # atau manual, misalnya classes = [0, 1, 2, 3, 4, 5, 6]\n",
    "\n",
    "# Menghitung class weights\n",
    "class_weights = sklearn.utils.compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=classes,\n",
    "    y=train_labels_numpy\n",
    ")\n",
    "\n",
    "# Membuat dictionary class weights\n",
    "class_weight_dict = {classes[i]: class_weights[i] for i in range(len(classes))}\n",
    "\n",
    "print(f\"Class weights: {class_weight_dict}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rhi60hT1fmdu"
   },
   "source": [
    "### Step 2e: Architect the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 433
    },
    "id": "rq4EGjQ1fmdu",
    "outputId": "a05392d5-7865-4976-c6b9-354185d1c1c9"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "EMBEDDING_DIM = 64\n",
    "RNN_DIM = 64\n",
    "DENSE_DIM = 128\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.Input(shape=(None,)),\n",
    "    tf.keras.layers.Embedding(vectorizer.vocabulary_size(), EMBEDDING_DIM),\n",
    "    #tf.keras.layers.Conv1D(32, 2, activation='relu'),\n",
    "    #tf.keras.layers.MaxPooling1D(2),\n",
    "    #tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.SimpleRNN(RNN_DIM, return_sequences=True, dropout=0.2, recurrent_dropout=0.1),\n",
    "    tf.keras.layers.SimpleRNN(RNN_DIM // 2, return_sequences=True, dropout=0.2, recurrent_dropout=0.1),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.GlobalMaxPooling1D(),\n",
    "    tf.keras.layers.Dense(7, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=3e-6),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NG8hLpP5fmdv",
    "outputId": "df4c08d2-fbc8-4d1e-9442-d45e11112e2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions have shape: (32, 7)\n"
     ]
    }
   ],
   "source": [
    "# Check model compatibility\n",
    "example_batch = train_dataset_final.take(1)\n",
    "\n",
    "try:\n",
    "    model.evaluate(example_batch, verbose=False)\n",
    "except Exception as e:\n",
    "    print(f\"Your model is not compatible with the dataset you defined earlier. Check that the loss function and last layer are compatible with one another. Error: {e}\")\n",
    "else:\n",
    "    predictions = model.predict(example_batch, verbose=False)\n",
    "    print(f\"predictions have shape: {predictions.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gmBrP0z0fmdv"
   },
   "source": [
    "# Step 3: How To Train Your ~~Dragon~~ ML Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HedQzuKZfmdv"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "1zGaikEDfmdv"
   },
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "checkpoint_path = data_dir\n",
    "checkpoint_model_filepath = checkpoint_path+\"/checkpoint.keras\"\n",
    "checkpoint_num_epoch_filepath = checkpoint_path+\"/current_epoch.txt\"\n",
    "training_log_filepath = checkpoint_path+\"/training_log.json\"\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_model_filepath,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "\n",
    "class CustomCheckpointCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, epoch_file, log_file):\n",
    "        self.epoch_file = epoch_file\n",
    "        self.log_file = log_file\n",
    "        self.history = {\"epoch\": [], \"train_loss\": [], \"train_accuracy\": [],\n",
    "                        \"val_loss\": [], \"val_accuracy\": []}\n",
    "\n",
    "        # Load history and starting epoch if they exist\n",
    "        if os.path.exists(self.log_file):\n",
    "            with open(self.log_file, 'r') as f:\n",
    "                self.history = json.load(f)\n",
    "\n",
    "        if os.path.exists(self.epoch_file):\n",
    "            with open(self.epoch_file, 'r') as f:\n",
    "                self.starting_epoch = int(f.read())\n",
    "        else:\n",
    "            # with open(self.epoch_file, 'w') as f:\n",
    "            #     f.write(str(0))\n",
    "            self.starting_epoch = 0\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current_epoch = self.starting_epoch + epoch + 1\n",
    "        # Save the current epoch\n",
    "        with open(self.epoch_file, 'w') as f:\n",
    "            f.write(str(current_epoch + 1))\n",
    "\n",
    "        # Save logs (loss, accuracy, etc.) for plotting\n",
    "        self.history[\"epoch\"].append(current_epoch + 1)\n",
    "        self.history[\"train_loss\"].append(logs.get(\"loss\"))\n",
    "        self.history[\"train_accuracy\"].append(logs.get(\"accuracy\"))\n",
    "        self.history[\"val_loss\"].append(logs.get(\"val_loss\"))\n",
    "        self.history[\"val_accuracy\"].append(logs.get(\"val_accuracy\"))\n",
    "\n",
    "        # Save history to the log file\n",
    "        with open(self.log_file, 'w') as f:\n",
    "            json.dump(self.history, f, indent=4)\n",
    "\n",
    "custom_checkpoint_callback = CustomCheckpointCallback(checkpoint_num_epoch_filepath, training_log_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jZ-YPeiLfmdv",
    "outputId": "a70746fb-f93d-46c3-9f5c-b110bec5c659"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint deleted successfully\n"
     ]
    }
   ],
   "source": [
    "# run this code to delete checkpoint\n",
    "try:\n",
    "    os.remove(checkpoint_model_filepath)\n",
    "    os.remove(checkpoint_num_epoch_filepath)\n",
    "    os.remove(training_log_filepath)\n",
    "    print(\"Checkpoint deleted successfully\")\n",
    "except FileNotFoundError:\n",
    "    print(\"No saved epoch found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wSc1uib7fmdv",
    "outputId": "3f9c3f88-7b32-4639-a0f0-a1bd57ba923f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No saved epoch found. Starting from epoch 0\n"
     ]
    }
   ],
   "source": [
    "# run this code to load from checkpoint\n",
    "try:\n",
    "    with open(checkpoint_num_epoch_filepath, 'r') as f:\n",
    "        start_epoch = int(f.read())\n",
    "    print(f\"Resuming training from epoch {start_epoch}\")\n",
    "except FileNotFoundError:\n",
    "    start_epoch = 0\n",
    "    print(\"No saved epoch found. Starting from epoch 0\")\n",
    "\n",
    "# Load saved weights\n",
    "if start_epoch > 0:\n",
    "    model.load_weights(checkpoint_model_filepath)\n",
    "    print(f\"Loaded weights from {checkpoint_model_filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jYeszbOpfmdw",
    "outputId": "5c2f7d4d-23ce-46f6-95c6-cf8af2034129"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1325/1325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 86ms/step - accuracy: 0.3001 - loss: 2.9302 - val_accuracy: 0.2948 - val_loss: 1.9144\n",
      "Epoch 2/10\n",
      "\u001b[1m1325/1325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 79ms/step - accuracy: 0.3001 - loss: 2.5854 - val_accuracy: 0.2290 - val_loss: 1.8953\n",
      "Epoch 3/10\n",
      "\u001b[1m 786/1325\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 72ms/step - accuracy: 0.3000 - loss: 2.3638"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[78], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m num_of_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m----> 2\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset_final\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_of_epoch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstart_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_dataset_final\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_checkpoint_callback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_checkpoint_callback\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/capstone/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/capstone/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py:368\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[1;32m    367\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 368\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    369\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[0;32m~/anaconda3/envs/capstone/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py:217\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    214\u001b[0m     iterator, (tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterator, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedIterator)\n\u001b[1;32m    215\u001b[0m ):\n\u001b[1;32m    216\u001b[0m     opt_outputs \u001b[38;5;241m=\u001b[39m multi_step_on_iterator(iterator)\n\u001b[0;32m--> 217\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mopt_outputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhas_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m opt_outputs\u001b[38;5;241m.\u001b[39mget_value()\n",
      "File \u001b[0;32m~/anaconda3/envs/capstone/lib/python3.10/site-packages/tensorflow/python/data/ops/optional_ops.py:176\u001b[0m, in \u001b[0;36m_OptionalImpl.has_value\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhas_value\u001b[39m(\u001b[38;5;28mself\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    175\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mcolocate_with(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variant_tensor):\n\u001b[0;32m--> 176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgen_optional_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptional_has_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variant_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/capstone/lib/python3.10/site-packages/tensorflow/python/ops/gen_optional_ops.py:172\u001b[0m, in \u001b[0;36moptional_has_value\u001b[0;34m(optional, name)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[1;32m    171\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 172\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mOptionalHasValue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptional\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[1;32m    175\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_of_epoch = 10\n",
    "history = model.fit(\n",
    "    train_dataset_final,\n",
    "    epochs=num_of_epoch - start_epoch,\n",
    "    validation_data=test_dataset_final,\n",
    "    callbacks=[\n",
    "        model_checkpoint_callback,\n",
    "        custom_checkpoint_callback\n",
    "    ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 957
    },
    "id": "TTR38rCrfmdw",
    "outputId": "0ad68241-5100-4ba0-81ee-ca57889d36c6"
   },
   "outputs": [],
   "source": [
    "def plot_graphs(training_log):\n",
    "    # Load the log file\n",
    "    with open(training_log, \"r\") as f:\n",
    "        history = json.load(f)\n",
    "\n",
    "    # Plot loss\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history[\"epoch\"], history[\"train_loss\"], label=\"Training Loss\")\n",
    "    plt.plot(history[\"epoch\"], history[\"val_loss\"], label=\"Validation Loss\")\n",
    "    plt.title(\"Loss per Epoch\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history[\"epoch\"], history[\"train_accuracy\"], label=\"Training Accuracy\")\n",
    "    plt.plot(history[\"epoch\"], history[\"val_accuracy\"], label=\"Validation Accuracy\")\n",
    "    plt.title(\"Accuracy per Epoch\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_graphs(training_log_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IuvJIudufmdw"
   },
   "source": [
    "# Step 4: Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ByeseFDbfmdw",
    "outputId": "59a193a8-02bb-454a-b301-e0906c000d86"
   },
   "outputs": [],
   "source": [
    "rawtext_test = [\"i'm really nervous\"]\n",
    "sequence_test = padding_func(tf.data.Dataset.from_tensors(tokenizer.tokenize(rawtext_test)))\n",
    "for element in sequence_test:\n",
    "    print(element)\n",
    "predictions = model.predict(sequence_test)\n",
    "\n",
    "# predictions will be a numpy array of shape (1, num_classes) with probabilities for each class\n",
    "print(predictions)\n",
    "\n",
    "# To get the predicted class index\n",
    "predicted_class_index = tf.argmax(predictions, axis=1).numpy()[0]\n",
    "print(f\"Predicted class: {label_encoder.get_vocabulary()[predicted_class_index]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VcaCRQZcfmdx"
   },
   "source": [
    "# Step 5: Done, save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y-L64a4Kfmdx"
   },
   "outputs": [],
   "source": [
    "# Run this if you happy with the model\n",
    "with open(data_dir+\"/label_vocabulary.txt\", \"w\") as f:\n",
    "    for label in label_encoder.get_vocabulary():\n",
    "        f.write(label + \"\\n\")\n",
    "\n",
    "model.save(data_dir+'/second_iteration.keras')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
